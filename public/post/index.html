<!DOCTYPE html>
<html lang="en-uk">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.55.0" />
  <meta name="author" content="Robert William Schlegel">

  
  
  
  
  <meta name="description" content="Data Scientist">

  
  <link rel="alternate" hreflang="en-uk" href="https://theoceancode.netlify.app/post/">

  


  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'G-P6C1DFHPRD', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://theoceancode.netlify.app/post/index.xml" type="application/rss+xml" title="The Ocean Code">
  <link rel="feed" href="https://theoceancode.netlify.app/post/index.xml" type="application/rss+xml" title="The Ocean Code">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://theoceancode.netlify.app/post/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@robwschlegel">
  <meta property="twitter:creator" content="@robwschlegel">
  
  <meta property="og:site_name" content="The Ocean Code">
  <meta property="og:url" content="https://theoceancode.netlify.app/post/">
  <meta property="og:title" content="Posts | The Ocean Code">
  <meta property="og:description" content="Data Scientist">
  <meta property="og:locale" content="en-uk">
  
  <meta property="og:updated_time" content="2021-05-21T00:00:00&#43;00:00">
  

  

  

  <title>Posts | The Ocean Code</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">The Ocean Code</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#packages">
            
            <span>Packages</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Blog</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posters">
            
            <span>Posters</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#workshops">
            
            <span>Workshops</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>





<div class="universal-wrapper">

  <h1>Posts</h1>

  

  
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2021-05-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2021-05-21 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2021-05-21
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/odv_bathy/" itemprop="url">ODV figures in R with bathymetry</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective Nearly four years after writing a blog post about recreating R figures in ODV I had someone reach out to me expressing interest in adding a bathymetry layer over the interpolated data. It&rsquo;s always nice to know that these blog posts are being found useful for other researchers. And I have to admit I&rsquo;m a bit surprised that the code still runs 4 years later. Especially considering that it uses the tidyverse which is notorious for breaking backwards compatibility.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/odv_bathy/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2020-06-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2020-06-18 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2020-06-18
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/bo_analysis/" itemprop="url">Analysis of Bio-Oracle data</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective While running some brief quality control tests on Bio-Oracle layers before using them for a recent project it was detected that some of the layers in the current version of the Bio-Oracle product appear to have very large errors. Specifically the error is that there are layers where the minimum values are greater than the maximum values. It is unclear how this could be possible, so in the following text and code we will look into how we go about investigating these data layers and we will discuss which layers are fine, and which are not.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/bo_analysis/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2020-06-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2020-06-18 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2020-06-18
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/bo_analysis/" itemprop="url">Analysis of Bio-Oracle data</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective While running some brief quality control tests on Bio-Oracle layers before using them for a recent project it was detected that some of the layers in the current version of the Bio-Oracle product appear to have very large errors. Specifically the error is that there are layers where the minimum values are greater than the maximum values. It is unclear how this could be possible, so in the following text and code we will look into how we go about investigating these data layers and we will discuss which layers are fine, and which are not.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/bo_analysis/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2020-02-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2020-02-14 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2020-02-14
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    0 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/dl_env_data_r/" itemprop="url">Downloading environmental data in R</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    


<div id="objective" class="section level2">
<h2>Objective</h2>
<p>Having been working in environmental science for several years now, entirely using R, I’ve come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn’t be. Most data hosting organisations do want scientists to use their data and do make it freely available. But sometimes it feels like the path to access was designed by crab people, rather than normal topside humans. I recently needed to gather several new data products and in classic ‘cut your nose off to spite your face’ fashion I insisted on doing all of it directly through an R script that could be run in RStudio. Besides being stubborn, one of the main reasons I felt this was necessary is that I wanted these download scripts to be able to be run operationally via a cron job. I think I came out pretty successful in the end so wanted to share the code with the rest of the internet. Enjoy.</p>
<pre class="r"><code># Packages not available via CRAN
remotes::install_github(&quot;skgrange/threadr&quot;)
remotes::install_github(&quot;markpayneatwork/RCMEMS&quot;)

# The packages we will use
library(tidyverse) # A staple for most modern data management in R
library(RCurl) # For helping R to make sense of URLs for web hosted data
library(XML) # For reading the HTML tables created by RCurl
library(tidync) # For easily dealing with NetCDF data
library(doParallel) # For parallel processing
library(threadr) # For downloading from FTP sites that require user credentials
library(RCMEMS) # For subsetting CMEMS data before download</code></pre>
</div>
<div id="downloading-noaa-oisst" class="section level2">
<h2>Downloading NOAA OISST</h2>
<p>I’ve already written a post about how to download NOAA OISST data using the <strong><code>rerddap</code></strong> package which may be found <a href="https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html">here</a>. That post talks about how to get subsets of NOAA data, which is useful for projects with a refined scope, but it is laboriously slow if one simply wants the full global product. It must also be noted that as of this writing (June 3rd, 2020) the new OISST v2.1 data were not yet available on the ERDDAP server even though the old v2 data have now been rendered unavailable. For the time being it is necessary to download the full global data and then subset down to one’s desired study area. The following section of this blog post will outline how to do that.</p>
<p>I need to stress that this is a very direct and unlimited method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset just because you can.</p>
<pre class="r"><code># First we tell R where the data are on the interwebs
OISST_url_month &lt;- &quot;https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/&quot;

# Then we pull that into a happy format
  # There is a lot here so it takes ~1 minute
OISST_url_month_get &lt;- getURL(OISST_url_month)

# Before we continue let&#39;s set a limit on the data we are going to download
  # NB: One should not simply download the entire dataset just because it is possible.
  # There should be a compelling reason for doing so.
start_date &lt;- as.Date(&quot;2019-01-01&quot;)

# Now we strip away all of the unneeded stuff to get just the months of data that are available
OISST_months &lt;- data.frame(months = readHTMLTable(OISST_url_month_get, skip.rows = 1:2)[[1]]$Name) %&gt;% 
  mutate(months = lubridate::as_date(str_replace(as.character(months), &quot;/&quot;, &quot;01&quot;))) %&gt;% 
  filter(months &gt;= max(lubridate::floor_date(start_date, unit = &quot;month&quot;))) %&gt;% # Filtering out months before Jan 2019
  mutate(months = gsub(&quot;-&quot;, &quot;&quot;, substr(months, 1, 7))) %&gt;% 
  na.omit()

# Up next we need to now find the URLs for each individual day of data
# To do this we will wrap the following chunk of code into a function so we can loop it more easily
OISST_url_daily &lt;- function(target_month){
  OISST_url &lt;- paste0(OISST_url_month, target_month,&quot;/&quot;)
  OISST_url_get &lt;- getURL(OISST_url)
  OISST_table &lt;- data.frame(files = readHTMLTable(OISST_url_get, skip.rows = 1:2)[[1]]$Name) %&gt;% 
    mutate(files = as.character(files)) %&gt;% 
    filter(grepl(&quot;avhrr&quot;, files)) %&gt;% 
    mutate(t = lubridate::as_date(sapply(strsplit(files, &quot;[.]&quot;), &quot;[[&quot;, 2)),
           full_name = paste0(OISST_url, files))
  return(OISST_table)
}

# Here we collect the URLs for every day of data available from 2019 onwards
OISST_filenames &lt;- plyr::ldply(OISST_months$months, .fun = OISST_url_daily)

# Just to keep things tidy in this vignette I am now going to limit this data collection even further
OISST_filenames &lt;- OISST_filenames %&gt;% 
  filter(t &lt;= &quot;2019-01-31&quot;)

# This function will go about downloading each day of data as a NetCDF file
# We will run this via plyr to expedite the process
# Note that this will download files into a &#39;data/OISST&#39; folder in the root directory
# If this folder does not exist it will create it
# This function will also check if the file has been previously downloaded
OISST_url_daily_dl &lt;- function(target_URL){
  dir.create(&quot;~/data/OISST&quot;, showWarnings = F)
  file_name &lt;- paste0(&quot;~/data/OISST/&quot;,sapply(strsplit(target_URL, split = &quot;/&quot;), &quot;[[&quot;, 10))
  if(!file.exists(file_name)) download.file(url = target_URL, method = &quot;libcurl&quot;, destfile = file_name)
}

# The way this code has been written it may be run on multiple cores
# Most modern laptops have at least 4 cores, so we will utilise 3 of them here
# One should always leave at least 1 core free
doParallel::registerDoParallel(cores = 3)

# And with that we are clear for take off
system.time(plyr::ldply(OISST_filenames$full_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds

# In roughly 15 seconds a user may have a full month of global data downloaded
# This scales well into years and decades, too</code></pre>
<p>Because it is not currently possible to download subsetted OISST data from a GRIDDAP server I find that it is useful to include here the code one would use to load and subset downloaded OISST data. Please note that the OISST data have longitude values from 0 to 360, not -180 to 180.</p>
<pre class="r"><code># This function will load and subset daily data into one data.frame
# Note that the subsetting of lon/lat is done before the data are loaded
# This means it will use much less RAM and is viable for use on most laptops
# Assuming one&#39;s study area is not too large
OISST_load &lt;- function(file_name, lon1, lon2, lat1, lat2){
      OISST_dat &lt;- tidync(file_name) %&gt;%
        hyper_filter(lon = between(lon, lon1, lon2),
                     lat = between(lat, lat1, lat2)) %&gt;% 
        hyper_tibble() %&gt;% 
        select(lon, lat, time, sst) %&gt;% 
        dplyr::rename(t = time, temp = sst) %&gt;% 
        mutate(t = as.Date(t, origin = &quot;1978-01-01&quot;))
      return(OISST_dat)
}

# Locate the files that will be loaded
OISST_files &lt;- dir(&quot;~/data/OISST&quot;, full.names = T)

# Load the data in parallel
OISST_dat &lt;- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T,
                         lon1 = 260, lon2 = 280, lat1 = 30, lat2 = 50)

# This should only take a few seconds to run at most</code></pre>
</div>
<div id="downloading-cci" class="section level2">
<h2>Downloading CCI</h2>
<p>An up-and-coming star in the world of remotely sensed data products, the Climate Change Initiative (CCI) has recently been putting out some industry leading products. These are all freely available for access and use for scientific research purposes. These have quickly become regarded as the most accurate products available and their use is now encouraged over other products. Unfortunately they are not available in near-real-time and so can currently only be used for historic analyses. A recent update of these data for 2017 and 2018 was made available and one assumes that 2019 will follow suit some time by the end of 2020.</p>
<pre class="r"><code># The URLs where the data are housed for direct download
  # NB: Note that the versions are different; v2.1 vs. v2.0
  # NB: It looks like going straight through the thredds server is a more stable option
CCI_URL_old &lt;- &quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/esacci/sst/data/CDR_v2/Analysis/L4/v2.1&quot;
CCI_URL_new &lt;- &quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/c3s_sst/data/ICDR_v2/Analysis/L4/v2.0&quot;

# The date ranges that are housed therein
  # NB: These are historic repos and therefore the dates are static
  # I assume that the &#39;new&#39; data will be updated through 2019 by the end of 2020
date_range_old &lt;- seq(as.Date(&quot;1981-09-01&quot;), as.Date(&quot;2016-12-31&quot;), by = &quot;day&quot;)
date_range_new &lt;- seq(as.Date(&quot;2017-01-01&quot;), as.Date(&quot;2018-12-31&quot;), by = &quot;day&quot;)

# The function we will use to download the data
download_CCI &lt;- function(date_choice, CCI_URL){
  
  # Prep the necessary URL pieces
  date_slash &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;/&quot;)
  date_nogap &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;&quot;)
  
  if(str_detect(CCI_URL, &quot;esacci&quot;)){
    tail_chunk &lt;- &quot;120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.nc&quot;
  } else if(str_detect(CCI_URL, &quot;c3s_sst&quot;)){
    tail_chunk &lt;- &quot;120000-C3S-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR2.0-v02.0-fv01.0.nc&quot;
  } else{
    stop(&quot;The URL structure has changed.&quot;)
  }
  
  complete_URL &lt;- paste0(CCI_URL,&quot;/&quot;,date_slash,&quot;/&quot;,date_nogap,tail_chunk)
  # Note that this will download the files to data/CCI in the root directory
  file_name &lt;- paste0(&quot;~/data/CCI/&quot;,date_nogap,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download.file(url = complete_URL, method = &quot;libcurl&quot;, destfile = file_name)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
# Most laptops have 4 cores, so 3 is a good choice
doParallel::registerDoParallel(cores = 3)

# Download all old data: 1981-09-01 to 2016-12-31
  # NB: Note the &#39;[1:3]&#39; below. This limits the downloads to only the first three files
  # Delete that to download everything.
  # But please do not download ALL of the files unless there is a need to do so.
plyr::l_ply(date_range_old[1:3], .fun = download_CCI, CCI_URL = CCI_URL_old, .parallel = T)

# Download all new data: 2016-01-01 to 2018-12-31
plyr::l_ply(date_range_new[1:3], .fun = download_CCI, CCI_URL = CCI_URL_new, .parallel = T)</code></pre>
</div>
<div id="downloading-ostia" class="section level2">
<h2>Downloading OSTIA</h2>
<p>As noted above, CCI data products are quickly becoming the preferred standard. Unfortunately they are not available in near-real-time. This is where OSTIA data come in to fill the gap. Though not exactly the same assimilation process as CCI, these products come from the same suite of data sources. I do not yet know if these data for 2019 onwards can be used in combination with a climatology created from the CCI data, but it is on my to do list to find out. In order to download these data one will need to have a <a href="http://marine.copernicus.eu/">CMEMS</a> account. This is free for researchers and very fast to sign up for. Once one has received a user name and password it is possible to use the code below to download the data via their FTP server. No Python required!</p>
<pre class="r"><code># The URL where the data are housed for FTP
OSTIA_URL &lt;- &quot;ftp://nrt.cmems-du.eu/Core/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2&quot;

# The date ranges that are housed therein
# NB: These are historic repos and therefore the dates are static
# I assume that the &#39;new&#39; data will be updated through 2019 by the end of 2020
date_range &lt;- seq(as.Date(&quot;2019-01-01&quot;), as.Date(&quot;2019-01-03&quot;), by = &quot;day&quot;)

# Enter ones credentials here
# Note that the &#39;:&#39;  between &#39;username&#39; and &#39;password&#39; is required
user_credentials &lt;- &quot;username:password&quot;

# Download function 
download_OSTIA &lt;- function(date_choice, user_credentials){
  
  # Prep the necessary URL pieces
  date_slash &lt;- strtrim(str_replace_all(date_choice, &quot;-&quot;, &quot;/&quot;), width = 7)
  date_nogap_day &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;&quot;)
  
  tail_chunk &lt;- &quot;120000-UKMO-L4_GHRSST-SSTfnd-OSTIA-GLOB-v02.0-fv02.0.nc&quot;

  complete_URL &lt;- paste0(OSTIA_URL,&quot;/&quot;,date_slash,&quot;/&quot;,date_nogap_day,tail_chunk)
  file_name &lt;- paste0(&quot;~/data/OSTIA/&quot;,date_nogap_day,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download_ftp_file(complete_URL, file_name, verbose = TRUE, credentials = user_credentials)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
doParallel::registerDoParallel(cores = 3)

# Download data from 2019-01-01 to present day
  # NB: Some files won&#39;t download when run in parallel
  # I think the FTP server may be more aware of multiple requests than an HTTPS server
  # It may also have been due to high server use at the time, too
plyr::l_ply(date_range, .fun = download_OSTIA, .parallel = T, 
            user_credentials = user_credentials)</code></pre>
</div>
<div id="downloading-glorys" class="section level2">
<h2>Downloading GLORYS</h2>
<p>At this point one may be thinking “wait a second, these have all been SST only products, this isn’t really a post about environmental data!”. But that’s where GLORYS comes in. This product has a range of variables one may be interested in. Not just SST. But yes, they are all physical variables. No bio-geochemistry in sight. Bamboozled! But if you’ve read this far, why stop now?! These data require a <a href="http://marine.copernicus.eu/">CMEMS</a> account, same as the OSTIA data. They can also be downloaded via direct FTP access, but these files are enormous so in almost every use case this is not what one is intending to do. Rather these data are almost always subsetted in some way first. Luckily CMEMS has made this available to their user base with the introduction of the <a href="https://github.com/clstoulouse/motu-client-python/releases">MOTU client</a>. Unluckily they have only made this available for use in Python. I have asked the people behind this process in person if there are plans for an officially supported R version and the short and long answers were no. That’s where Mark Payne and his <a href="https://github.com/markpayneatwork/RCMEMS">RCMEMS</a> package enter the picture. He has wrapped the MOTU client for Python up in a handy R package that allows us to access, subset, and download CMEMS data all through the comfort of the RStudio interface! There is a tutorial on the GitHub repo that walks through the process that I show below if one would like an additional look at this process.</p>
<pre class="r"><code># Non-R software
# Unfortunately the subsetting of CMEMS data will require that one has Python installed
# https://www.python.org/downloads/
# Then one must download
# https://github.com/clstoulouse/motu-client-python/releases
# For instructions on how and why to properly install please see:
# https://github.com/markpayneatwork/RCMEMS

# Here is a cunning method of generating a brick of year-month values
date_range &lt;- base::expand.grid(1993:2018, 1:12) %&gt;% 
  dplyr::rename(year = Var1, month = Var2) %&gt;% 
  arrange(year, month) %&gt;% 
  mutate(year_mon = paste0(year,&quot;-&quot;,month)) %&gt;% 
  dplyr::select(year_mon)

# Download function
  # NB: This function is currently designed to subset data to a specific domain
  # Please change your lon/lat accordingly
  # NB: This function will save files to data/GLORYS in the root directory
  # To change this change the --out-dir argument near the end of the chunk of text
  # NB: This big text chunk needs to be left as one long line
  # NB: The --user and --pwd arguments need to be given the users real username and passwords
  # from their CMEMS account
download_GLORYS &lt;- function(date_choice){
  
  # The GLORYS script
    # This is a dummy script first generated by using the UI on the CMEMS website
    # No need to change anything here except for the --user and --pwd at the end
    # Please place your CMEMS username and password in those fields
  GLORYS_script &lt;- &#39;python ~/motuclient-python/motuclient.py --motu http://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_030-TDS --product-id global-reanalysis-phy-001-030-daily --longitude-min -180 --longitude-max 179.9166717529297 --latitude-min -80 --latitude-max 90 --date-min &quot;2018-12-25 12:00:00&quot; --date-max &quot;2018-12-25 12:00:00&quot; --depth-min 0.493 --depth-max 0.4942 --variable thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi --out-dir data --out-name test.nc --user username --pwd password&#39;
  
  # Prep the necessary URL pieces
  date_start &lt;- parse_date(date_choice, format = &quot;%Y-%m&quot;)
  # A clever way of finding the end date of any month!
    # I found this on stackoverflow somewhere...
  date_end &lt;- date_start %m+% months(1) - 1
  
  # Cannot get data past 2018-12-25
  if(date_end &gt; as.Date(&quot;2018-12-25&quot;)) date_end &lt;- as.Date(&quot;2018-12-25&quot;)
  
  # Set the file name
  file_name &lt;- paste0(&quot;GLORYS_&quot;,date_choice,&quot;.nc&quot;)
  
  # Take the chunk of code above and turn it into something useful
  cfg &lt;- parse.CMEMS.script(GLORYS_script, parse.user = T)
  
  # This is where one should make any required changes to the subsetting of the data
  # This is now the magic of the RCMEMS package, which allows us to interface with the Python code as though it were R
  cfg_update &lt;- RCMEMS::update(cfg, variable = &quot;thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi&quot;,
                               longitude.min = &quot;-80.5&quot;,
                               longitude.max = &quot;-40.5&quot;,
                               latitude.min = &quot;31.5&quot;,
                               latitude.max = &quot;63.5&quot;,
                               date.min = as.character(date_start),
                               date.max = as.character(date_end),
                               out.name = file_name)
  
  # Download and save the file if needed
  if(file.exists(paste0(&quot;~/data/GLORYS/&quot;,file_name))){
    return()
  } else{
    CMEMS.download(cfg_update)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# I&#39;ve limited the download to only 1 file
# Delete &#39;[1]&#39; to download everything
  #NB: The CMEMS server is a little wonky, rather not try to multicore this
plyr::l_ply(date_range$year_mon[1], .fun = download_GLORYS, .parallel = F)</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I hope this has been a useful whack of data for anyone looking to download any of these products for their science. The techniques laid out in the code here should apply to most other data products as well as there aren’t that many different methods of hosting data. If I’ve missed anything that people feel is an important data source that can’t be adapted from the code here let me know and I’m happy to see what I can do.</p>
</div>

    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/dl_env_data_r/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2020-02-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2020-02-14 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2020-02-14
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    0 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/dl_env_data_r/" itemprop="url">Downloading environmental data in R</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    

<h2 id="objective">Objective</h2>

<p>Having been working in environmental science for several years now, entirely using R, I&rsquo;ve come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn&rsquo;t be. Most data hosting organisations do want scientists to use their data and do make it freely available. But sometimes it feels like the path to access was designed by crab people, rather than normal topside humans. I recently needed to gather several new data products and in classic &lsquo;cut your nose off to spite your face&rsquo; fashion I insisted on doing all of it directly through an R script that could be run in RStudio. Besides being stubborn, one of the main reasons I felt this was necessary is that I wanted these download scripts to be able to be run operationally via a cron job. I think I came out pretty successful in the end so wanted to share the code with the rest of the internet. Enjoy.</p>

<pre><code class="language-r"># Packages not available via CRAN
remotes::install_github(&quot;skgrange/threadr&quot;)
remotes::install_github(&quot;markpayneatwork/RCMEMS&quot;)

# The packages we will use
library(tidyverse) # A staple for most modern data management in R
library(RCurl) # For helping R to make sense of URLs for web hosted data
library(XML) # For reading the HTML tables created by RCurl
library(tidync) # For easily dealing with NetCDF data
library(doParallel) # For parallel processing
library(threadr) # For downloading from FTP sites that require user credentials
library(RCMEMS) # For subsetting CMEMS data before download
</code></pre>

<h2 id="downloading-noaa-oisst">Downloading NOAA OISST</h2>

<p>I&rsquo;ve already written a post about how to download NOAA OISST data using the <strong><code>rerddap</code></strong> package which may be found <a href="https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html" target="_blank">here</a>. That post talks about how to get subsets of NOAA data, which is useful for projects with a refined scope, but it is laboriously slow if one simply wants the full global product. It must also be noted that as of this writing (June 3rd, 2020) the new OISST v2.1 data were not yet available on the ERDDAP server even though the old v2 data have now been rendered unavailable. For the time being it is necessary to download the full global data and then subset down to one&rsquo;s desired study area. The following section of this blog post will outline how to do that.</p>

<p>I need to stress that this is a very direct and unlimited method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset just because you can.</p>

<pre><code class="language-r"># First we tell R where the data are on the interwebs
OISST_url_month &lt;- &quot;https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/&quot;

# Then we pull that into a happy format
  # There is a lot here so it takes ~1 minute
OISST_url_month_get &lt;- getURL(OISST_url_month)

# Before we continue let's set a limit on the data we are going to download
  # NB: One should not simply download the entire dataset just because it is possible.
  # There should be a compelling reason for doing so.
start_date &lt;- as.Date(&quot;2019-01-01&quot;)

# Now we strip away all of the unneeded stuff to get just the months of data that are available
OISST_months &lt;- data.frame(months = readHTMLTable(OISST_url_month_get, skip.rows = 1:2)[[1]]$Name) %&gt;% 
  mutate(months = lubridate::as_date(str_replace(as.character(months), &quot;/&quot;, &quot;01&quot;))) %&gt;% 
  filter(months &gt;= max(lubridate::floor_date(start_date, unit = &quot;month&quot;))) %&gt;% # Filtering out months before Jan 2019
  mutate(months = gsub(&quot;-&quot;, &quot;&quot;, substr(months, 1, 7))) %&gt;% 
  na.omit()

# Up next we need to now find the URLs for each individual day of data
# To do this we will wrap the following chunk of code into a function so we can loop it more easily
OISST_url_daily &lt;- function(target_month){
  OISST_url &lt;- paste0(OISST_url_month, target_month,&quot;/&quot;)
  OISST_url_get &lt;- getURL(OISST_url)
  OISST_table &lt;- data.frame(files = readHTMLTable(OISST_url_get, skip.rows = 1:2)[[1]]$Name) %&gt;% 
    mutate(files = as.character(files)) %&gt;% 
    filter(grepl(&quot;avhrr&quot;, files)) %&gt;% 
    mutate(t = lubridate::as_date(sapply(strsplit(files, &quot;[.]&quot;), &quot;[[&quot;, 2)),
           full_name = paste0(OISST_url, files))
  return(OISST_table)
}

# Here we collect the URLs for every day of data available from 2019 onwards
OISST_filenames &lt;- plyr::ldply(OISST_months$months, .fun = OISST_url_daily)

# Just to keep things tidy in this vignette I am now going to limit this data collection even further
OISST_filenames &lt;- OISST_filenames %&gt;% 
  filter(t &lt;= &quot;2019-01-31&quot;)

# This function will go about downloading each day of data as a NetCDF file
# We will run this via plyr to expedite the process
# Note that this will download files into a 'data/OISST' folder in the root directory
# If this folder does not exist it will create it
# This function will also check if the file has been previously downloaded
OISST_url_daily_dl &lt;- function(target_URL){
  dir.create(&quot;~/data/OISST&quot;, showWarnings = F)
  file_name &lt;- paste0(&quot;~/data/OISST/&quot;,sapply(strsplit(target_URL, split = &quot;/&quot;), &quot;[[&quot;, 10))
  if(!file.exists(file_name)) download.file(url = target_URL, method = &quot;libcurl&quot;, destfile = file_name)
}

# The way this code has been written it may be run on multiple cores
# Most modern laptops have at least 4 cores, so we will utilise 3 of them here
# One should always leave at least 1 core free
doParallel::registerDoParallel(cores = 3)

# And with that we are clear for take off
system.time(plyr::ldply(OISST_filenames$full_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds

# In roughly 15 seconds a user may have a full month of global data downloaded
# This scales well into years and decades, too
</code></pre>

<p>Because it is not currently possible to download subsetted OISST data from a GRIDDAP server I find that it is useful to include here the code one would use to load and subset downloaded OISST data. Please note that the OISST data have longitude values from 0 to 360, not -180 to 180.</p>

<pre><code class="language-r"># This function will load and subset daily data into one data.frame
# Note that the subsetting of lon/lat is done before the data are loaded
# This means it will use much less RAM and is viable for use on most laptops
# Assuming one's study area is not too large
OISST_load &lt;- function(file_name, lon1, lon2, lat1, lat2){
      OISST_dat &lt;- tidync(file_name) %&gt;%
        hyper_filter(lon = between(lon, lon1, lon2),
                     lat = between(lat, lat1, lat2)) %&gt;% 
        hyper_tibble() %&gt;% 
        select(lon, lat, time, sst) %&gt;% 
        dplyr::rename(t = time, temp = sst) %&gt;% 
        mutate(t = as.Date(t, origin = &quot;1978-01-01&quot;))
      return(OISST_dat)
}

# Locate the files that will be loaded
OISST_files &lt;- dir(&quot;~/data/OISST&quot;, full.names = T)

# Load the data in parallel
OISST_dat &lt;- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T,
                         lon1 = 260, lon2 = 280, lat1 = 30, lat2 = 50)

# This should only take a few seconds to run at most
</code></pre>

<h2 id="downloading-cci">Downloading CCI</h2>

<p>An up-and-coming star in the world of remotely sensed data products, the Climate Change Initiative (CCI) has recently been putting out some industry leading products. These are all freely available for access and use for scientific research purposes. These have quickly become regarded as the most accurate products available and their use is now encouraged over other products. Unfortunately they are not available in near-real-time and so can currently only be used for historic analyses. A recent update of these data for 2017 and 2018 was made available and one assumes that 2019 will follow suit some time by the end of 2020.</p>

<pre><code class="language-r"># The URLs where the data are housed for direct download
  # NB: Note that the versions are different; v2.1 vs. v2.0
  # NB: It looks like going straight through the thredds server is a more stable option
CCI_URL_old &lt;- &quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/esacci/sst/data/CDR_v2/Analysis/L4/v2.1&quot;
CCI_URL_new &lt;- &quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/c3s_sst/data/ICDR_v2/Analysis/L4/v2.0&quot;

# The date ranges that are housed therein
  # NB: These are historic repos and therefore the dates are static
  # I assume that the 'new' data will be updated through 2019 by the end of 2020
date_range_old &lt;- seq(as.Date(&quot;1981-09-01&quot;), as.Date(&quot;2016-12-31&quot;), by = &quot;day&quot;)
date_range_new &lt;- seq(as.Date(&quot;2017-01-01&quot;), as.Date(&quot;2018-12-31&quot;), by = &quot;day&quot;)

# The function we will use to download the data
download_CCI &lt;- function(date_choice, CCI_URL){
  
  # Prep the necessary URL pieces
  date_slash &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;/&quot;)
  date_nogap &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;&quot;)
  
  if(str_detect(CCI_URL, &quot;esacci&quot;)){
    tail_chunk &lt;- &quot;120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.nc&quot;
  } else if(str_detect(CCI_URL, &quot;c3s_sst&quot;)){
    tail_chunk &lt;- &quot;120000-C3S-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR2.0-v02.0-fv01.0.nc&quot;
  } else{
    stop(&quot;The URL structure has changed.&quot;)
  }
  
  complete_URL &lt;- paste0(CCI_URL,&quot;/&quot;,date_slash,&quot;/&quot;,date_nogap,tail_chunk)
  # Note that this will download the files to data/CCI in the root directory
  file_name &lt;- paste0(&quot;~/data/CCI/&quot;,date_nogap,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download.file(url = complete_URL, method = &quot;libcurl&quot;, destfile = file_name)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
# Most laptops have 4 cores, so 3 is a good choice
doParallel::registerDoParallel(cores = 3)

# Download all old data: 1981-09-01 to 2016-12-31
  # NB: Note the '[1:3]' below. This limits the downloads to only the first three files
  # Delete that to download everything.
  # But please do not download ALL of the files unless there is a need to do so.
plyr::l_ply(date_range_old[1:3], .fun = download_CCI, CCI_URL = CCI_URL_old, .parallel = T)

# Download all new data: 2016-01-01 to 2018-12-31
plyr::l_ply(date_range_new[1:3], .fun = download_CCI, CCI_URL = CCI_URL_new, .parallel = T)
</code></pre>

<h2 id="downloading-ostia">Downloading OSTIA</h2>

<p>As noted above, CCI data products are quickly becoming the preferred standard. Unfortunately they are not available in near-real-time. This is where OSTIA data come in to fill the gap. Though not exactly the same assimilation process as CCI, these products come from the same suite of data sources. I do not yet know if these data for 2019 onwards can be used in combination with a climatology created from the CCI data, but it is on my to do list to find out. In order to download these data one will need to have a <a href="http://marine.copernicus.eu/" target="_blank">CMEMS</a> account. This is free for researchers and very fast to sign up for. Once one has received a user name and password it is possible to use the code below to download the data via their FTP server. No Python required!</p>

<pre><code class="language-r"># The URL where the data are housed for FTP
OSTIA_URL &lt;- &quot;ftp://nrt.cmems-du.eu/Core/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2&quot;

# The date ranges that are housed therein
# NB: These are historic repos and therefore the dates are static
# I assume that the 'new' data will be updated through 2019 by the end of 2020
date_range &lt;- seq(as.Date(&quot;2019-01-01&quot;), as.Date(&quot;2019-01-03&quot;), by = &quot;day&quot;)

# Enter ones credentials here
# Note that the ':'  between 'username' and 'password' is required
user_credentials &lt;- &quot;username:password&quot;

# Download function 
download_OSTIA &lt;- function(date_choice, user_credentials){
  
  # Prep the necessary URL pieces
  date_slash &lt;- strtrim(str_replace_all(date_choice, &quot;-&quot;, &quot;/&quot;), width = 7)
  date_nogap_day &lt;- str_replace_all(date_choice, &quot;-&quot;, &quot;&quot;)
  
  tail_chunk &lt;- &quot;120000-UKMO-L4_GHRSST-SSTfnd-OSTIA-GLOB-v02.0-fv02.0.nc&quot;

  complete_URL &lt;- paste0(OSTIA_URL,&quot;/&quot;,date_slash,&quot;/&quot;,date_nogap_day,tail_chunk)
  file_name &lt;- paste0(&quot;~/data/OSTIA/&quot;,date_nogap_day,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download_ftp_file(complete_URL, file_name, verbose = TRUE, credentials = user_credentials)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
doParallel::registerDoParallel(cores = 3)

# Download data from 2019-01-01 to present day
  # NB: Some files won't download when run in parallel
  # I think the FTP server may be more aware of multiple requests than an HTTPS server
  # It may also have been due to high server use at the time, too
plyr::l_ply(date_range, .fun = download_OSTIA, .parallel = T, 
            user_credentials = user_credentials)
</code></pre>

<h2 id="downloading-glorys">Downloading GLORYS</h2>

<p>At this point one may be thinking &ldquo;wait a second, these have all been SST only products, this isn&rsquo;t really a post about environmental data!&rdquo;. But that&rsquo;s where GLORYS comes in. This product has a range of variables one may be interested in. Not just SST. But yes, they are all physical variables. No bio-geochemistry in sight. Bamboozled! But if you&rsquo;ve read this far, why stop now?! These data require a <a href="http://marine.copernicus.eu/" target="_blank">CMEMS</a> account, same as the OSTIA data. They can also be downloaded via direct FTP access, but these files are enormous so in almost every use case this is not what one is intending to do. Rather these data are almost always subsetted in some way first. Luckily CMEMS has made this available to their user base with the introduction of the <a href="https://github.com/clstoulouse/motu-client-python/releases" target="_blank">MOTU client</a>. Unluckily they have only made this available for use in Python. I have asked the people behind this process in person if there are plans for an officially supported R version and the short and long answers were no. That&rsquo;s where Mark Payne and his <a href="https://github.com/markpayneatwork/RCMEMS" target="_blank">RCMEMS</a> package enter the picture. He has wrapped the MOTU client for Python up in a handy R package that allows us to access, subset, and download CMEMS data all through the comfort of the RStudio interface! There is a tutorial on the GitHub repo that walks through the process that I show below if one would like an additional look at this process.</p>

<pre><code class="language-r"># Non-R software
# Unfortunately the subsetting of CMEMS data will require that one has Python installed
# https://www.python.org/downloads/
# Then one must download
# https://github.com/clstoulouse/motu-client-python/releases
# For instructions on how and why to properly install please see:
# https://github.com/markpayneatwork/RCMEMS

# Here is a cunning method of generating a brick of year-month values
date_range &lt;- base::expand.grid(1993:2018, 1:12) %&gt;% 
  dplyr::rename(year = Var1, month = Var2) %&gt;% 
  arrange(year, month) %&gt;% 
  mutate(year_mon = paste0(year,&quot;-&quot;,month)) %&gt;% 
  dplyr::select(year_mon)

# Download function
  # NB: This function is currently designed to subset data to a specific domain
  # Please change your lon/lat accordingly
  # NB: This function will save files to data/GLORYS in the root directory
  # To change this change the --out-dir argument near the end of the chunk of text
  # NB: This big text chunk needs to be left as one long line
  # NB: The --user and --pwd arguments need to be given the users real username and passwords
  # from their CMEMS account
download_GLORYS &lt;- function(date_choice){
  
  # The GLORYS script
    # This is a dummy script first generated by using the UI on the CMEMS website
    # No need to change anything here except for the --user and --pwd at the end
    # Please place your CMEMS username and password in those fields
  GLORYS_script &lt;- 'python ~/motuclient-python/motuclient.py --motu http://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_030-TDS --product-id global-reanalysis-phy-001-030-daily --longitude-min -180 --longitude-max 179.9166717529297 --latitude-min -80 --latitude-max 90 --date-min &quot;2018-12-25 12:00:00&quot; --date-max &quot;2018-12-25 12:00:00&quot; --depth-min 0.493 --depth-max 0.4942 --variable thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi --out-dir data --out-name test.nc --user username --pwd password'
  
  # Prep the necessary URL pieces
  date_start &lt;- parse_date(date_choice, format = &quot;%Y-%m&quot;)
  # A clever way of finding the end date of any month!
    # I found this on stackoverflow somewhere...
  date_end &lt;- date_start %m+% months(1) - 1
  
  # Cannot get data past 2018-12-25
  if(date_end &gt; as.Date(&quot;2018-12-25&quot;)) date_end &lt;- as.Date(&quot;2018-12-25&quot;)
  
  # Set the file name
  file_name &lt;- paste0(&quot;GLORYS_&quot;,date_choice,&quot;.nc&quot;)
  
  # Take the chunk of code above and turn it into something useful
  cfg &lt;- parse.CMEMS.script(GLORYS_script, parse.user = T)
  
  # This is where one should make any required changes to the subsetting of the data
  # This is now the magic of the RCMEMS package, which allows us to interface with the Python code as though it were R
  cfg_update &lt;- RCMEMS::update(cfg, variable = &quot;thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi&quot;,
                               longitude.min = &quot;-80.5&quot;,
                               longitude.max = &quot;-40.5&quot;,
                               latitude.min = &quot;31.5&quot;,
                               latitude.max = &quot;63.5&quot;,
                               date.min = as.character(date_start),
                               date.max = as.character(date_end),
                               out.name = file_name)
  
  # Download and save the file if needed
  if(file.exists(paste0(&quot;~/data/GLORYS/&quot;,file_name))){
    return()
  } else{
    CMEMS.download(cfg_update)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# I've limited the download to only 1 file
# Delete '[1]' to download everything
  #NB: The CMEMS server is a little wonky, rather not try to multicore this
plyr::l_ply(date_range$year_mon[1], .fun = download_GLORYS, .parallel = F)
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>I hope this has been a useful whack of data for anyone looking to download any of these products for their science. The techniques laid out in the code here should apply to most other data products as well as there aren&rsquo;t that many different methods of hosting data. If I&rsquo;ve missed anything that people feel is an important data source that can&rsquo;t be adapted from the code here let me know and I&rsquo;m happy to see what I can do.</p>

    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/dl_env_data_r/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2018-07-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2018-07-17 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2018-07-17
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/sa_time_survey/" itemprop="url">South Africa time survey</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective In South Africa there are a range of idioms for different time frames in which someone may (or may not) do something. The most common of these are: &lsquo;now&rsquo;, &lsquo;just now&rsquo;, and &lsquo;now now&rsquo;. If one were to Google these sayings one would find that there is general agreements on how long these time frames are, but that agreement is not absolute.
This got me to wondering just how much disagreement there may be around the country.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/sa_time_survey/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2017-12-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2017-12-08 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2017-12-08
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    0 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/transects/" itemprop="url">Transects</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    

<h2 id="preface">Preface</h2>

<p>This week I have expanded the <code>coastR</code> package with the inclusion of a function that calculates the angle of the heading for alongshore or shore-normal transects. The rest of this blog post is the vignette that I&rsquo;ve written detailing the set of this function. Next week I&rsquo;ll likely be taking a break from <code>coastR</code> development to finally create a package for the SACTN dataset. That is a project that has been in the works for a loooong time and it will be good to finally see a development release available to the public.</p>

<h2 id="overview">Overview</h2>

<p>There are a number of reasons why one would want to calculate transects along or away from a coastline. Examples include: finding the fetch across an embayment, finding the coordinates of a point 200 km from the coast, finding the appropriate series of SST pixels along/away from the coast, (or if one is feeling particular feisty) the creation of shape files for a given area away from the coast. The function that we will be introducing here does none of these things. What the <code>transects()</code> function does do is calculate the angle of the heading along or away from the coast against true North, which is then the basis for all of the other fancy things one may want to do. Baby steps people. Baby steps.</p>

<pre><code class="language-r"># devtools::install_github(&quot;robwschlegel/coastR&quot;) # Install coastR
library(coastR)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(geosphere)
</code></pre>

<h2 id="sample-locations">Sample locations</h2>

<p>For this vignette we will re-use the same coastlines as those created for the sequential sites vignette. The ordering of the sites remains jumbled up to demonstrate that <code>transects()</code> does not require orderly data. Should one want to order ones site list before calculating transect headings it is possible to do so with <code>seq_sites()</code>. This is of course a recommended step in any workflow.</p>

<pre><code class="language-r"># Cape Point, South Africa
cape_point &lt;- SACTN_site_list %&gt;% 
  slice(c(31, 22, 26, 17, 19, 21, 30)) %&gt;% 
  mutate(order = 1:n())

# South Africa
south_africa &lt;- SACTN_site_list %&gt;% 
  slice(c(1,34, 10, 20, 50, 130, 90)) %&gt;% 
  mutate(order = 1:n())

# Baja Peninsula, Mexico
baja_pen &lt;- data.frame(
  order = 1:7,
  lon = c(-116.4435, -114.6800, -109.6574, -111.9503, -112.2537, -113.7918, -114.1881),
  lat = c(30.9639, 30.7431, 22.9685, 26.9003, 25.0391, 29.4619, 28.0929)
)

# Bohai Sea, China
bohai_sea &lt;- data.frame(
  order = 1:7,
  lon = c(122.0963, 121.2723, 121.0687, 121.8742, 120.2962, 117.6650, 122.6380),
  lat = c(39.0807, 39.0086, 37.7842, 40.7793, 40.0691, 38.4572, 37.4494)
)
</code></pre>

<h2 id="transects">Transects</h2>

<p>With our site lists created we now want to see what the correct headings for alongshore and shore-normal transects are for our sites. We will also demonstrate what happens when we increase the <code>spread</code> used in the calculation and also how the inclusion of island masks affects the angle of the headings.</p>

<pre><code class="language-r"># Cape Point, South Africa
cape_point_along &lt;- transects(cape_point, alongshore = T)
cape_point_away &lt;- transects(cape_point)

# South Africa
south_africa_along &lt;- transects(south_africa, alongshore = T)
south_africa_away &lt;- transects(south_africa)
  # NB: Note here the use of the `spread` argument
south_africa_along_wide &lt;- transects(south_africa, alongshore = T, spread = 30)
south_africa_away_wide &lt;- transects(south_africa, spread = 30)

# Baja Peninsula, Mexico
baja_pen_along &lt;- transects(baja_pen, alongshore = T)
baja_pen_away &lt;- transects(baja_pen)
  # NB: Note here the use of the `coast` argument
baja_pen_island &lt;- transects(baja_pen, coast = FALSE)

# Bohai sea, China
bohai_sea_along &lt;- transects(bohai_sea, alongshore = T)
bohai_sea_away &lt;- transects(bohai_sea)
</code></pre>

<h2 id="visualise">Visualise</h2>

<p>Now that the correct headings have been calculated for our alongshore and shore-normal transects let&rsquo;s visualise them with ggplot. First we will create a function that does this in order to keep the length of this vignette down.</p>

<pre><code class="language-r"># Create base map
world_map &lt;- ggplot() + 
  borders(fill = &quot;grey40&quot;, colour = &quot;black&quot;)

# Create titles
titles &lt;- c(&quot;Alongshore&quot;, &quot;Shore-normal&quot;, &quot;Islands&quot;)

# Plotting function
plot_sites &lt;- function(site_list, buffer, title_choice, dist){
  
  # Find the point 200 km from the site manually to pass to ggplot
  heading2 &lt;- data.frame(geosphere::destPoint(p = select(site_list, lon, lat),  
                                              b = site_list$heading, d = dist))
  
  # Add the new coordinates tot he site list
  site_list &lt;- site_list %&gt;% 
    mutate(lon_dest = heading2$lon,
           lat_dest = heading2$lat)
  
  # Visualise
  world_map +
    geom_segment(data = site_list, colour = &quot;red4&quot;, 
                 aes(x = lon, y = lat, xend = lon_dest, yend = lat_dest)) +
    geom_point(data = site_list, size = 3, colour = &quot;black&quot;, aes(x = lon, y = lat)) +
    geom_point(data = site_list, size = 3, colour = &quot;red&quot;, aes(x = lon_dest, y = lat_dest)) +
    coord_cartesian(xlim = c(min(site_list$lon - buffer), 
                             max(site_list$lon + buffer)),
                    ylim = c(min(site_list$lat - buffer), 
                             max(site_list$lat + buffer))) +
    labs(x = &quot;&quot;, y = &quot;&quot;, colour = &quot;Site\norder&quot;) +
    ggtitle(titles[title_choice])
}
</code></pre>

<h3 id="cape-point-south-africa">Cape Point, South Africa</h3>

<p>The <code>transect()</code> function is designed to work well at small scales by default. We may see this here with the effortlessness of plotting transects around a peninsula and then across an embayment in one go.</p>

<pre><code class="language-r">cape_point_along_map &lt;- plot_sites(cape_point_along, 0.5, 1, 10000)
cape_point_away_map &lt;- plot_sites(cape_point_away, 0.5, 2, 10000)
grid.arrange(cape_point_along_map, cape_point_away_map, nrow = 1)
</code></pre>

<div class="figure">
<img src="/post/transects_files/figure-html/cape_point_trans-1.png" alt="Alongshore and shore-normal transects around Cape Point and False Bay, South Africa." width="960" />
<p class="caption">(\#fig:cape_point_trans)Alongshore and shore-normal transects around Cape Point and False Bay, South Africa.</p>
</div>

<h3 id="south-africa">South Africa</h3>

<p>The intentions one may have for calculating shore-normal transects will differ depending on ones research question. If one is interested in visualising the convolutions of a coastline at a sub-meso-scale then the default <code>spread</code> of the <code>transect()</code> function is probably the way to go, as shown above. If however one is interested in seeing the shore-normal transects broadly for the coastline of an entire country it is likely that one will want to greatly expand the <code>spread</code> of coastline used to calculate said transects. In the figure below we may see how changing the <code>spread</code> of the coastline considered for the transects changes the results. The top row shows the transects resulting from the narrow default <code>spread</code>, while the bottom row shows the results of using a much wider <code>spread</code> for the calculation. Note particularly how the transect changes at St. Helena Bay and Gansbaai (second and fourth sites from the top left), as well as a general smoothing of all of the other transects. This is due to the sensitivity of the function. The St. Helena Bay and Gansbaai sites lay within embayments; therefore, the shore-normal transects that would come out directly from these sites will not follow the general contour of the coastline of South Africa. Should we be interested in the &ldquo;bigger picture&rdquo; we must increase the <code>spread</code> argument in <code>transects()</code>. This may require some trial and error for particularly difficult coastlines before a satisfactory result is produced, but it is certainly still faster than running the calculations by hand. Should small scale accuracy along part of the coast, and broader accuracy elsewhere be required, one must simply divide the site list into the different sections and run <code>transects()</code> on each subset with the desired <code>spread</code>.</p>

<pre><code class="language-r">south_africa_along_map &lt;- plot_sites(south_africa_along, 1, 1, 100000)
south_africa_away_map &lt;- plot_sites(south_africa_away, 1, 2, 100000)
south_africa_along_wide_map &lt;- plot_sites(south_africa_along_wide, 1, 1, 100000)
south_africa_away_wide_map &lt;- plot_sites(south_africa_away_wide, 1, 2, 100000)
grid.arrange(south_africa_along_map, south_africa_away_map, 
             south_africa_along_wide_map, south_africa_away_wide_map, nrow = 2)
</code></pre>

<div class="figure">
<img src="/post/transects_files/figure-html/south_africa_trans-1.png" alt="Alongshore and shore-normal transects around all of South Africa." width="864" />
<p class="caption">(\#fig:south_africa_trans)Alongshore and shore-normal transects around all of South Africa.</p>
</div>

<h3 id="baja-peninsula-mexico">Baja Peninsula, Mexico</h3>

<p>In the following figure we see how the inclusion of islands affects the results of our transects. The first site up from the tip of the peninsula on the left-hand side is on an island. Note the minor adjustment to the transect when the island mask is used for the calculation. In this case it&rsquo;s not large, but in other instances it may be massive. By default island masks are removed and it is our advice that they not be used unless extreme caution is observed.</p>

<pre><code class="language-r">baja_pen_along_map &lt;- plot_sites(baja_pen_along, 1, 1, 100000)
baja_pen_away_map &lt;- plot_sites(baja_pen_away, 1, 2, 100000)
baja_pen_island_map &lt;- plot_sites(baja_pen_island, 1, 3, 100000)
grid.arrange(baja_pen_along_map, baja_pen_away_map, baja_pen_island_map, nrow = 1)
</code></pre>

<div class="figure">
<img src="/post/transects_files/figure-html/baja_pen_trans-1.png" alt="Alongshore and shore-normal transects around the Baja Peninsula." width="960" />
<p class="caption">(\#fig:baja_pen_trans)Alongshore and shore-normal transects around the Baja Peninsula.</p>
</div>

<h3 id="bohai-sea-china">Bohai Sea, China</h3>

<p>This figure serves as a good visualisation for just how localised the coastline is that is used to calculate the shore-normal transects. Note how the alongshore transects look a little dodgy, but when shown as shore-normal transects everything works out. This is something to consider if one is interested in calculating alongshore transects rather than shore-normal transects. For alongshore transects that show more fidelity for coastal direction it is advisable to increase the <code>spread</code> argument.</p>

<pre><code class="language-r">bohai_sea_along_map &lt;- plot_sites(bohai_sea_along, 1, 1, 70000)
bohai_sea_away_map &lt;- plot_sites(bohai_sea_away, 1, 2, 70000)
grid.arrange(bohai_sea_along_map, bohai_sea_away_map, nrow = 1)
</code></pre>

<div class="figure">
<img src="/post/transects_files/figure-html/bohai_sea_trans-1.png" alt="Alongshore and shore-normal transects within the Bohai Sea." width="960" />
<p class="caption">(\#fig:bohai_sea_trans)Alongshore and shore-normal transects within the Bohai Sea.</p>
</div>

<h2 id="conclusion">Conclusion</h2>

<p>As we may see in the previous example figures, the <code>transect()</code> function tends to work better by default at smaller scales. This was an intentional decision as it is much more accurate when scaling the function up for larger coastal features than when scaling it down for smaller ones.</p>

<p>The calculation of the heading for alongshore and shore-normal transects is rarely the end goal itself. One then generally wants to find specific points from the coastline along the transects that have been determined. This is done in the code above within the <code>plot_sites()</code> function created within this vignette, but the process is not detailed specifically. How to do more elaborate things with transects will be explained with the following functions to be added to <code>coastR</code>. This will include how to draw coastal polygons based on distance and bathymetry.</p>

    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/transects/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2017-08-23 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2017-08-23 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2017-08-23
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/polar_plot_clims/" itemprop="url">Polar plot climatologies</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective Whilst cruising about on Imgur I found a post about science stuff. Not uncommon, which is nice. These sorts of grab-bag posts about nothing in particular often include some mention of climate science, almost exclusively some sort of clever visualisation of a warming planet. That seems to be what people are most interested in. I&rsquo;m not complaining though, it keeps me employed. The aforementioned post caught my attention more than usual because it included a GIF, and not just a static picture of some sort of blue thing that is becoming alarmingly red (that was not meant to be a political metaphor).
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/polar_plot_clims/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2017-08-23 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2017-08-23 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2017-08-23
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/seq_sites/" itemprop="url">Sequential sites</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Preface The rest of the blog post after this preface section is a copy of the vignette I&rsquo;ve written for the first function in the new package I am developing: coastR. This package aims to provide functions that are useful for coastal oceanography but that do not yet exist in the R language. It is not my intention to provide algorithms for physical oceanography as these may already be found elsewhere.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/seq_sites/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  
    
    
      

<div class="card-simple" itemscope itemprop="blogPost" itemtype="http://schema.org/BlogPosting">
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>
  

  <span class="article-date">
    
    <meta content="2017-07-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2017-07-17 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      2017-07-17
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Robert William Schlegel">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://theoceancode.netlify.app/categories/r/">R</a>
    
  </span>
  
  

  

</div>


  
  
  <h3 class="article-title" itemprop="headline">
    <a href="https://theoceancode.netlify.app/post/mapping_with_ggplot2/" itemprop="url">Mapping with ggplot2</a>
  </h3>
  <div class="article-style" itemprop="articleBody">
    
    Objective There are many different things that require scientists to use programming languages (like R). Far too many to count here. There is however one common use amongst almost all environmental scientists: mapping. Almost every report, research project or paper will have need to refer to a study area. This is almost always &ldquo;Figure 1&rdquo;. To this end, whenever I teach R, or run workshops on it, one of the questions I am always prepared for is how to create a map of a particular area.
    
  </div>
  <p class="read-more" itemprop="mainEntityOfPage">
    <a href="https://theoceancode.netlify.app/post/mapping_with_ggplot2/" class="btn btn-primary btn-outline">
      CONTINUE READING
    </a>
  </p>
</div>

    
  

  
<nav>
  <ul class="pager">
    
    
    <li><a href="/post/page/2/">&gt;</a></li>
    
  </ul>
</nav>



</div>
<footer class="site-footer">
  <div class="container">

    
    <p class="powered-by">
      <a href="https://theoceancode.netlify.app/privacy/">Privacy Policy</a>
    </p>
    

    <p class="powered-by">

      &copy; 2019 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    

    
    
    <script>
      const search_index_filename = "/search.json";
      const i18n = {
        'placeholder': "Search...",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

