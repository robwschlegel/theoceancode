<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Ocean Code on The Ocean Code</title>
    <link>https://theoceancode.netlify.com/</link>
    <description>Recent content in The Ocean Code on The Ocean Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Fri, 21 May 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ODV figures in R with bathymetry</title>
      <link>https://theoceancode.netlify.com/post/odv_bathy/</link>
      <pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/odv_bathy/</guid>
      <description>
&lt;script src=&#34;https://theoceancode.netlify.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;objective&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;Nearly four years after writing a blog post about &lt;a href=&#34;https://theoceancode.netlify.app/post/odv_figures/&#34;&gt;recreating R figures in ODV&lt;/a&gt; I had someone reach out to me expressing interest in adding a bathymetry layer over the interpolated data. It’s always nice to know that these blog posts are being found useful for other researchers. And I have to admit I’m a bit surprised that the code still runs 4 years later. Especially considering that it uses the &lt;strong&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/strong&gt; which is notorious for breaking backwards compatibility. In order to demonstrate the overlaying of bathymetry data on a CTD transect we will need to use a different dataset than in the previous blog post. One may use any data one would like, but for this blog I went to this &lt;a href=&#34;https://robert-schlegel.shinyapps.io/CTD_project/&#34;&gt;shiny app&lt;/a&gt; to extract some data from the coast of South Africa. Specifically I filtered for temperature data from November 1990 at all depths. We won’t go back over the theory for recreating the ODV figure in this blog post, so please revisit that for a recap as necessary. Below I will show two of the necessary steps to get interpolated CTD data before we begin on the bathymetry mask.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(tidyverse)
library(lubridate)
library(reshape2)
library(MBA)
library(mgcv)
library(marmap)
library(FNN)

# The transects in this dataset do not have an ID column
# So we manually select the first transect (rows 1 - 12)
# This is then used as a mask to select all depths for these pixels
# We will also use these unique lon/lat coords for bathymetry points
ctd_mask &amp;lt;- read_csv(&amp;quot;../../static/data/CTD_transect.csv&amp;quot;) %&amp;gt;% 
  select(lon, lat) %&amp;gt;% 
  slice(1:12) %&amp;gt;% 
  unique()

# Load and screen data
  # For ease I am only using monthly means
  # and depth values rounded to 10 metres
ctd &amp;lt;- read_csv(&amp;quot;../../static/data/CTD_transect.csv&amp;quot;) %&amp;gt;% 
  mutate(depth = -depth) %&amp;gt;%  # Correct for plotting
  right_join(ctd_mask, by = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;)) %&amp;gt;% 
  select(lon, lat, depth, temp)

# Manually extracted hexidecimal ODV colour palette
ODV_colours &amp;lt;- c(&amp;quot;#feb483&amp;quot;, &amp;quot;#d31f2a&amp;quot;, &amp;quot;#ffc000&amp;quot;, &amp;quot;#27ab19&amp;quot;, &amp;quot;#0db5e6&amp;quot;, &amp;quot;#7139fe&amp;quot;, &amp;quot;#d16cfa&amp;quot;)

# Create quick scatterplot
ggplot(data = ctd, aes(x = lon, y = depth)) +
  geom_point(aes(colour = temp)) +
  scale_colour_gradientn(colours = rev(ODV_colours)) +
  labs(y = &amp;quot;depth (m)&amp;quot;, x = &amp;quot;longitude (°E)&amp;quot;, colour = &amp;quot;temp. (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/post/odv_bathy_files/figure-html/setup-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: A non-interpolated scatterplot of our temperature (°C) data shown as a function of depth (m) over longitude (°E).&lt;/p&gt;
&lt;p&gt;It looks like we have a nice little upwelling signal coming through at the coast. It will be interesting to see how the interpolation handles that. We’ll quickly run the interpolation and then get to the bathymetry overlay. Note that these data are not on a straight latitude transect, but we are not going to worry about that in this blog post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now we may interpolate the data
# NB: The columns that mba.surf() will interpolate are the X, Y, Z values in that order
ctd_mba &amp;lt;- mba.surf(ctd[c(&amp;quot;lon&amp;quot;, &amp;quot;depth&amp;quot;, &amp;quot;temp&amp;quot;)], no.X = 300, no.Y = 300, extend = T)
dimnames(ctd_mba$xyz.est$z) &amp;lt;- list(ctd_mba$xyz.est$x, ctd_mba$xyz.est$y)
ctd_mba &amp;lt;- melt(ctd_mba$xyz.est$z, varnames = c(&amp;#39;lon&amp;#39;, &amp;#39;depth&amp;#39;), value.name = &amp;#39;temp&amp;#39;) %&amp;gt;% 
  filter(depth &amp;lt; 0) %&amp;gt;% 
  mutate(temp = round(temp, 1))

# Finally we create our gridded result
ggplot(data = ctd_mba, aes(x = lon, y = depth)) +
  geom_raster(aes(fill = temp)) +
  geom_contour(aes(z = temp), binwidth = 2, colour = &amp;quot;black&amp;quot;, alpha = 0.2) +
  geom_contour(aes(z = temp), breaks = 20, colour = &amp;quot;black&amp;quot;) +
  scale_fill_gradientn(colours = rev(ODV_colours)) +
  labs(y = &amp;quot;depth (m)&amp;quot;, x = &amp;quot;longitude (°E)&amp;quot;, fill = &amp;quot;temp. (°C)&amp;quot;) +
  coord_cartesian(expand = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/post/odv_bathy_files/figure-html/interp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: The same temperature (°C) profiles seen in Figure 1 with the missing values filled in with multilevel B-splines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bathymetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bathymetry&lt;/h2&gt;
&lt;p&gt;The interpolation seems to have done a decent job of acknowledging the upwelling signal. It may be possible to tweak the interpolation more, as desired, but I’m happy enough with it for the purposes of this post. We are going to skip over the step to cut out the artefacts at the bottom of the figure where there are not data points because we are rather going to just overlay our bathymetry mask. First we will download the bathymetry data. Then we find the points that are closest to our CTD transect. These will then be used for the grey overlay at the bottom of the figure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bathymetry within transect bounding box
bathy &amp;lt;- getNOAA.bathy(lon1 = min(ctd$lon), lon2 = max(ctd$lon), 
                       lat1 = min(ctd$lat), lat2 = max(ctd$lat), 
                       resolution = 5) # Larger numbers for coarser data

# Convert bathy object to a data.frame for easier use
bathy_df &amp;lt;- data.frame(lon_b = as.numeric(rownames(bathy)),
                       lat_b = as.numeric(colnames(bathy)),
                       depth = as.numeric(bathy)) %&amp;gt;% 
  mutate(bathy_idx = 1:n()) # Used for merging CTD and bathy data

# Find nearest points to transect data
ctd_mask &amp;lt;- ctd_mask %&amp;gt;% 
  mutate(bathy_idx = as.vector(knnx.index(as.matrix(bathy_df[,c(&amp;quot;lon_b&amp;quot;, &amp;quot;lat_b&amp;quot;)]),
                                           as.matrix(ctd_mask[,c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;)]), k = 1))) %&amp;gt;% 
  left_join(bathy_df, by = &amp;quot;bathy_idx&amp;quot;)

# Manually create bottom of the bathy mask polygon
bathy_mask &amp;lt;- data.frame(lon = c(ctd_mask$lon, rev(ctd_mask$lon)),
                         depth = c(ctd_mask$depth, rep(min(ctd_mask$depth), nrow(ctd_mask))))

# We may now use that bathy mask for our final figure
ggplot(data = ctd_mba, aes(x = lon, y = depth)) +
  geom_raster(aes(fill = temp)) +
  geom_contour(aes(z = temp), binwidth = 2, colour = &amp;quot;black&amp;quot;, alpha = 0.2) +
  geom_contour(aes(z = temp), breaks = 20, colour = &amp;quot;black&amp;quot;) +
  geom_polygon(data = bathy_mask, fill = &amp;quot;grey80&amp;quot;, colour = &amp;quot;black&amp;quot;) +
  scale_fill_gradientn(colours = rev(ODV_colours)) +
  labs(y = &amp;quot;depth (m)&amp;quot;, x = &amp;quot;longitude (°E)&amp;quot;, fill = &amp;quot;temp. (°C)&amp;quot;) +
  coord_cartesian(expand = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/post/odv_bathy_files/figure-html/bathy-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: The same temperature (°C) profiles seen in Figure 2 with the bathymetry values overlaid.&lt;/p&gt;
&lt;p&gt;Unfortunately in this example there is a bit of blank space in the bottom left of the plot because the CTD casts do not go deeper than 200 m, and the interpolation doesn’t fill in values outside of the rectangular box dictated by the X (lon) and Y (depth) values. A cheeky workaround for this issue would be to simply crop the figure to the bottom of the interpolated data, and not the bathymetry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We may now use that bathy mask for our final figure
ggplot(data = ctd_mba, aes(x = lon, y = depth)) +
  geom_raster(aes(fill = temp)) +
  geom_contour(aes(z = temp), binwidth = 2, colour = &amp;quot;black&amp;quot;, alpha = 0.2) +
  geom_contour(aes(z = temp), breaks = 20, colour = &amp;quot;black&amp;quot;) +
  geom_polygon(data = bathy_mask, fill = &amp;quot;grey80&amp;quot;, colour = &amp;quot;black&amp;quot;) +
  geom_point(data = ctd, aes(x = lon, y = depth),
             colour = &amp;#39;black&amp;#39;, size = 0.2, alpha = 0.4, shape = 8) +
  scale_fill_gradientn(colours = rev(ODV_colours)) +
  labs(y = &amp;quot;depth (m)&amp;quot;, x = &amp;quot;longitude (°E)&amp;quot;, fill = &amp;quot;temp. (°C)&amp;quot;) +
  coord_cartesian(expand = F, ylim = c(-200, 0)) +
  theme(panel.border = element_rect(fill = NA, colour = &amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/post/odv_bathy_files/figure-html/bathy-crop-1.png&#34; width=&#34;672&#34; /&gt;
&lt;strong&gt;Figure 4&lt;/strong&gt;: The plotting area cropped to the interpolated data, rather than the bathymetry mask. Also shown with black dots are the original CTD data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this tutorial we have seen how to plot a bathymetry overlay that matches the lon/lat coordinates of the CTD casts. I’m sure there is a way to force the interpolation to fill in values at a greater depth to match the bathymetry, but the focus of this blog was on adding the bathymetry mask itself, and I think we have addressed this issue. The workflow outlined above has a couple of bumps in it, but should be adaptable to a range of applications.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of Bio-Oracle data</title>
      <link>https://theoceancode.netlify.com/post/bo_analysis/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/bo_analysis/</guid>
      <description>


&lt;div id=&#34;objective&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;While running some brief quality control tests on Bio-Oracle layers before using them for a recent project it was detected that some of the layers in the current version of the Bio-Oracle product appear to have very large errors. Specifically the error is that there are layers where the minimum values are greater than the maximum values. It is unclear how this could be possible, so in the following text and code we will look into how we go about investigating these data layers and we will discuss which layers are fine, and which are not. This error was first detected in the current velocity layers but a brief search turned up errors in other layers, too. So in this post we will be going through each individual layer to test for this max less than min error. We will look at all of the different depths as well as the future projections.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load required libraries
library(tidyverse)
library(sdmpredictors)

# The possible layers for download from Bio-Oracle
BO_layers &amp;lt;- list_layers(datasets = &amp;quot;Bio-ORACLE&amp;quot;)

# The future layers
BO_layers_future &amp;lt;- list_layers_future(datasets = &amp;quot;Bio-ORACLE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-pipeline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing pipeline&lt;/h2&gt;
&lt;p&gt;The following code chunk contains a function that will run the testing pipeline that highlights any errors in the data. To use it we choose a variable from the list of Bio-Oracle variables shown above that have a max and min version of the layer. One must replace the ‘max’ or ‘min’ with ‘X’ and give that to the function, it will do the rest. Note that if one is running this script the figures this function will save to disk take about 1 minute to render due to their high resolution. Also please note that this function assumes there is a “figures” folder in the root directory on the computer on which this code is being run. If not, one must be created or the function must be changed to point to the desired folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BO_test &amp;lt;- function(var_name, scenario = &amp;quot;present&amp;quot;, year = NA){
  
  # Establish min/max layer names
  min_layer &amp;lt;- gsub(&amp;quot;X&amp;quot;, &amp;quot;min&amp;quot;, var_name)
  max_layer &amp;lt;- gsub(&amp;quot;X&amp;quot;, &amp;quot;max&amp;quot;, var_name)
  
  # Download data
  if(scenario == &amp;quot;present&amp;quot;){
    BO_layers_dl &amp;lt;- load_layers(c(min_layer, max_layer))
    var_title &amp;lt;- var_name
  } else {
    BO_layer_names &amp;lt;- get_future_layers(c(min_layer, max_layer), scenario = scenario, year = year)
    BO_layers_dl &amp;lt;- load_layers(BO_layer_names$layer_code)
    var_title &amp;lt;- gsub(&amp;quot;max_&amp;quot;, &amp;quot;X_&amp;quot;, BO_layer_names$layer_code[1])
  }
  
  # Prepare data for plotting
  BO_layers_test &amp;lt;- as.data.frame(BO_layers_dl, xy = T) %&amp;gt;% 
    dplyr::rename(lon = x, lat = y) %&amp;gt;% 
    mutate(lon = round(lon, 4), 
           lat = round(lat, 4)) %&amp;gt;% 
    na.omit() %&amp;gt;% 
    `colnames&amp;lt;-`(c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;min_val&amp;quot;, &amp;quot;max_val&amp;quot;)) %&amp;gt;% 
    mutate(max_min = ifelse(max_val &amp;gt;= min_val, TRUE, FALSE))
  
  # Visualise pixels where min values are greater than the max
  test_plot &amp;lt;- ggplot(data = BO_layers_test, aes(x = lon, y = lat)) +
    geom_raster(aes(fill = max_min)) +
    coord_quickmap(expand = F) +
    labs(fill = &amp;quot;Max greater than min&amp;quot;, x = NULL, y = NULL, title = var_title) +
    theme(legend.position = &amp;quot;bottom&amp;quot;)
  
  # Save figure to disk
  ggsave(paste0(&amp;quot;~/figures/&amp;quot;,var_title,&amp;quot;.png&amp;quot;), test_plot, height = 5, width = 8)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;look-at-layers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Look at layers&lt;/h2&gt;
&lt;p&gt;In this section we will go through all of the BO layers that have a min/max option and we will compare them to ascertain whether the maximum values are always greater than the minimums, which they should be, but we have found that sometimes this is not the case. When possible we will also look at future projections of the layers with RCP8.5 at 2050 and 2100. In the first code chunk in this section we will look at the older Bio-Oracle layers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Up first we start with the older Bio-Oracle layers

# Bathymetry
BO_test(&amp;quot;BO_bathyX&amp;quot;) # No issues

# Chlorophyll
BO_test(&amp;quot;BO_chloX&amp;quot;) # No issues

# Cloud fraction
BO_test(&amp;quot;BO_cloudX&amp;quot;) # No issues

# Diffuse attenuation
BO_test(&amp;quot;BO_daX&amp;quot;) # No issues

# SST
BO_test(&amp;quot;BO_sstX&amp;quot;) # No issues&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is reassuring to see that all of the older BO layers have no issues in them. From my initial testing it looked like the layers with errors may have been from data assimilation from the GLORYS product for the most recent Bio-Oracle layers. That the older layers have no issues appears to support the hypothesis that the bug in the Bio-Oracle pipeline was introduced in the BO2 version of the product.&lt;/p&gt;
&lt;p&gt;The next code chunk goes through all of the newer layers and where possible the future projections, too. Note that many layers have four different depth options. The surface (ss), the min (bdmin), the mean (bdmean), and the max (bdmax) depths present at each pixel. We are testing all of these as I have hypothesised that the inclusion of these three different depths may be responsible for some of the errors observed. Another distinction to make for the following tests is that there are min/max values for each layer, which take the absolute min/max recorded at a pixel. And then there are the long-term min/max values, which are the average annual min/max recorded over the length of the available data. The long-term min/max values are more representative of the climatological means within an area, and the absolute min/max are representative of the most extreme events that may occur in an area. Generally one is going to be more interested in the long-term values for normal species distribution modelling (SDM) applications. Because these min/max values are calculated differently it is necessary to test both of them to see if the errors in the data differ in any discernible way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Carbon phytoplankton biomass absolute
BO_test(&amp;quot;BO2_carbonphytoX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoX_ss&amp;quot;) # No issues

# Carbon phytoplankton biomass long-term
BO_test(&amp;quot;BO2_carbonphytoltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_carbonphytoltX_ss&amp;quot;) # No issues

# Chlorophyll absolute
BO_test(&amp;quot;BO2_chloX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Most pixels fail
BO_test(&amp;quot;BO2_chloX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Slightly better than the 2050 data

# Chlorophyll long-term
BO_test(&amp;quot;BO2_chloltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloltX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_chloltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Same issues as absolute layer
BO_test(&amp;quot;BO2_chloltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Slightly better than the 2050 data

# Current velocities absolute
BO_test(&amp;quot;BO2_curvelX_bdmax&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmean&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmin&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelX_ss&amp;quot;) # Mostly fails around the equator
BO_test(&amp;quot;BO2_curvelX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Mirror image errors of present day
BO_test(&amp;quot;BO2_curvelX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Mirror image errors of present day

# Current velocities long-term
# The first issue noted in the BO2 layers were these
BO_test(&amp;quot;BO2_curvelltX_bdmax&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmean&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmin&amp;quot;) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_curvelltX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_curvelltX_ss&amp;quot;) # Mostly fails around the equator
BO_test(&amp;quot;BO2_curvelltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Mirror image errors of present day
BO_test(&amp;quot;BO2_curvelltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Mirror image errors of present day

# Dissolved oxygen
BO_test(&amp;quot;BO2_dissoxX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxX_ss&amp;quot;) # No issues

# Dissolved oxygen long-term
BO_test(&amp;quot;BO2_dissoxltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_dissoxltX_ss&amp;quot;) # No issues

# Ice cover
BO_test(&amp;quot;BO2_icecoverX_ss&amp;quot;) # No issues

# Ice cover long-term
BO_test(&amp;quot;BO2_icecoverltX_ss&amp;quot;) # No issues

# Ice thickness
BO_test(&amp;quot;BO2_icethickX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_icethickX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # All ice layers areas appear to be wrong
BO_test(&amp;quot;BO2_icethickX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # All ice layers areas appear to be wrong

# Ice thickness long-term
BO_test(&amp;quot;BO2_icethickltX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_icethickltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # All ice layers areas appear to be wrong
BO_test(&amp;quot;BO2_icethickltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # All ice layers areas appear to be wrong

# Iron
BO_test(&amp;quot;BO2_ironX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironX_ss&amp;quot;) # No issues

# Iron long-term
BO_test(&amp;quot;BO2_ironltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ironltX_ss&amp;quot;) # No issues

# Light at bottom
BO_test(&amp;quot;BO2_lightbotX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_lightbotX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_lightbotX_bdmin&amp;quot;) # No issues

# Light at bottom long-term
BO_test(&amp;quot;BO2_lightbotltX_bdmax&amp;quot;) # Global errors different from current velocity errors
BO_test(&amp;quot;BO2_lightbotltX_bdmean&amp;quot;) # Global errors different from current velocity errors
BO_test(&amp;quot;BO2_lightbotltX_bdmin&amp;quot;) # Global errors different from current velocity errors

# Nitrate
BO_test(&amp;quot;BO2_nitratemax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitratemax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitratemax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitratemax_ss&amp;quot;) # No issues

# Nitrate long-term
BO_test(&amp;quot;BO2_nitrateltmax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitrateltmax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitrateltmax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_nitrateltmax_ss&amp;quot;) # No issues

# Phosphate absolute
BO_test(&amp;quot;BO2_phosphateX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateX_ss&amp;quot;) # No issues

# Phosphate long-term
BO_test(&amp;quot;BO2_phosphateltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_phosphateltX_ss&amp;quot;) # No issues

# Primary production absolute
BO_test(&amp;quot;BO2_ppmax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppmax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppmax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppmax_ss&amp;quot;) # No issues

# Primary production long-term
BO_test(&amp;quot;BO2_ppltmax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppltmax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppltmax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_ppltmax_ss&amp;quot;) # No issues

# Salinity absolute
BO_test(&amp;quot;BO2_salinityX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Most pixels fail
BO_test(&amp;quot;BO2_salinityX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_salinityX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail

# Salinity long-term
BO_test(&amp;quot;BO2_salinityltX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityltX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_salinityltX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_salinityltX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityltX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_salinityltX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_salinityltX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityltX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Global issues
BO_test(&amp;quot;BO2_salinityltX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Global issues
BO_test(&amp;quot;BO2_salinityltX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_salinityltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_salinityltX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail

# Silicate absolute
BO_test(&amp;quot;BO2_silicatemax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicatemax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicatemax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicatemax_ss&amp;quot;) # No issues

# Silicate long-term
BO_test(&amp;quot;BO2_silicateltmax_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicateltmax_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicateltmax_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_silicateltmax_ss&amp;quot;) # No issues

# Temperature absolute
BO_test(&amp;quot;BO2_tempX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_tempX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_tempX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_tempX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_tempX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_tempX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # All pixels fail
BO_test(&amp;quot;BO2_tempX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # All pixels fail

# Temperature long-term
BO_test(&amp;quot;BO2_templtX_bdmax&amp;quot;) # No issues
BO_test(&amp;quot;BO2_templtX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_bdmax&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_bdmean&amp;quot;) # No issues
BO_test(&amp;quot;BO2_templtX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_bdmean&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_bdmin&amp;quot;) # No issues
BO_test(&amp;quot;BO2_templtX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_bdmin&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # Almost all pixels fail
BO_test(&amp;quot;BO2_templtX_ss&amp;quot;) # No issues
BO_test(&amp;quot;BO2_templtX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2050) # All but a few pixels fail
BO_test(&amp;quot;BO2_templtX_ss&amp;quot;, scenario = &amp;quot;RCP85&amp;quot;, year = 2100) # All but a few different pixels fail&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;This is not meant to be exhaustive, but does represent the majority of the data layers offered by Bio-Oracle. I had hypothesised that the three different depth options per pixel would have been related to the issues in the data assimilation but it does not appear to be the case. I’ve concluded this because whenever a depth layer has issues, those errors appear to be very similar across the three different depth layers. One consistent pattern we do see is if there are any errors, then every depth layer for that variable will also have errors. This also applies to absolute vs. long-term max/min layers. Errors in one means errors in all. Another consistent pattern in the error was that all of the future projections at all depths for absolute and long-term min/max values had errors in them. This begs the question of how it is that the future projection can have no errors, while the present day layers do not. How are these future projection layers calculated differently from the present day? Are they not based on the same data?&lt;/p&gt;
&lt;p&gt;Most of the layers that have issues are physical layers. It is my understanding that these layers would have been adapted from the GLORYS reanalysis product. Therefore the next logical step in understanding this issue would be to investigate the GLORYS data. But even if there were issues in the GLORYS product, which I doubt, it would not explain how the data layers here could have minimum values being reported as greater than the maximum values in the distribution. The only thing that makes any sense is that the data layers are created independently of each other. But why?&lt;/p&gt;
&lt;p&gt;Without being able to see the pipeline code myself all I can do is ponder, which isn’t terribly useful. So to wrap things up I’ll provide two tables; the layers with no issues, and those with issues. I would strongly recommend against using any data layers that did not pass the tests in this analysis until the curators of the Bio-Oracle data address these issues in a future release/version.&lt;/p&gt;
&lt;p&gt;Layers with no issues (fine for use):
- All of the older BO layers appear fine
- Carbon phytoplankton biomass absolute and long-term at all depths
- Chlorophyll absolute and long-term at all depths for present day projections only
- Dissolved oxygen absolute and long-term at all depths
- Ice cover absolute and long-term
- Ice thickness absolute and long-term for present day projections only
- Iron absolute and long-term for all depths
- Nitrate absolute and long-term for all depths
- Phosphate absolute and long-term for all depths
- Primary productivity absolute and long-term for all depths
- Salinity absolute and long-term at all depths for present day projections only
- Temperature absolute and long-term at all depths for present day projections only&lt;/p&gt;
&lt;p&gt;Problem layers (do not use):
- Chlorophyll future projections for absolutes and long-terms at all depths
- Current velocity absolutes and long-terms for all depths and all present and future projections
- Ice thickness absolute and long-term for future projections
- Light at bottom absolute and long-term for all depths
- Salinity absolute and long-term at all depths for future projections
- Temperature absolute and long-term at all depths for future projections&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Downloading environmental data in R</title>
      <link>https://theoceancode.netlify.com/post/dl_env_data_r/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/dl_env_data_r/</guid>
      <description>


&lt;div id=&#34;objective&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;Having been working in environmental science for several years now, entirely using R, I’ve come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn’t be. Most data hosting organisations do want scientists to use their data and do make it freely available. But sometimes it feels like the path to access was designed by crab people, rather than normal topside humans. I recently needed to gather several new data products and in classic ‘cut your nose off to spite your face’ fashion I insisted on doing all of it directly through an R script that could be run in RStudio. Besides being stubborn, one of the main reasons I felt this was necessary is that I wanted these download scripts to be able to be run operationally via a cron job. I think I came out pretty successful in the end so wanted to share the code with the rest of the internet. Enjoy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Packages not available via CRAN
remotes::install_github(&amp;quot;skgrange/threadr&amp;quot;)
remotes::install_github(&amp;quot;markpayneatwork/RCMEMS&amp;quot;)

# The packages we will use
library(tidyverse) # A staple for most modern data management in R
library(RCurl) # For helping R to make sense of URLs for web hosted data
library(XML) # For reading the HTML tables created by RCurl
library(tidync) # For easily dealing with NetCDF data
library(doParallel) # For parallel processing
library(threadr) # For downloading from FTP sites that require user credentials
library(RCMEMS) # For subsetting CMEMS data before download&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-noaa-oisst&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading NOAA OISST&lt;/h2&gt;
&lt;p&gt;I’ve already written a post about how to download NOAA OISST data using the &lt;strong&gt;&lt;code&gt;rerddap&lt;/code&gt;&lt;/strong&gt; package which may be found &lt;a href=&#34;https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html&#34;&gt;here&lt;/a&gt;. That post talks about how to get subsets of NOAA data, which is useful for projects with a refined scope, but it is laboriously slow if one simply wants the full global product. It must also be noted that as of this writing (June 3rd, 2020) the new OISST v2.1 data were not yet available on the ERDDAP server even though the old v2 data have now been rendered unavailable. For the time being it is necessary to download the full global data and then subset down to one’s desired study area. The following section of this blog post will outline how to do that.&lt;/p&gt;
&lt;p&gt;I need to stress that this is a very direct and unlimited method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset just because you can.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First we tell R where the data are on the interwebs
OISST_url_month &amp;lt;- &amp;quot;https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/&amp;quot;

# Then we pull that into a happy format
  # There is a lot here so it takes ~1 minute
OISST_url_month_get &amp;lt;- getURL(OISST_url_month)

# Before we continue let&amp;#39;s set a limit on the data we are going to download
  # NB: One should not simply download the entire dataset just because it is possible.
  # There should be a compelling reason for doing so.
start_date &amp;lt;- as.Date(&amp;quot;2019-01-01&amp;quot;)

# Now we strip away all of the unneeded stuff to get just the months of data that are available
OISST_months &amp;lt;- data.frame(months = readHTMLTable(OISST_url_month_get, skip.rows = 1:2)[[1]]$Name) %&amp;gt;% 
  mutate(months = lubridate::as_date(str_replace(as.character(months), &amp;quot;/&amp;quot;, &amp;quot;01&amp;quot;))) %&amp;gt;% 
  filter(months &amp;gt;= max(lubridate::floor_date(start_date, unit = &amp;quot;month&amp;quot;))) %&amp;gt;% # Filtering out months before Jan 2019
  mutate(months = gsub(&amp;quot;-&amp;quot;, &amp;quot;&amp;quot;, substr(months, 1, 7))) %&amp;gt;% 
  na.omit()

# Up next we need to now find the URLs for each individual day of data
# To do this we will wrap the following chunk of code into a function so we can loop it more easily
OISST_url_daily &amp;lt;- function(target_month){
  OISST_url &amp;lt;- paste0(OISST_url_month, target_month,&amp;quot;/&amp;quot;)
  OISST_url_get &amp;lt;- getURL(OISST_url)
  OISST_table &amp;lt;- data.frame(files = readHTMLTable(OISST_url_get, skip.rows = 1:2)[[1]]$Name) %&amp;gt;% 
    mutate(files = as.character(files)) %&amp;gt;% 
    filter(grepl(&amp;quot;avhrr&amp;quot;, files)) %&amp;gt;% 
    mutate(t = lubridate::as_date(sapply(strsplit(files, &amp;quot;[.]&amp;quot;), &amp;quot;[[&amp;quot;, 2)),
           full_name = paste0(OISST_url, files))
  return(OISST_table)
}

# Here we collect the URLs for every day of data available from 2019 onwards
OISST_filenames &amp;lt;- plyr::ldply(OISST_months$months, .fun = OISST_url_daily)

# Just to keep things tidy in this vignette I am now going to limit this data collection even further
OISST_filenames &amp;lt;- OISST_filenames %&amp;gt;% 
  filter(t &amp;lt;= &amp;quot;2019-01-31&amp;quot;)

# This function will go about downloading each day of data as a NetCDF file
# We will run this via plyr to expedite the process
# Note that this will download files into a &amp;#39;data/OISST&amp;#39; folder in the root directory
# If this folder does not exist it will create it
# This function will also check if the file has been previously downloaded
OISST_url_daily_dl &amp;lt;- function(target_URL){
  dir.create(&amp;quot;~/data/OISST&amp;quot;, showWarnings = F)
  file_name &amp;lt;- paste0(&amp;quot;~/data/OISST/&amp;quot;,sapply(strsplit(target_URL, split = &amp;quot;/&amp;quot;), &amp;quot;[[&amp;quot;, 10))
  if(!file.exists(file_name)) download.file(url = target_URL, method = &amp;quot;libcurl&amp;quot;, destfile = file_name)
}

# The way this code has been written it may be run on multiple cores
# Most modern laptops have at least 4 cores, so we will utilise 3 of them here
# One should always leave at least 1 core free
doParallel::registerDoParallel(cores = 3)

# And with that we are clear for take off
system.time(plyr::ldply(OISST_filenames$full_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds

# In roughly 15 seconds a user may have a full month of global data downloaded
# This scales well into years and decades, too&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because it is not currently possible to download subsetted OISST data from a GRIDDAP server I find that it is useful to include here the code one would use to load and subset downloaded OISST data. Please note that the OISST data have longitude values from 0 to 360, not -180 to 180.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This function will load and subset daily data into one data.frame
# Note that the subsetting of lon/lat is done before the data are loaded
# This means it will use much less RAM and is viable for use on most laptops
# Assuming one&amp;#39;s study area is not too large
OISST_load &amp;lt;- function(file_name, lon1, lon2, lat1, lat2){
      OISST_dat &amp;lt;- tidync(file_name) %&amp;gt;%
        hyper_filter(lon = between(lon, lon1, lon2),
                     lat = between(lat, lat1, lat2)) %&amp;gt;% 
        hyper_tibble() %&amp;gt;% 
        select(lon, lat, time, sst) %&amp;gt;% 
        dplyr::rename(t = time, temp = sst) %&amp;gt;% 
        mutate(t = as.Date(t, origin = &amp;quot;1978-01-01&amp;quot;))
      return(OISST_dat)
}

# Locate the files that will be loaded
OISST_files &amp;lt;- dir(&amp;quot;~/data/OISST&amp;quot;, full.names = T)

# Load the data in parallel
OISST_dat &amp;lt;- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T,
                         lon1 = 260, lon2 = 280, lat1 = 30, lat2 = 50)

# This should only take a few seconds to run at most&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-cci&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading CCI&lt;/h2&gt;
&lt;p&gt;An up-and-coming star in the world of remotely sensed data products, the Climate Change Initiative (CCI) has recently been putting out some industry leading products. These are all freely available for access and use for scientific research purposes. These have quickly become regarded as the most accurate products available and their use is now encouraged over other products. Unfortunately they are not available in near-real-time and so can currently only be used for historic analyses. A recent update of these data for 2017 and 2018 was made available and one assumes that 2019 will follow suit some time by the end of 2020.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The URLs where the data are housed for direct download
  # NB: Note that the versions are different; v2.1 vs. v2.0
  # NB: It looks like going straight through the thredds server is a more stable option
CCI_URL_old &amp;lt;- &amp;quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/esacci/sst/data/CDR_v2/Analysis/L4/v2.1&amp;quot;
CCI_URL_new &amp;lt;- &amp;quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/c3s_sst/data/ICDR_v2/Analysis/L4/v2.0&amp;quot;

# The date ranges that are housed therein
  # NB: These are historic repos and therefore the dates are static
  # I assume that the &amp;#39;new&amp;#39; data will be updated through 2019 by the end of 2020
date_range_old &amp;lt;- seq(as.Date(&amp;quot;1981-09-01&amp;quot;), as.Date(&amp;quot;2016-12-31&amp;quot;), by = &amp;quot;day&amp;quot;)
date_range_new &amp;lt;- seq(as.Date(&amp;quot;2017-01-01&amp;quot;), as.Date(&amp;quot;2018-12-31&amp;quot;), by = &amp;quot;day&amp;quot;)

# The function we will use to download the data
download_CCI &amp;lt;- function(date_choice, CCI_URL){
  
  # Prep the necessary URL pieces
  date_slash &amp;lt;- str_replace_all(date_choice, &amp;quot;-&amp;quot;, &amp;quot;/&amp;quot;)
  date_nogap &amp;lt;- str_replace_all(date_choice, &amp;quot;-&amp;quot;, &amp;quot;&amp;quot;)
  
  if(str_detect(CCI_URL, &amp;quot;esacci&amp;quot;)){
    tail_chunk &amp;lt;- &amp;quot;120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.nc&amp;quot;
  } else if(str_detect(CCI_URL, &amp;quot;c3s_sst&amp;quot;)){
    tail_chunk &amp;lt;- &amp;quot;120000-C3S-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR2.0-v02.0-fv01.0.nc&amp;quot;
  } else{
    stop(&amp;quot;The URL structure has changed.&amp;quot;)
  }
  
  complete_URL &amp;lt;- paste0(CCI_URL,&amp;quot;/&amp;quot;,date_slash,&amp;quot;/&amp;quot;,date_nogap,tail_chunk)
  # Note that this will download the files to data/CCI in the root directory
  file_name &amp;lt;- paste0(&amp;quot;~/data/CCI/&amp;quot;,date_nogap,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download.file(url = complete_URL, method = &amp;quot;libcurl&amp;quot;, destfile = file_name)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
# Most laptops have 4 cores, so 3 is a good choice
doParallel::registerDoParallel(cores = 3)

# Download all old data: 1981-09-01 to 2016-12-31
  # NB: Note the &amp;#39;[1:3]&amp;#39; below. This limits the downloads to only the first three files
  # Delete that to download everything.
  # But please do not download ALL of the files unless there is a need to do so.
plyr::l_ply(date_range_old[1:3], .fun = download_CCI, CCI_URL = CCI_URL_old, .parallel = T)

# Download all new data: 2016-01-01 to 2018-12-31
plyr::l_ply(date_range_new[1:3], .fun = download_CCI, CCI_URL = CCI_URL_new, .parallel = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-ostia&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading OSTIA&lt;/h2&gt;
&lt;p&gt;As noted above, CCI data products are quickly becoming the preferred standard. Unfortunately they are not available in near-real-time. This is where OSTIA data come in to fill the gap. Though not exactly the same assimilation process as CCI, these products come from the same suite of data sources. I do not yet know if these data for 2019 onwards can be used in combination with a climatology created from the CCI data, but it is on my to do list to find out. In order to download these data one will need to have a &lt;a href=&#34;http://marine.copernicus.eu/&#34;&gt;CMEMS&lt;/a&gt; account. This is free for researchers and very fast to sign up for. Once one has received a user name and password it is possible to use the code below to download the data via their FTP server. No Python required!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The URL where the data are housed for FTP
OSTIA_URL &amp;lt;- &amp;quot;ftp://nrt.cmems-du.eu/Core/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2&amp;quot;

# The date ranges that are housed therein
# NB: These are historic repos and therefore the dates are static
# I assume that the &amp;#39;new&amp;#39; data will be updated through 2019 by the end of 2020
date_range &amp;lt;- seq(as.Date(&amp;quot;2019-01-01&amp;quot;), as.Date(&amp;quot;2019-01-03&amp;quot;), by = &amp;quot;day&amp;quot;)

# Enter ones credentials here
# Note that the &amp;#39;:&amp;#39;  between &amp;#39;username&amp;#39; and &amp;#39;password&amp;#39; is required
user_credentials &amp;lt;- &amp;quot;username:password&amp;quot;

# Download function 
download_OSTIA &amp;lt;- function(date_choice, user_credentials){
  
  # Prep the necessary URL pieces
  date_slash &amp;lt;- strtrim(str_replace_all(date_choice, &amp;quot;-&amp;quot;, &amp;quot;/&amp;quot;), width = 7)
  date_nogap_day &amp;lt;- str_replace_all(date_choice, &amp;quot;-&amp;quot;, &amp;quot;&amp;quot;)
  
  tail_chunk &amp;lt;- &amp;quot;120000-UKMO-L4_GHRSST-SSTfnd-OSTIA-GLOB-v02.0-fv02.0.nc&amp;quot;

  complete_URL &amp;lt;- paste0(OSTIA_URL,&amp;quot;/&amp;quot;,date_slash,&amp;quot;/&amp;quot;,date_nogap_day,tail_chunk)
  file_name &amp;lt;- paste0(&amp;quot;~/data/OSTIA/&amp;quot;,date_nogap_day,tail_chunk)
  
  # Download and save the file if needed
  if(file.exists(file_name)){
    return()
  } else{
    download_ftp_file(complete_URL, file_name, verbose = TRUE, credentials = user_credentials)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# Run in parallel
doParallel::registerDoParallel(cores = 3)

# Download data from 2019-01-01 to present day
  # NB: Some files won&amp;#39;t download when run in parallel
  # I think the FTP server may be more aware of multiple requests than an HTTPS server
  # It may also have been due to high server use at the time, too
plyr::l_ply(date_range, .fun = download_OSTIA, .parallel = T, 
            user_credentials = user_credentials)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-glorys&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading GLORYS&lt;/h2&gt;
&lt;p&gt;At this point one may be thinking “wait a second, these have all been SST only products, this isn’t really a post about environmental data!”. But that’s where GLORYS comes in. This product has a range of variables one may be interested in. Not just SST. But yes, they are all physical variables. No bio-geochemistry in sight. Bamboozled! But if you’ve read this far, why stop now?! These data require a &lt;a href=&#34;http://marine.copernicus.eu/&#34;&gt;CMEMS&lt;/a&gt; account, same as the OSTIA data. They can also be downloaded via direct FTP access, but these files are enormous so in almost every use case this is not what one is intending to do. Rather these data are almost always subsetted in some way first. Luckily CMEMS has made this available to their user base with the introduction of the &lt;a href=&#34;https://github.com/clstoulouse/motu-client-python/releases&#34;&gt;MOTU client&lt;/a&gt;. Unluckily they have only made this available for use in Python. I have asked the people behind this process in person if there are plans for an officially supported R version and the short and long answers were no. That’s where Mark Payne and his &lt;a href=&#34;https://github.com/markpayneatwork/RCMEMS&#34;&gt;RCMEMS&lt;/a&gt; package enter the picture. He has wrapped the MOTU client for Python up in a handy R package that allows us to access, subset, and download CMEMS data all through the comfort of the RStudio interface! There is a tutorial on the GitHub repo that walks through the process that I show below if one would like an additional look at this process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Non-R software
# Unfortunately the subsetting of CMEMS data will require that one has Python installed
# https://www.python.org/downloads/
# Then one must download
# https://github.com/clstoulouse/motu-client-python/releases
# For instructions on how and why to properly install please see:
# https://github.com/markpayneatwork/RCMEMS

# Here is a cunning method of generating a brick of year-month values
date_range &amp;lt;- base::expand.grid(1993:2018, 1:12) %&amp;gt;% 
  dplyr::rename(year = Var1, month = Var2) %&amp;gt;% 
  arrange(year, month) %&amp;gt;% 
  mutate(year_mon = paste0(year,&amp;quot;-&amp;quot;,month)) %&amp;gt;% 
  dplyr::select(year_mon)

# Download function
  # NB: This function is currently designed to subset data to a specific domain
  # Please change your lon/lat accordingly
  # NB: This function will save files to data/GLORYS in the root directory
  # To change this change the --out-dir argument near the end of the chunk of text
  # NB: This big text chunk needs to be left as one long line
  # NB: The --user and --pwd arguments need to be given the users real username and passwords
  # from their CMEMS account
download_GLORYS &amp;lt;- function(date_choice){
  
  # The GLORYS script
    # This is a dummy script first generated by using the UI on the CMEMS website
    # No need to change anything here except for the --user and --pwd at the end
    # Please place your CMEMS username and password in those fields
  GLORYS_script &amp;lt;- &amp;#39;python ~/motuclient-python/motuclient.py --motu http://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_030-TDS --product-id global-reanalysis-phy-001-030-daily --longitude-min -180 --longitude-max 179.9166717529297 --latitude-min -80 --latitude-max 90 --date-min &amp;quot;2018-12-25 12:00:00&amp;quot; --date-max &amp;quot;2018-12-25 12:00:00&amp;quot; --depth-min 0.493 --depth-max 0.4942 --variable thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi --out-dir data --out-name test.nc --user username --pwd password&amp;#39;
  
  # Prep the necessary URL pieces
  date_start &amp;lt;- parse_date(date_choice, format = &amp;quot;%Y-%m&amp;quot;)
  # A clever way of finding the end date of any month!
    # I found this on stackoverflow somewhere...
  date_end &amp;lt;- date_start %m+% months(1) - 1
  
  # Cannot get data past 2018-12-25
  if(date_end &amp;gt; as.Date(&amp;quot;2018-12-25&amp;quot;)) date_end &amp;lt;- as.Date(&amp;quot;2018-12-25&amp;quot;)
  
  # Set the file name
  file_name &amp;lt;- paste0(&amp;quot;GLORYS_&amp;quot;,date_choice,&amp;quot;.nc&amp;quot;)
  
  # Take the chunk of code above and turn it into something useful
  cfg &amp;lt;- parse.CMEMS.script(GLORYS_script, parse.user = T)
  
  # This is where one should make any required changes to the subsetting of the data
  # This is now the magic of the RCMEMS package, which allows us to interface with the Python code as though it were R
  cfg_update &amp;lt;- RCMEMS::update(cfg, variable = &amp;quot;thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi&amp;quot;,
                               longitude.min = &amp;quot;-80.5&amp;quot;,
                               longitude.max = &amp;quot;-40.5&amp;quot;,
                               latitude.min = &amp;quot;31.5&amp;quot;,
                               latitude.max = &amp;quot;63.5&amp;quot;,
                               date.min = as.character(date_start),
                               date.max = as.character(date_end),
                               out.name = file_name)
  
  # Download and save the file if needed
  if(file.exists(paste0(&amp;quot;~/data/GLORYS/&amp;quot;,file_name))){
    return()
  } else{
    CMEMS.download(cfg_update)
  }
  Sys.sleep(2) # Give the server a quick breather
}

# I&amp;#39;ve limited the download to only 1 file
# Delete &amp;#39;[1]&amp;#39; to download everything
  #NB: The CMEMS server is a little wonky, rather not try to multicore this
plyr::l_ply(date_range$year_mon[1], .fun = download_GLORYS, .parallel = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope this has been a useful whack of data for anyone looking to download any of these products for their science. The techniques laid out in the code here should apply to most other data products as well as there aren’t that many different methods of hosting data. If I’ve missed anything that people feel is an important data source that can’t be adapted from the code here let me know and I’m happy to see what I can do.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Marine Heatwaves With Sub-Optimal Data</title>
      <link>https://theoceancode.netlify.com/publication/detection/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 -0800</pubDate>
      
      <guid>https://theoceancode.netlify.com/publication/detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>heatwaveR</title>
      <link>https://theoceancode.netlify.com/package/heatwaver/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 -0800</pubDate>
      
      <guid>https://theoceancode.netlify.com/package/heatwaver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>coastR</title>
      <link>https://theoceancode.netlify.com/package/coastr/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 -0800</pubDate>
      
      <guid>https://theoceancode.netlify.com/package/coastr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Detecting marine heatwaves with sub-optimal data</title>
      <link>https://theoceancode.netlify.com/poster/mhw_detection_poster/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/poster/mhw_detection_poster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Marine heatwave tracker</title>
      <link>https://theoceancode.netlify.com/project/mhwtracker/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/project/mhwtracker/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Marine heatwaves: the new normal</title>
      <link>https://theoceancode.netlify.com/talk/mhw_2019/</link>
      <pubDate>Tue, 19 Mar 2019 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/talk/mhw_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>heatwaveR: A central algorithm for the detection of heatwaves and cold-spells</title>
      <link>https://theoceancode.netlify.com/publication/heatwaver/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/publication/heatwaver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>South Africa time survey</title>
      <link>https://theoceancode.netlify.com/post/sa_time_survey/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/sa_time_survey/</guid>
      <description>


&lt;div id=&#34;objective&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;In South Africa there are a range of idioms for different time frames in which someone may (or may not) do something. The most common of these are: ‘now’, ‘just now’, and ‘now now’. If one were to Google these sayings one would find that there is general agreements on how long these time frames are, but that agreement is not absolute.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/img/just_now.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Advice from the internet.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This got me to wondering just how much disagreement there may be around the country. And more specifically I wanted to know how these times changed between specific locations. If one is interested in contributing to the survey, it may be taken &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSeNyF8XJeLXLoPCfE9VdEMc_SOHkX84KF82OOudVKq6K15YTg/viewform?usp=sf_link&#34;&gt;here&lt;/a&gt;. To avoid too much confusion with the answers that could be given, specific times were provided to the participants in a multiple choice format. These times (minutes) were: 5, 15, 30, 60, 120, 300.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-prep&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data prep&lt;/h2&gt;
&lt;p&gt;The survey data are downloaded from &lt;a href=&#34;https://www.google.com/forms/about/&#34;&gt;Google Forms&lt;/a&gt; rather easily as a .csv file, so that’s nice. Unfortunately, the way in which one sets up the survey for humans is difficult for R to understand so we need quite a bit of processing after we load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggpubr)
library(broom)

survey &amp;lt;- read_csv(&amp;quot;../../static/data/SA Time Survey.csv&amp;quot;, skip = 1, col_types = &amp;quot;cccccc&amp;quot;,
                   col_names = c(&amp;quot;time&amp;quot;, &amp;quot;just now&amp;quot;, &amp;quot;now now&amp;quot;, &amp;quot;now&amp;quot;, &amp;quot;province&amp;quot;, &amp;quot;city&amp;quot;)) %&amp;gt;% 
  select(-time) %&amp;gt;%
  mutate(`just now` = as.numeric(sapply(strsplit(`just now`, &amp;quot; &amp;quot;), &amp;quot;[[&amp;quot;, 1)),
         `now now` = as.numeric(sapply(strsplit(`now now`, &amp;quot; &amp;quot;), &amp;quot;[[&amp;quot;, 1)),
         `now` = as.numeric(sapply(strsplit(`now`, &amp;quot; &amp;quot;), &amp;quot;[[&amp;quot;, 1)),
         province = gsub(&amp;quot;Kzn&amp;quot;, &amp;quot;KZN&amp;quot;, province),
         province = gsub(&amp;quot;GP&amp;quot;, &amp;quot;Gauteng&amp;quot;, province),
         province = as.factor(province),
         city = as.factor(city))

# Check that city and province names are all lekker
# levels(survey$province)
# levels(survey$city)

# Create a long version for easier stats
survey_long &amp;lt;- survey %&amp;gt;% 
  gather(key = &amp;quot;saying&amp;quot;, value = &amp;quot;minutes&amp;quot;, -province, -city) %&amp;gt;% 
  na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;participant-locations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Participant locations&lt;/h2&gt;
&lt;p&gt;With our data prepared, I first wanted to see from where the surveys were taken. I’ve done this by splitting them up into province or city. This is also one of the few situations in which a bar plot is an appropriate visualisation. The columns below that show ‘NA’ are for participants that declined to share their location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;province_plot &amp;lt;- ggplot(data = survey, aes(x = province)) +
  geom_bar(aes(fill = province), show.legend = F) +
  ggtitle(&amp;quot;Provinces of participants&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;)
# province_plot
city_plot &amp;lt;- ggplot(data = survey, aes(x = city)) +
  geom_bar(aes(fill = city), show.legend = F) +
  ggtitle(&amp;quot;Cities of participants&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;) +
  theme(axis.text.x = element_text(angle = 15))
# city_plot
ggarrange(province_plot, city_plot, ncol = 1, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:location-bar&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/SA_time_survey_files/figure-html/location-bar-1.png&#34; alt=&#34;Bar plots showing the total pariticpants by province or city.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plots showing the total pariticpants by province or city.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;participant-time-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Participant time frames&lt;/h2&gt;
&lt;p&gt;With the locations of of our participants visualised we now want to see what sort of time frames people around the country attribute to the three most common idioms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = survey_long, aes(x = saying, y = minutes, fill = saying)) +
  geom_boxplot(show.legend = FALSE, outlier.colour = NA) +
  geom_jitter(shape = 21, alpha = 0.6, width = 0.3, height = 0.0, show.legend = FALSE) +
  scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) +
  scale_fill_brewer(palette = &amp;quot;Accent&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:basic-plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/SA_time_survey_files/figure-html/basic-plot-1.png&#34; alt=&#34;Boxplots showing the distribution of times (minutes) survey participants gave for the three most common time frame idioms in South Africa.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Boxplots showing the distribution of times (minutes) survey participants gave for the three most common time frame idioms in South Africa.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we may see in Figure &lt;a href=&#34;#fig:basic-plot&#34;&gt;2&lt;/a&gt;, ‘just now’ and ‘now now’ appear to have similar distributions, whereas ‘now’ is markedly different. Participants answered the maximum score of 300 minutes for all idioms, but for ‘just now’ and ‘now now’ this was infrequent enough that the large scores are considered to be outliers. For ‘now’, enough participants answered with longer times that the distribution appears much larger than for the other two idioms. Even though these distributions appear different, let’s run the stats on them to make sure. Because we want to compare distributions of scores for three different categories we will be using an ANOVA if the data meet a few basic assumptions.&lt;/p&gt;
&lt;p&gt;Just as a quick recap, these assumptions are: homoscedasticity (homogeneity of variance), normality of distribution, random &amp;amp; independently sampled. It is also a good idea to have at least a sample size of ten for each category. Seeing as how this was an Internet survey, we were not able to ensure that the participants are a random representation of the South African populace, nor can we be certain that they submitted their answers independent of one another. I am however just going to go ahead and assume that they did. As for the assumptions of homoscedasticity and the normality of the distributions, we can directly test these. In R, the normality of a distribution is tested with &lt;code&gt;shapiro.test()&lt;/code&gt;. Any non-significant result (&lt;em&gt;p&lt;/em&gt; &amp;gt; 0.05) means that the data are normally distributed. To test for homoscedasticity we will use &lt;code&gt;bartlett.test()&lt;/code&gt;. A non-significant result (&lt;em&gt;p&lt;/em&gt; &amp;gt; 0.05) from this test indicates that the variances between the categories are equivalent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normality
survey_long %&amp;gt;% 
  group_by(saying) %&amp;gt;% 
  summarise(noramlity = as.numeric(shapiro.test(minutes)[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 2
##   saying   noramlity
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 just now     0.565
## 2 now          0.713
## 3 now now      0.671&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test for homoscedasticity
survey_long %&amp;gt;% 
  bartlett.test(minutes ~ saying, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Bartlett test of homogeneity of variances
## 
## data:  minutes by saying
## Bartlett&amp;#39;s K-squared = 5.227, df = 2, p-value = 0.07328&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Surprisingly these data meet all of our assumptions so we may perform a simple one-way ANOVA on them to detect any significant differences.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(aov(minutes ~ saying, data = survey_long))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##   logLik   AIC   BIC deviance  nobs r.squared
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1  -670. 1349. 1360. 1143843.   111    0.0615&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Less surprisingly, there is a significant difference in the times given for these three idioms. But let’s dive just a bit deeper with a post-hoc Tukey (not Turkey) test to see which categories specifically are different from which.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TukeyHSD(aov(minutes ~ saying, data = survey_long))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = minutes ~ saying, data = survey_long)
## 
## $saying
##                       diff         lwr       upr     p adj
## now-just now      60.81081    3.949653 117.67197 0.0330869
## now now-just now  14.18919  -42.671969  71.05035 0.8241544
## now now-now      -46.62162 -103.482779  10.23954 0.1302148&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the names given in the ‘saying’ column, and the values given in the ‘p adj’ column we may see which individual idioms are different from which. Unsurprisingly, judging from Figure &lt;a href=&#34;#fig:basic-plot&#34;&gt;2&lt;/a&gt;, ‘now now’ and ‘just now’ are not different. Interesting though is that ‘now’ is significantly different from ‘just now’, but not ‘now now’. The initial results made it look as though ‘now’ would have been significantly different from both.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;province-time-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Province time frames&lt;/h2&gt;
&lt;p&gt;Preferably, many more people would have taken the survey so that we could draw more conclusive results. I’m rather certain that should more people take the survey the distributions of the scores for the three idioms would even out more until there were no significant differences between any of them. Enough participants have taken the test however that we may compare the results for a few different provinces.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove provinces with fewer than nine entries
# Ten would be preferable, but at nine we allow the inclusion of KZN
survey_province &amp;lt;- survey_long %&amp;gt;% 
  group_by(province) %&amp;gt;% 
  filter(n() &amp;gt;= 9) %&amp;gt;% 
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now with the provinces that have only a few answers filtered out, let’s see what the data look like as boxplots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = survey_province, aes(x = saying, y = minutes, fill = province)) +
  geom_boxplot(outlier.colour = NA) +
  geom_point(shape = 21, position = position_jitterdodge()) +
  scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) +
  scale_fill_brewer(palette = &amp;quot;Set2&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:province-plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/SA_time_survey_files/figure-html/province-plot-1.png&#34; alt=&#34;Boxplots showing the distribution of times by province.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Boxplots showing the distribution of times by province.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Right away for me it appears that the time frames for the Eastern Cape are shorter than for the other three provinces. KZN appears to be the longest, with Gauteng and the Western Cape seeming similar. Because we have multiple independent variables (saying and province) we want a two-way ANOVA. But before we do that, let’s again check for normality and homoscedasticity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normality
survey_province %&amp;gt;% 
  group_by(saying, province) %&amp;gt;% 
  summarise(normality = as.numeric(shapiro.test(minutes)[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 3
## # Groups:   saying [3]
##    saying   province     normality
##    &amp;lt;chr&amp;gt;    &amp;lt;fct&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 just now Eastern Cape     0.640
##  2 just now Gauteng          0.867
##  3 just now KZN              0.865
##  4 just now Western Cape     0.537
##  5 now      Eastern Cape     0.766
##  6 now      Gauteng          0.721
##  7 now      KZN              0.75 
##  8 now      Western Cape     0.729
##  9 now now  Eastern Cape     0.684
## 10 now now  Gauteng          0.689
## 11 now now  KZN              0.813
## 12 now now  Western Cape     0.718&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test for homoscedasticity
survey_province %&amp;gt;% 
  bartlett.test(minutes ~ interaction(saying, province), data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Bartlett test of homogeneity of variances
## 
## data:  minutes by interaction(saying, province)
## Bartlett&amp;#39;s K-squared = 69.523, df = 11, p-value = 1.505e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The different groups of time are all normally distributed, but the variances differ. Regardless, we are going to stick to a normal two-way ANOVA as there are no good alternatives to this for non-parametric data and transforming these data is a bother.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(aov(minutes ~ saying + province, data = survey_province))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##   logLik   AIC   BIC deviance  nobs r.squared
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1  -623. 1261. 1279.  881685.   105     0.204&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Significant differences are to be had here. So let’s break it down and see which groups specifically differ.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TukeyHSD(aov(minutes ~ saying + province, data = survey_province))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = minutes ~ saying + province, data = survey_province)
## 
## $saying
##                       diff        lwr        upr     p adj
## now-just now      70.14286   16.46427 123.821448 0.0068600
## now now-just now  15.42857  -38.25002  69.107162 0.7734013
## now now-now      -54.71429 -108.39288  -1.035695 0.0447008
## 
## $province
##                                 diff         lwr       upr     p adj
## Gauteng-Eastern Cape       63.452381  -15.761193 142.66595 0.1624466
## KZN-Eastern Cape          139.722222   39.043527 240.40092 0.0025348
## Western Cape-Eastern Cape  73.289474    6.613379 139.96557 0.0252743
## KZN-Gauteng                76.269841  -21.982505 174.52219 0.1845631
## Western Cape-Gauteng        9.837093  -53.115472  72.78966 0.9768852
## Western Cape-KZN          -66.432749 -154.888584  22.02309 0.2092492&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may see that by removing some of the answers from the less represented provinces there is now a significant difference between the scores for ‘now’ and ‘now now’ as well as ‘just now’. Additionally we may see that the scores for Eastern Cape differ significantly from KZN and the Western Cape. One could also look at the interactions between the two independent variables but I’m not quite interested in that level of depth here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;city-time-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;City time frames&lt;/h2&gt;
&lt;p&gt;With the breakdown for the province time frames out of the way, we are going to wrap up this analysis by looking at the difference between cities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove provinces with fewer than five entries
# Ten is preferable, but at five we may include Mthatha
survey_city &amp;lt;- survey_long %&amp;gt;% 
  group_by(saying, city) %&amp;gt;% 
  filter(n() &amp;gt;= 5) %&amp;gt;% 
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = survey_city, aes(x = saying, y = minutes, fill = city)) +
  geom_boxplot(outlier.colour = NA) +
  geom_point(shape = 21, position = position_jitterdodge()) +
  scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) +
  scale_fill_brewer(palette = &amp;quot;Set3&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:city-plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/SA_time_survey_files/figure-html/city-plot-1.png&#34; alt=&#34;Boxplots showing the distribution of times by city.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Boxplots showing the distribution of times by city.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again with the assumptions…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for normality
survey_city %&amp;gt;% 
  group_by(saying, city) %&amp;gt;% 
  summarise(normality = as.numeric(shapiro.test(minutes)[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 3
## # Groups:   saying [3]
##   saying   city         normality
##   &amp;lt;chr&amp;gt;    &amp;lt;fct&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 just now Cape Town        0.577
## 2 just now Johannesburg     0.912
## 3 just now Mthatha          0.552
## 4 now      Cape Town        0.739
## 5 now      Johannesburg     0.757
## 6 now      Mthatha          0.754
## 7 now now  Cape Town        0.739
## 8 now now  Johannesburg     0.741
## 9 now now  Mthatha          0.552&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test for homoscedasticity
survey_city %&amp;gt;% 
  bartlett.test(minutes ~ interaction(saying, city), data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Bartlett test of homogeneity of variances
## 
## data:  minutes by interaction(saying, city)
## Bartlett&amp;#39;s K-squared = 55.177, df = 8, p-value = 4.078e-09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and again we see that while normally distributed, the variance of our sample sets differ significantly from one another. Regardless, we’ll stick to the two-way ANOVA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(aov(minutes ~ saying + city, data = survey_city))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##   logLik   AIC   BIC deviance  nobs r.squared
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1  -481.  973.  987.  674933.    81     0.205&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As one likely would have deduced from Figure &lt;a href=&#34;#fig:city-plot&#34;&gt;4&lt;/a&gt;, the scores are significantly different from one another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TukeyHSD(aov(minutes ~ saying + city, data = survey_city))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = minutes ~ saying + city, data = survey_city)
## 
## $saying
##                       diff        lwr         upr     p adj
## now-just now      77.96296   16.65151 139.2744119 0.0090081
## now now-just now  16.85185  -44.45960  78.1633008 0.7889537
## now now-now      -61.11111 -122.42256   0.2003379 0.0509401
## 
## $city
##                             diff        lwr        upr     p adj
## Johannesburg-Cape Town -10.72917  -72.99124  51.532904 0.9108240
## Mthatha-Cape Town      -84.89583 -151.53238 -18.259285 0.0088590
## Mthatha-Johannesburg   -74.16667 -152.92265   4.589316 0.0691761&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the post-hoc test we may see that ‘now’ differs significantly from ‘just now in these three cities, and if we’re feeling generous we may say that ’now’ also differs significantly from ‘now now’. But like with the rest of the country, ‘now now’ and ‘just now’ are not significantly different time frames. Looking at the city breakdown we see that the only significant difference is between Mthatha and Cape Town. Johannesburg and Cape Town do not differ.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The general conclusion I’ve drawn from the analysis of the South Africa time survey results is that the understanding people in the Eastern Cape have of the time frames for these idioms is significantly faster than in the other major provinces in South Africa. The numbers don’t lie!&lt;/p&gt;
&lt;p&gt;Speaking to people about this survey (before the results were out), the general consensus was that Johannesburg people would give significantly faster scores than people in other parts of the country, particularly Cape Town. That does not however appear to be the case. Also surprising from these results was that ‘now’ tended to be considered to be a significantly longer time frame than ‘just now’ and ‘now now’. Most people I’ve talked to about these idioms agree that ‘now’ is meant to be the fastest… so I’m not sure how that worked out. Unsurprising to me, but perhaps to some South Africans, was that there is no difference between ‘just now’ and ‘now now’. I was not surprised by this because talking with people around the country I very infrequently heard people, even while in the same room, agree on the time frames for these idioms.&lt;/p&gt;
&lt;p&gt;There are of course a host of issues with this study. One thing I’m wondering about the Eastern Cape scores being significantly faster than the other provinces is if perhaps people in the Eastern Cape have a wider range of idioms for giving someone a time frame? Meaning, perhaps ‘now’, ‘just now’, and ‘now now’ are all much faster than other provinces because there is some other popular idiom people use there that denotes a longer time frame. It’s also possible that people in other provinces misunderstood the survey. Though I did make it very clear that the answers were in minutes and not seconds, specifically to attempt to prevent people from making that mistake. Lastly, this analysis suffers from a regrettably small sample size. I would have preferred at least 100 responses, rather than 40. It is still wonderful to be able to get some numbers in the game and get a glimpse of the fact that there is little agreement about these time frames.&lt;/p&gt;
&lt;p&gt;Hopefully posting these results will snare a few more people into taking the survey and in another couple of months I can make a follow up post with the additional feedback.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://theoceancode.netlify.com/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transects</title>
      <link>https://theoceancode.netlify.com/post/transects/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/transects/</guid>
      <description>


&lt;div id=&#34;preface&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;This week I have expanded the &lt;code&gt;coastR&lt;/code&gt; package with the inclusion of a function that calculates the angle of the heading for alongshore or shore-normal transects. The rest of this blog post is the vignette that I’ve written detailing the set of this function. Next week I’ll likely be taking a break from &lt;code&gt;coastR&lt;/code&gt; development to finally create a package for the SACTN dataset. That is a project that has been in the works for a loooong time and it will be good to finally see a development release available to the public.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;There are a number of reasons why one would want to calculate transects along or away from a coastline. Examples include: finding the fetch across an embayment, finding the coordinates of a point 200 km from the coast, finding the appropriate series of SST pixels along/away from the coast, (or if one is feeling particular feisty) the creation of shape files for a given area away from the coast. The function that we will be introducing here does none of these things. What the &lt;code&gt;transects()&lt;/code&gt; function does do is calculate the angle of the heading along or away from the coast against true North, which is then the basis for all of the other fancy things one may want to do. Baby steps people. Baby steps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(&amp;quot;robwschlegel/coastR&amp;quot;) # Install coastR
library(coastR)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(geosphere)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-locations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample locations&lt;/h2&gt;
&lt;p&gt;For this vignette we will re-use the same coastlines as those created for the sequential sites vignette. The ordering of the sites remains jumbled up to demonstrate that &lt;code&gt;transects()&lt;/code&gt; does not require orderly data. Should one want to order ones site list before calculating transect headings it is possible to do so with &lt;code&gt;seq_sites()&lt;/code&gt;. This is of course a recommended step in any workflow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cape Point, South Africa
cape_point &amp;lt;- SACTN_site_list %&amp;gt;% 
  slice(c(31, 22, 26, 17, 19, 21, 30)) %&amp;gt;% 
  mutate(order = 1:n())

# South Africa
south_africa &amp;lt;- SACTN_site_list %&amp;gt;% 
  slice(c(1,34, 10, 20, 50, 130, 90)) %&amp;gt;% 
  mutate(order = 1:n())

# Baja Peninsula, Mexico
baja_pen &amp;lt;- data.frame(
  order = 1:7,
  lon = c(-116.4435, -114.6800, -109.6574, -111.9503, -112.2537, -113.7918, -114.1881),
  lat = c(30.9639, 30.7431, 22.9685, 26.9003, 25.0391, 29.4619, 28.0929)
)

# Bohai Sea, China
bohai_sea &amp;lt;- data.frame(
  order = 1:7,
  lon = c(122.0963, 121.2723, 121.0687, 121.8742, 120.2962, 117.6650, 122.6380),
  lat = c(39.0807, 39.0086, 37.7842, 40.7793, 40.0691, 38.4572, 37.4494)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transects&lt;/h2&gt;
&lt;p&gt;With our site lists created we now want to see what the correct headings for alongshore and shore-normal transects are for our sites. We will also demonstrate what happens when we increase the &lt;code&gt;spread&lt;/code&gt; used in the calculation and also how the inclusion of island masks affects the angle of the headings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cape Point, South Africa
cape_point_along &amp;lt;- transects(cape_point, alongshore = T)
cape_point_away &amp;lt;- transects(cape_point)

# South Africa
south_africa_along &amp;lt;- transects(south_africa, alongshore = T)
south_africa_away &amp;lt;- transects(south_africa)
  # NB: Note here the use of the `spread` argument
south_africa_along_wide &amp;lt;- transects(south_africa, alongshore = T, spread = 30)
south_africa_away_wide &amp;lt;- transects(south_africa, spread = 30)

# Baja Peninsula, Mexico
baja_pen_along &amp;lt;- transects(baja_pen, alongshore = T)
baja_pen_away &amp;lt;- transects(baja_pen)
  # NB: Note here the use of the `coast` argument
baja_pen_island &amp;lt;- transects(baja_pen, coast = FALSE)

# Bohai sea, China
bohai_sea_along &amp;lt;- transects(bohai_sea, alongshore = T)
bohai_sea_away &amp;lt;- transects(bohai_sea)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualise&lt;/h2&gt;
&lt;p&gt;Now that the correct headings have been calculated for our alongshore and shore-normal transects let’s visualise them with ggplot. First we will create a function that does this in order to keep the length of this vignette down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create base map
world_map &amp;lt;- ggplot() + 
  borders(fill = &amp;quot;grey40&amp;quot;, colour = &amp;quot;black&amp;quot;)

# Create titles
titles &amp;lt;- c(&amp;quot;Alongshore&amp;quot;, &amp;quot;Shore-normal&amp;quot;, &amp;quot;Islands&amp;quot;)

# Plotting function
plot_sites &amp;lt;- function(site_list, buffer, title_choice, dist){
  
  # Find the point 200 km from the site manually to pass to ggplot
  heading2 &amp;lt;- data.frame(geosphere::destPoint(p = select(site_list, lon, lat),  
                                              b = site_list$heading, d = dist))
  
  # Add the new coordinates tot he site list
  site_list &amp;lt;- site_list %&amp;gt;% 
    mutate(lon_dest = heading2$lon,
           lat_dest = heading2$lat)
  
  # Visualise
  world_map +
    geom_segment(data = site_list, colour = &amp;quot;red4&amp;quot;, 
                 aes(x = lon, y = lat, xend = lon_dest, yend = lat_dest)) +
    geom_point(data = site_list, size = 3, colour = &amp;quot;black&amp;quot;, aes(x = lon, y = lat)) +
    geom_point(data = site_list, size = 3, colour = &amp;quot;red&amp;quot;, aes(x = lon_dest, y = lat_dest)) +
    coord_cartesian(xlim = c(min(site_list$lon - buffer), 
                             max(site_list$lon + buffer)),
                    ylim = c(min(site_list$lat - buffer), 
                             max(site_list$lat + buffer))) +
    labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;, colour = &amp;quot;Site\norder&amp;quot;) +
    ggtitle(titles[title_choice])
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;cape-point-south-africa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cape Point, South Africa&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;transect()&lt;/code&gt; function is designed to work well at small scales by default. We may see this here with the effortlessness of plotting transects around a peninsula and then across an embayment in one go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cape_point_along_map &amp;lt;- plot_sites(cape_point_along, 0.5, 1, 10000)
cape_point_away_map &amp;lt;- plot_sites(cape_point_away, 0.5, 2, 10000)
grid.arrange(cape_point_along_map, cape_point_away_map, nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/transects_files/figure-html/cape_point_trans-1.png&#34; alt=&#34;Alongshore and shore-normal transects around Cape Point and False Bay, South Africa.&#34; width=&#34;960&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
(#fig:cape_point_trans)Alongshore and shore-normal transects around Cape Point and False Bay, South Africa.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;south-africa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;South Africa&lt;/h3&gt;
&lt;p&gt;The intentions one may have for calculating shore-normal transects will differ depending on ones research question. If one is interested in visualising the convolutions of a coastline at a sub-meso-scale then the default &lt;code&gt;spread&lt;/code&gt; of the &lt;code&gt;transect()&lt;/code&gt; function is probably the way to go, as shown above. If however one is interested in seeing the shore-normal transects broadly for the coastline of an entire country it is likely that one will want to greatly expand the &lt;code&gt;spread&lt;/code&gt; of coastline used to calculate said transects. In the figure below we may see how changing the &lt;code&gt;spread&lt;/code&gt; of the coastline considered for the transects changes the results. The top row shows the transects resulting from the narrow default &lt;code&gt;spread&lt;/code&gt;, while the bottom row shows the results of using a much wider &lt;code&gt;spread&lt;/code&gt; for the calculation. Note particularly how the transect changes at St. Helena Bay and Gansbaai (second and fourth sites from the top left), as well as a general smoothing of all of the other transects. This is due to the sensitivity of the function. The St. Helena Bay and Gansbaai sites lay within embayments; therefore, the shore-normal transects that would come out directly from these sites will not follow the general contour of the coastline of South Africa. Should we be interested in the “bigger picture” we must increase the &lt;code&gt;spread&lt;/code&gt; argument in &lt;code&gt;transects()&lt;/code&gt;. This may require some trial and error for particularly difficult coastlines before a satisfactory result is produced, but it is certainly still faster than running the calculations by hand. Should small scale accuracy along part of the coast, and broader accuracy elsewhere be required, one must simply divide the site list into the different sections and run &lt;code&gt;transects()&lt;/code&gt; on each subset with the desired &lt;code&gt;spread&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;south_africa_along_map &amp;lt;- plot_sites(south_africa_along, 1, 1, 100000)
south_africa_away_map &amp;lt;- plot_sites(south_africa_away, 1, 2, 100000)
south_africa_along_wide_map &amp;lt;- plot_sites(south_africa_along_wide, 1, 1, 100000)
south_africa_away_wide_map &amp;lt;- plot_sites(south_africa_away_wide, 1, 2, 100000)
grid.arrange(south_africa_along_map, south_africa_away_map, 
             south_africa_along_wide_map, south_africa_away_wide_map, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/transects_files/figure-html/south_africa_trans-1.png&#34; alt=&#34;Alongshore and shore-normal transects around all of South Africa.&#34; width=&#34;864&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
(#fig:south_africa_trans)Alongshore and shore-normal transects around all of South Africa.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;baja-peninsula-mexico&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Baja Peninsula, Mexico&lt;/h3&gt;
&lt;p&gt;In the following figure we see how the inclusion of islands affects the results of our transects. The first site up from the tip of the peninsula on the left-hand side is on an island. Note the minor adjustment to the transect when the island mask is used for the calculation. In this case it’s not large, but in other instances it may be massive. By default island masks are removed and it is our advice that they not be used unless extreme caution is observed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baja_pen_along_map &amp;lt;- plot_sites(baja_pen_along, 1, 1, 100000)
baja_pen_away_map &amp;lt;- plot_sites(baja_pen_away, 1, 2, 100000)
baja_pen_island_map &amp;lt;- plot_sites(baja_pen_island, 1, 3, 100000)
grid.arrange(baja_pen_along_map, baja_pen_away_map, baja_pen_island_map, nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/transects_files/figure-html/baja_pen_trans-1.png&#34; alt=&#34;Alongshore and shore-normal transects around the Baja Peninsula.&#34; width=&#34;960&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
(#fig:baja_pen_trans)Alongshore and shore-normal transects around the Baja Peninsula.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bohai-sea-china&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bohai Sea, China&lt;/h3&gt;
&lt;p&gt;This figure serves as a good visualisation for just how localised the coastline is that is used to calculate the shore-normal transects. Note how the alongshore transects look a little dodgy, but when shown as shore-normal transects everything works out. This is something to consider if one is interested in calculating alongshore transects rather than shore-normal transects. For alongshore transects that show more fidelity for coastal direction it is advisable to increase the &lt;code&gt;spread&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bohai_sea_along_map &amp;lt;- plot_sites(bohai_sea_along, 1, 1, 70000)
bohai_sea_away_map &amp;lt;- plot_sites(bohai_sea_away, 1, 2, 70000)
grid.arrange(bohai_sea_along_map, bohai_sea_away_map, nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://theoceancode.netlify.com/post/transects_files/figure-html/bohai_sea_trans-1.png&#34; alt=&#34;Alongshore and shore-normal transects within the Bohai Sea.&#34; width=&#34;960&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
(#fig:bohai_sea_trans)Alongshore and shore-normal transects within the Bohai Sea.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As we may see in the previous example figures, the &lt;code&gt;transect()&lt;/code&gt; function tends to work better by default at smaller scales. This was an intentional decision as it is much more accurate when scaling the function up for larger coastal features than when scaling it down for smaller ones.&lt;/p&gt;
&lt;p&gt;The calculation of the heading for alongshore and shore-normal transects is rarely the end goal itself. One then generally wants to find specific points from the coastline along the transects that have been determined. This is done in the code above within the &lt;code&gt;plot_sites()&lt;/code&gt; function created within this vignette, but the process is not detailed specifically. How to do more elaborate things with transects will be explained with the following functions to be added to &lt;code&gt;coastR&lt;/code&gt;. This will include how to draw coastal polygons based on distance and bathymetry.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predominant Atmospheric and Oceanic Patterns during Coastal Marine Heatwaves</title>
      <link>https://theoceancode.netlify.com/publication/predominant/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 -0700</pubDate>
      
      <guid>https://theoceancode.netlify.com/publication/predominant/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Polar plot climatologies</title>
      <link>https://theoceancode.netlify.com/post/polar_plot_clims/</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://theoceancode.netlify.com/post/polar_plot_clims/</guid>
      <description>


&lt;div id=&#34;objective&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;Whilst cruising about on &lt;a href=&#34;http://imgur.com&#34;&gt;Imgur&lt;/a&gt; I found a post about science stuff. Not uncommon, which is nice. These sorts of grab-bag posts about nothing in particular often include some mention of climate science, almost exclusively some sort of clever visualisation of a warming planet. That seems to be what people are most interested in. I’m not complaining though, it keeps me employed. The aforementioned post caught my attention more than usual because it included a GIF, and not just a static picture of some sort of blue thing that is becoming alarmingly red (that was not meant to be a political metaphor). I’m referring to the now famous GIF by climate scientist Ed Hawkins (&lt;span class=&#34;citation&#34;&gt;@ed_hawkins&lt;/span&gt;) whose blog may be found &lt;a href=&#34;https://www.climate-lab-book.ac.uk/&#34;&gt;here&lt;/a&gt;, and the specific post in question &lt;a href=&#34;https://www.climate-lab-book.ac.uk/2016/spiralling-global-temperatures/&#34;&gt;here&lt;/a&gt;. A quick bit of research on this animation revealed that it has likely been viewed by millions of people, was featured in the opening ceremony of the Rio Olympics, and was created in MATLAB. Those three key points made me decide to do a post on how to re-create this exact figure in R via a bit of reverse engineering. The original GIF in question is below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/img/polar_temp.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The ever broadening spiral of global temperatures created by &lt;a href=&#34;https://www.climate-lab-book.ac.uk/&#34;&gt;Ed Hawkins&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;Figure 1 above uses the global mean temperature anomalies taken from &lt;a href=&#34;http://www.metoffice.gov.uk/hadobs/hadcrut4/&#34;&gt;HadCRUT4&lt;/a&gt;. These data have an impressive range of collection, going back to 1850. Very few datasets match this length of collection, and I’m not going to attempt to do so here. What I am going to do is use the data that I work with on a daily basis. These are the &lt;a href=&#34;https://github.com/ajsmit/SACTN&#34;&gt;SACTN&lt;/a&gt; data that may also be downloaded &lt;a href=&#34;https://robert-schlegel.shinyapps.io/SACTN/&#34;&gt;here&lt;/a&gt; via a GUI. As a coastal oceanographer I am mostly interested in changing climates in the near shore. While not publish explicitly, a &lt;a href=&#34;http://journals.ametsoc.org/doi/abs/10.1175/JCLI-D-16-0014.1&#34;&gt;paper&lt;/a&gt; about the appropriate methodology one should use does exist, and this methodology has been applied to all of the time series in the SACTN dataset accordingly. It is therefore known what the rates of decadal change along the coast of South Africa are, and we may rely on this in order to cherry pick the more dramatic time series in order to make prettier visuals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;With our end goal established (Figure 1), and our dataset chosen (SACTN), we may now get busy with the actual code necessary. As one may have inferred from the title of this post, Figure 1 is what we call a “polar plot”. This may appear complex to some, but is actually a very simple visualisation, as we shall see below. But first we need to prep our data. For consistency in the creation of the anomaly values below I will use 1981 – 2010 for the climatology of each time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Libraries
library(tidyverse)
library(viridis)
library(lubridate)
library(zoo)
library(gridExtra)
library(animation)

## Data
# SACTN
load(&amp;quot;../../static/data/SACTN_monthly_v4.2.RData&amp;quot;)

## Subset
# Subseting function
ts.sub &amp;lt;- function(site){
  ts &amp;lt;- SACTN_monthly_v4.2 %&amp;gt;% 
    filter(index == site) %&amp;gt;%
    mutate(year = year(as.yearmon(date)),
           month = month(as.yearmon(date), label = T),
           clim = mean(temp[year %in% seq(1981,2010)], na.rm = T),
           anom = temp-clim,
           index = as.character(index)) %&amp;gt;%
    rename(site = index) %&amp;gt;% 
    select(site, year, month, anom)
  return(ts)
}

# Warming site
PN &amp;lt;- ts.sub(&amp;quot;Port Nolloth/SAWS&amp;quot;)

# Cooling site
SP &amp;lt;- ts.sub(&amp;quot;Sea Point/SAWS&amp;quot;)

# Neutral site
KB &amp;lt;- ts.sub(&amp;quot;Kent Bay/KZNSB&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our data prepared we may now create the series of functions that will make a spiralling polar plot of temperatures for any time series we feed into it. I prefer to use the &lt;a href=&#34;https://cran.r-project.org/web/packages/animation/index.html&#34;&gt;animation&lt;/a&gt; package to create animations in R. This requires that one also installs &lt;a href=&#34;http://www.imagemagick.org/script/index.php&#34;&gt;image magick&lt;/a&gt; beforehand. This is a free software that is available for all major operating systems. There are a few ways to create animations in R, but I won’t go into that now. The method I employ to create the animations below may seem odd at first, but as far as I have seen it is the most efficient way to do so. The philosophy employed here is that we want to have one final function that simply counts forward one step at a time, creating each frame of the GIF. This function calls on other functions that are calculating the necessary stats and creating the visuals from them in the background. By creating animations in this way, our up front prep and calculation time is almost non-existent. It does mean that the animations take longer to compile, but they are also much more dynamic and we may feed any number of different dataframes into them to get different outputs. I have found over the years that the more automated ones code can be the better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Function that creates a polar plot
polar.plot &amp;lt;- function(df, i){
  # Add bridges for polar coordinates
  years &amp;lt;- unique(df$year)[1:i]
  df2 &amp;lt;- filter(df, year %in% years)
  bridges &amp;lt;- df2[df2$month == &amp;#39;Jan&amp;#39;,]
  bridges$year &amp;lt;- bridges$year - 1
  if(nrow(bridges) == 0){
    bridges &amp;lt;- data.frame(site = df2$site[1], year = min(df2$year), month = NA, anom = NA)
  } else {
    bridges$month &amp;lt;- NA
  }
  blanks &amp;lt;- data.frame(site = df2$site[1], expand.grid(year = min(df2$year)-1, month = month.abb), anom = NA)
  # Polar plot
  pp &amp;lt;- ggplot(data = rbind(blanks, df2, bridges), aes(x = month, y = anom, group = year)) +
    # Circular black background
    geom_rect(colour = &amp;quot;black&amp;quot;, fill = &amp;quot;black&amp;quot;, aes(xmin = &amp;quot;Jan&amp;quot;, xmax = NA,
                  ymin = min(df$anom, na.rm = T), ymax = max(df$anom, na.rm = T))) +
                  # ymin = min(df$anom, na.rm = T), ymax = 3)) +
    # Anomaly threshold labels
    geom_hline(aes(yintercept = 1.0), colour = &amp;quot;red&amp;quot;) +
    geom_label(aes(x = &amp;quot;Jan&amp;quot;, y = 1.0, label = &amp;quot;1.0°C&amp;quot;),
               colour = &amp;quot;red&amp;quot;, fill = &amp;quot;black&amp;quot;, size = 3) +
    geom_hline(aes(yintercept = 2.0), colour = &amp;quot;red&amp;quot;) +
    geom_label(aes(x = &amp;quot;Jan&amp;quot;, y = 2.0, label = &amp;quot;2.0°C&amp;quot;),
               colour = &amp;quot;red&amp;quot;, fill = &amp;quot;black&amp;quot;, size = 3) +
    geom_hline(aes(yintercept = 3.0), colour = &amp;quot;red&amp;quot;) +
    geom_label(aes(x = &amp;quot;Jan&amp;quot;, y = 3.0, label = &amp;quot;3.0°C&amp;quot;),
               colour = &amp;quot;red&amp;quot;, fill = &amp;quot;black&amp;quot;, size = 3) +
    geom_hline(aes(yintercept = 4.0), colour = &amp;quot;red&amp;quot;) +
    geom_label(aes(x = &amp;quot;Jan&amp;quot;, y = 4.0, label = &amp;quot;4.0°C&amp;quot;),
               colour = &amp;quot;red&amp;quot;, fill = &amp;quot;black&amp;quot;, size = 3) +
    # Temperature spiral
    geom_path(aes(colour = anom), show.legend = F) +
    # Scale corrections
    scale_colour_viridis(limits = c(min(df$anom, na.rm = T), max(df$anom, na.rm = T))) +
    scale_x_discrete(expand = c(0,0), breaks = month.abb) +
    scale_y_continuous(expand = c(0,0),
                       limits = c(min(df$anom, na.rm = T), max(df$anom, na.rm = T))) +
    # Year label
    geom_text(aes(x = &amp;quot;Jan&amp;quot;, y = min(df$anom, na.rm = T), label = max(df2$year, na.rm = T)),
              colour = &amp;quot;ivory&amp;quot;, size = 8) +
    # Additional tweaks
    ggtitle(paste0(df$site[1],&amp;quot; temperature change (&amp;quot;,min(df$year),&amp;quot;-&amp;quot;,max(df$year),&amp;quot;)&amp;quot;)) +
    coord_polar() +
    theme(panel.background = element_rect(fill = &amp;quot;grey20&amp;quot;),
          plot.background = element_rect(fill = &amp;quot;grey20&amp;quot;),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          # axis.text.x = element_text(colour = &amp;quot;ivory&amp;quot;),
          axis.text.x = element_text(colour = &amp;quot;ivory&amp;quot;, angle =
            (360/(2*pi)*rev(seq(pi/12, 2*pi-pi/12, len = 12)))+15,
            size = 12),
          axis.text.y = element_blank(),
          axis.title = element_blank(),
          axis.ticks = element_blank(),
          axis.ticks.length = unit(0, &amp;quot;cm&amp;quot;),
          plot.title = element_text(hjust = 0.5, colour = &amp;quot;ivory&amp;quot;, size = 15))
  print(pp)
}

## Create animation of polar plots
animate.polar.plot &amp;lt;- function(df) {
  lapply(seq(1,length(unique(df$year))), function(i) {
    polar.plot(df = df, i = i)
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the above two functions created, we may now call them nested within one another via the &lt;code&gt;saveGIF&lt;/code&gt; function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# By default &amp;#39;saveGIF()&amp;#39; outputs to the same folder 
# the script where the code is being run from is located.
# For that reason one may want to manually change the
# working directory beforehand.
# setwd(&amp;quot;somewhere else&amp;quot;)
system.time(saveGIF(animate.polar.plot(df = PN), interval = 0.4, ani.width = 457, 
                    movie.name = &amp;quot;polar_plot_PN.gif&amp;quot;)) ## 262 seconds
system.time(saveGIF(animate.polar.plot(df = SP), interval = 0.4, ani.width = 457, 
                    movie.name = &amp;quot;polar_plot_SP.gif&amp;quot;)) ## 221 seconds
system.time(saveGIF(animate.polar.plot(df = KB), interval = 0.4, ani.width = 457, 
                    movie.name = &amp;quot;polar_plot_KB.gif&amp;quot;)) ## 183 seconds
# setwd(&amp;quot;back to where you were&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;As one may see in the following GIFs, local extremes often outpace global averages. This should not be terribly surprising. In order to better illustrate this I have expanded the anomaly labels along the y-axes more so than seen in Figure 1. The increasing patterns are not as clear in these following GIFs as in the original that they are based on. This is because the original is based on a global average, which provides for a much smoother trend. I hope people enjoy these and feel free to plop your own temperature time series into the code to create your own polar plot figures!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/img/polar_plot_PN.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: The polar plot for Port Nolloth, where temperatures have been increasing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/img/polar_plot_SP.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: The polar plot for Sea Point, where temperatures have been decreasing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://theoceancode.netlify.com/img/polar_plot_KB.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: The polar plot for Kent Bay, where temperatures have been holding level.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
