[{"authors":null,"categories":null,"content":"","date":1701471600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701471600,"objectID":"9aa4d91042eada143106bfae8c4c713d","permalink":"https://theoceancode.netlify.app/package/fjordlight/","publishdate":"2023-12-02T00:00:00+01:00","relpermalink":"/package/fjordlight/","section":"package","summary":"An R package to aid in accessing the fjord PAR data product created for the FACE-IT project.","tags":["R","PAR"],"title":"FjordLight","type":"package"},{"authors":["RW Schlegel","J-P Gattuso"],"categories":null,"content":"","date":1692655200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692655200,"objectID":"dc707a925c57652eec746a797493ea60","permalink":"https://theoceancode.netlify.app/publication/drivers_data/","publishdate":"2023-08-22T00:00:00+02:00","relpermalink":"/publication/drivers_data/","section":"publication","summary":"The collection of in situ data is generally a costly process, with the Arctic being no exception. Indeed, there has been a perception that the Arctic is lacking in situ sampling; however, after many years of concerted effort and international collaboration, the Arctic is now rather well sampled, with many cruise expeditions every year. For example, the GLODAP (Global Ocean Data Analysis Project) product has a greater density of in situ sampling points within the Arctic than along the Equator. While this is useful for open-ocean processes, the fjords of the Arctic, which serve as crucially important intersections of terrestrial, coastal, and marine processes, are sampled in a much more ad hoc process. This is not to say they are not well sampled but rather that the data are more difficult to source and combine for further analysis. It was therefore noted that the fjords of the Arctic are lacking in FAIR (findable, accessible, interoperable, and reusable) data. To address this issue, a single dataset has been created from publicly available, predominantly in situ data from seven study sites in Svalbard and Greenland. After finding and accessing the data from a number of online platforms, they were amalgamated into a single project-wide standard, ensuring their interoperability. The dataset was then uploaded to PANGAEA so that it can be findable and reusable in the future. The focus of the data collection was driven by the key drivers of change in Arctic fjords identified in a companion review paper. To demonstrate the usability of this dataset, an analysis of the relationship between the different drivers was performed. Via the use of an Arctic biogeochemical model, these relationships were projected forward to 2100 via Representative Carbon Pathways (RCPs) 2.6, 4.5, and 8.5. This dataset is a work in progress, and as new datasets containing the relevant key drivers are released, they will be added to an updated version planned for the middle of 2024. The dataset (Schlegel and Gattuso, 2022) is available on PANGAEA at https://doi.org/10.1594/PANGAEA.953115. A live version is available at the FACE-IT WP1 site and can be accessed by clicking the “Data access” tab: https://face-it-project.github.io/WP1/ (last access: 17 August 2023)","tags":["R","Arctic","fjord","social"],"title":"A dataset for investigating socio-ecological changes in Arctic fjords","type":"publication"},{"authors":null,"categories":null,"content":"","date":1691618400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691618400,"objectID":"66b45f6f232bed54181ee261d724dc54","permalink":"https://theoceancode.netlify.app/package/heatwave3/","publishdate":"2023-08-10T00:00:00+02:00","relpermalink":"/package/heatwave3/","section":"package","summary":"An R package for the detection of heatwaves and cold-spells directly on NetCDF files.","tags":["marine heatwaves","R"],"title":"heatwave3","type":"package"},{"authors":null,"categories":null,"content":"","date":1690149600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690149600,"objectID":"76d8714837be0afebc6276f798e5fbfd","permalink":"https://theoceancode.netlify.app/talk/clivar_2023/","publishdate":"2023-07-24T00:00:00+02:00","relpermalink":"/talk/clivar_2023/","section":"talk","summary":"Everything you wanted to know about the marine heatwave definition, but were too afraid to ask. Now with 100% more interactivity!","tags":["marine heatwaves"],"title":"Basic detection and categorisation of events","type":"talk"},{"authors":["RW Schlegel","I Bartsch","K Bischof","LR Bjørst","H Dannevig","N Diehl","P Duarte","GK Hovelsrud","T Juul-Pedersen","A Lebrun","L Merillet","C Miller","C Ren","M Sejr","JE Søreide","TR Vonnahme","J-P Gattuso"],"categories":null,"content":"","date":1672873200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672873200,"objectID":"a205d8e187c06dc3f777aee95cd7c02c","permalink":"https://theoceancode.netlify.app/publication/drivers_arctic/","publishdate":"2023-01-05T00:00:00+01:00","relpermalink":"/publication/drivers_arctic/","section":"publication","summary":"Fjord systems are transition zones between land and sea, resulting in complex and dynamic environments. They are of particular interest in the Arctic as they harbour ecosystems inhabited by a rich range of species and provide many societal benefits. The key drivers of change in the European Arctic (i.e., Greenland, Svalbard, and Northern Norway) fjord socio-ecological systems are reviewed here, structured into five categories: cryosphere (sea ice, glacier mass balance, and glacial and riverine discharge), physics (seawater temperature, salinity, and light), chemistry (carbonate system, nutrients), biology (primary production, biomass, and species richness), and social (governance, tourism, and fisheries). The data available for the past and present state of these drivers, as well as future model projections, are analysed in a companion paper. Changes to the two drivers at the base of most interactions within fjords, seawater temperature and glacier mass balance, will have the most significant and profound consequences on the future of European Arctic fjords. This is because even though governance may be effective at mitigating/adapting to local disruptions caused by the changing climate, there is possibly nothing that can be done to halt the melting of glaciers, the warming of fjord waters, and all of the downstream consequences that these two changes will have. This review provides the first transdisciplinary synthesis of the interactions between the drivers of change within Arctic fjord socio-ecological systems. Knowledge of what these drivers of change are, and how they interact with one another, should provide more expedient focus for future research on the needs of adapting to the changing Arctic.","tags":["R","Arctic","fjord","social"],"title":"Drivers of change in Arctic fjord socio-ecological systems: Examples from the European Arctic","type":"publication"},{"authors":["J Garrabou","D Gómez-Gras","A Medrano","C Cerrano","M Ponti","RW Schlegel","N Bensoussan","E Turicchia","... +62"],"categories":null,"content":"","date":1652565600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652565600,"objectID":"2e9407ce5c7166a6cf1c6303c2d5502a","permalink":"https://theoceancode.netlify.app/publication/med_mass/","publishdate":"2022-05-15T00:00:00+02:00","relpermalink":"/publication/med_mass/","section":"publication","summary":"Climate change is causing an increase in the frequency and intensity of marine heatwaves (MHWs) and mass mortality events (MMEs) of marine organisms are one of their main ecological impacts. Here, we show that during the 2015–2019 period, the Mediterranean Sea has experienced exceptional thermal conditions resulting in the onset of five consecutive years of widespread MMEs across the basin. These MMEs affected thousands of kilometers of coastline from the surface to 45 m, across a range of marine habitats and taxa (50 taxa across 8 phyla). Significant relationships were found between the incidence of MMEs and the heat exposure associated with MHWs observed both at the surface and across depths. Our findings reveal that the Mediterranean Sea is experiencing an acceleration of the ecological impacts of MHWs which poses an unprecedented threat to its ecosystems' health and functioning. Overall, we show that increasing the resolution of empirical observation is critical to enhancing our ability to more effectively understand and manage the consequences of climate change.","tags":["MHW","biodiversity"],"title":"Marine heatwaves drive recurrent mass mortalities in the Mediterranean Sea","type":"publication"},{"authors":["K Filbee-Dexter","KA MacGregor","C Lavoie","I Garrido","J Goldsmit","LC de la Guardia","KL Howland","LE Johnson","Brenda Konar","CW McKindsey","CJ Mundy","RW Schlegel","P Archambault"],"categories":null,"content":"","date":1648677600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648677600,"objectID":"56a55d26ffde8f8d262816f32c9e911b","permalink":"https://theoceancode.netlify.app/publication/sea_ice_kelp/","publishdate":"2022-03-31T00:00:00+02:00","relpermalink":"/publication/sea_ice_kelp/","section":"publication","summary":"The coastal zone of the Canadian Arctic represents 10% of the world’s coastline and is one of the most rapidly changing marine regions on the planet. To predict the consequences of these environmental changes, a better understanding of how environmental gradients shape coastal habitat structure in this area is required. We quantified the abundance and diversity of canopy forming seaweeds throughout the nearshore zone (5–15 m) of the Eastern Canadian Arctic using diving surveys and benthic collections at 55 sites distributed over 3,000 km of coastline. Kelp forests were found throughout, covering on average 40.4% (±29.9 SD) of the seafloor across all sites and depths, despite thick sea ice and scarce hard substrata in some areas. Total standing macroalgal biomass ranged from 0 to 32 kg m–2 wet weight and averaged 3.7 kg m–2 (±0.6 SD) across all sites and depths. Kelps were less abundant at depths of 5 m compared to 10 or 15 m and distinct regional assemblages were related to sea ice cover, substratum type, and nutrient availability. The most common community configuration was a mixed assemblage of four species: Agarum clathratum (14.9% benthic cover ± 12.0 SD), Saccharina latissima (13% ± 14.7 SD), Alaria esculenta (5.4% ± 1.2 SD), and Laminaria solidungula (3.7% ± 4.9 SD). A. clathratum dominated northernmost regions and S. latissima and L. solidungula occurred at high abundance in regions with more open water days. In southeastern areas along the coast of northern Labrador, the coastal zone was mainly sea urchin barrens, with little vegetation. We found positive relationships between open water days (days without sea ice) and kelp biomass and seaweed diversity, suggesting kelp biomass could increase, and the species composition of kelp forests could shift, as sea ice diminishes in some areas of the Eastern Canadian Arctic. Our findings demonstrate the high potential productivity of this extensive coastal zone and highlight the need to better understand the ecology of this system and the services it provides.","tags":["R","SDM"],"title":"Sea Ice and Substratum Shape Extensive Kelp Forests in the Canadian Arctic","type":"publication"},{"authors":["J Goldsmit","RW Schlegel","K Filbee-Dexter","KA MacGregor","LE Johnson","CJ Mundy","AM Savoie","CW McKindsey","KL Howland","P Archambault"],"categories":null,"content":"","date":1633557600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633557600,"objectID":"819a9356b5d06f054cec012d923cba0c","permalink":"https://theoceancode.netlify.app/publication/eastern_arctic/","publishdate":"2021-10-07T00:00:00+02:00","relpermalink":"/publication/eastern_arctic/","section":"publication","summary":"Climate change is transforming marine ecosystems through the expansion and contraction of species’ ranges. Sea ice loss and warming temperatures are expected to expand habitat availability for macroalgae along long stretches of Arctic coastlines. To better understand the current distribution of kelp forests in the Eastern Canadian Arctic, kelps were sampled along the coasts for species identifications and percent cover. The sampling effort was supplemented with occurrence records from global biodiversity databases, searches in the literature, and museum records. Environmental information and occurrence records were used to develop ensemble models for predicting habitat suitability and a Random Forest model to predict kelp cover for the dominant kelp species in the region – Agarum clathratum, Alaria esculenta, and Laminariaceae species (Laminaria solidungula and Saccharina latissima). Ice thickness, sea temperature and salinity explained the highest percentage of kelp distribution. Both modeling approaches showed that the current extent of arctic kelps is potentially much greater than the available records suggest. These modeling approaches were projected into the future using predicted environmental data for 2050 and 2100 based on the most extreme emission scenario (RCP 8.5). The models agreed that predicted distribution of kelp in the Eastern Canadian Arctic is likely to expand to more northern locations under future emissions scenarios, with the exception of the endemic arctic kelp L. solidungula, which is more likely to lose a significant proportion of suitable habitat. However, there were differences among species regarding predicted cover for both current and future projections. Notwithstanding model-specific variation, it is evident that kelps are widespread throughout the area and likely contribute significantly to the functioning of current Arctic ecosystems. Our results emphasize the importance of kelp in Arctic ecosystems and the underestimation of their potential distribution there.","tags":["R","SDM"],"title":"Kelp in the Eastern Canadian Arctic: Current and Future Predictions of Habitat Suitability and Cover","type":"publication"},{"authors":["A Abrahams","RW Schlegel","AJ Smit"],"categories":null,"content":"","date":1625695200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625695200,"objectID":"b882760b445c638619faf612a9810679","permalink":"https://theoceancode.netlify.app/publication/upwelling_definition/","publishdate":"2021-07-08T00:00:00+02:00","relpermalink":"/publication/upwelling_definition/","section":"publication","summary":"The importance of coastal upwelling systems is widely recognized. However, several aspects of the current and future behaviors of these systems remain uncertain. Fluctuations in temperature because of anthropogenic climate change are hypothesized to affect upwelling-favorable winds and coastal upwelling is expected to intensify across all Eastern Boundary Upwelling Systems. To better understand how upwelling may change in the future, it is necessary to develop a more rigorous method of quantifying this phenomenon. In this paper, we use SST data and wind data in a novel method of detecting upwelling signals and quantifying metrics of upwelling intensity, duration, and frequency at four sites within the Benguela Upwelling System. We found that indicators of upwelling are uniformly detected across five SST products for each of the four sites and that the duration of those signals is longer in SST products with higher spatial resolutions. Moreover, the high-resolution SST products are significantly more likely to display upwelling signals at 25 km away from the coast when signals were also detected at the coast. Our findings promote the viability of using SST and wind time series data to detect upwelling signals within coastal upwelling systems. We highlight the importance of high-resolution data products to improve the reliability of such estimates. This study represents an important step towards the development of an objective method for describing the behavior of coastal upwelling systems.","tags":["R","coastal","atmosphere","ocean"],"title":"A novel approach to quantify metrics of upwelling intensity, frequency, and duration","type":"publication"},{"authors":null,"categories":["R"],"content":" Objective Nearly four years after writing a blog post about recreating R figures in ODV I had someone reach out to me expressing interest in adding a bathymetry layer over the interpolated data. It\u0026rsquo;s always nice to know that these blog posts are being found useful for other researchers. And I have to admit I\u0026rsquo;m a bit surprised that the code still runs 4 years later. Especially considering that it uses the tidyverse which is notorious for breaking backwards compatibility. In order to demonstrate the overlaying of bathymetry data on a CTD transect we will need to use a different dataset than in the previous blog post. One may use any data one would like, but for this blog I went to this shiny app to extract some data from the coast of South Africa. Specifically I filtered for temperature data from November 1990 at all depths. We won\u0026rsquo;t go back over the theory for recreating the ODV figure in this blog post, so please revisit that for a recap as necessary. Below I will show two of the necessary steps to get interpolated CTD data before we begin on the bathymetry mask.\n# Load libraries library(tidyverse) library(lubridate) library(reshape2) library(MBA) library(mgcv) library(marmap) library(FNN) # The transects in this dataset do not have an ID column # So we manually select the first transect (rows 1 - 12) # This is then used as a mask to select all depths for these pixels # We will also use these unique lon/lat coords for bathymetry points ctd_mask \u0026lt;- read_csv(\u0026quot;../../static/data/CTD_transect.csv\u0026quot;) %\u0026gt;% select(lon, lat) %\u0026gt;% slice(1:12) %\u0026gt;% unique() # Load and screen data # For ease I am only using monthly means # and depth values rounded to 10 metres ctd \u0026lt;- read_csv(\u0026quot;../../static/data/CTD_transect.csv\u0026quot;) %\u0026gt;% mutate(depth = -depth) %\u0026gt;% # Correct for plotting right_join(ctd_mask, by = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;)) %\u0026gt;% select(lon, lat, depth, temp) # Manually extracted hexidecimal ODV colour palette ODV_colours \u0026lt;- c(\u0026quot;#feb483\u0026quot;, \u0026quot;#d31f2a\u0026quot;, \u0026quot;#ffc000\u0026quot;, \u0026quot;#27ab19\u0026quot;, \u0026quot;#0db5e6\u0026quot;, \u0026quot;#7139fe\u0026quot;, \u0026quot;#d16cfa\u0026quot;) # Create quick scatterplot ggplot(data = ctd, aes(x = lon, y = depth)) + geom_point(aes(colour = temp)) + scale_colour_gradientn(colours = rev(ODV_colours)) + labs(y = \u0026quot;depth (m)\u0026quot;, x = \u0026quot;longitude (°E)\u0026quot;, colour = \u0026quot;temp. (°C)\u0026quot;)  Figure 1: A non-interpolated scatterplot of our temperature (°C) data shown as a function of depth (m) over longitude (°E).\nIt looks like we have a nice little upwelling signal coming through at the coast. It will be interesting to see how the interpolation handles that. We\u0026rsquo;ll quickly run the interpolation and then get to the bathymetry overlay. Note that these data are not on a straight latitude transect, but we are not going to worry about that in this blog post.\n# Now we may interpolate the data # NB: The columns that mba.surf() will interpolate are the X, Y, Z values in that order ctd_mba \u0026lt;- mba.surf(ctd[c(\u0026quot;lon\u0026quot;, \u0026quot;depth\u0026quot;, \u0026quot;temp\u0026quot;)], no.X = 300, no.Y = 300, extend = T) dimnames(ctd_mba$xyz.est$z) \u0026lt;- list(ctd_mba$xyz.est$x, ctd_mba$xyz.est$y) ctd_mba \u0026lt;- melt(ctd_mba$xyz.est$z, varnames = c('lon', 'depth'), value.name = 'temp') %\u0026gt;% filter(depth \u0026lt; 0) %\u0026gt;% mutate(temp = round(temp, 1)) # Finally we create our gridded result ggplot(data = ctd_mba, aes(x = lon, y = depth)) + geom_raster(aes(fill = temp)) + geom_contour(aes(z = temp), binwidth = 2, colour = \u0026quot;black\u0026quot;, alpha = 0.2) + geom_contour(aes(z = temp), breaks = 20, colour = \u0026quot;black\u0026quot;) + scale_fill_gradientn(colours = rev(ODV_colours)) + labs(y = \u0026quot;depth (m)\u0026quot;, x = \u0026quot;longitude (°E)\u0026quot;, fill = \u0026quot;temp. (°C)\u0026quot;) + coord_cartesian(expand = F)  Figure 2: The same temperature (°C) profiles seen in Figure 1 with the missing values filled in with multilevel B-splines.\nBathymetry The interpolation seems to have done a decent job of acknowledging the upwelling signal. It may be possible to tweak the interpolation more, as desired, but I\u0026rsquo;m happy enough with it for the purposes of this post. We are going to skip over the step to cut out the artefacts at the bottom of the figure where there are not data points because we are rather going to just overlay our bathymetry mask. First we will download the bathymetry data. Then we find the points that are closest to our CTD transect. These will then be used for the grey overlay at the bottom of the figure.\n# Bathymetry within transect bounding box bathy \u0026lt;- getNOAA.bathy(lon1 = min(ctd$lon), lon2 = max(ctd$lon), lat1 = min(ctd$lat), lat2 = max(ctd$lat), resolution = 5) # Larger numbers for coarser data # Convert bathy object to a data.frame for easier use bathy_df \u0026lt;- data.frame(lon_b = as.numeric(rownames(bathy)), lat_b = as.numeric(colnames(bathy)), depth = as.numeric(bathy)) %\u0026gt;% mutate(bathy_idx = 1:n()) # Used for merging CTD and bathy data # Find nearest points to transect data ctd_mask \u0026lt;- ctd_mask %\u0026gt;% mutate(bathy_idx = as.vector(knnx.index(as.matrix(bathy_df[,c(\u0026quot;lon_b\u0026quot;, \u0026quot;lat_b\u0026quot;)]), as.matrix(ctd_mask[,c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;)]), k = 1))) %\u0026gt;% left_join(bathy_df, by = \u0026quot;bathy_idx\u0026quot;) # Manually create bottom of the bathy mask polygon bathy_mask \u0026lt;- data.frame(lon = c(ctd_mask$lon, rev(ctd_mask$lon)), depth = c(ctd_mask$depth, rep(min(ctd_mask$depth), nrow(ctd_mask)))) # We may now use that bathy mask for our final figure ggplot(data = ctd_mba, aes(x = lon, y = depth)) + geom_raster(aes(fill = temp)) + geom_contour(aes(z = temp), binwidth = 2, colour = \u0026quot;black\u0026quot;, alpha = 0.2) + geom_contour(aes(z = temp), breaks = 20, colour = \u0026quot;black\u0026quot;) + geom_polygon(data = bathy_mask, fill = \u0026quot;grey80\u0026quot;, colour = \u0026quot;black\u0026quot;) + scale_fill_gradientn(colours = rev(ODV_colours)) + labs(y = \u0026quot;depth (m)\u0026quot;, x = \u0026quot;longitude (°E)\u0026quot;, fill = \u0026quot;temp. (°C)\u0026quot;) + coord_cartesian(expand = F)  Figure 3: The same temperature (°C) profiles seen in Figure 2 with the bathymetry values overlaid.\nUnfortunately in this example there is a bit of blank space in the bottom left of the plot because the CTD casts do not go deeper than 200 m, and the interpolation doesn\u0026rsquo;t fill in values outside of the rectangular box dictated by the X (lon) and Y (depth) values. A cheeky workaround for this issue would be to simply crop the figure to the bottom of the interpolated data, and not the bathymetry.\n# We may now use that bathy mask for our final figure ggplot(data = ctd_mba, aes(x = lon, y = depth)) + geom_raster(aes(fill = temp)) + geom_contour(aes(z = temp), binwidth = 2, colour = \u0026quot;black\u0026quot;, alpha = 0.2) + geom_contour(aes(z = temp), breaks = 20, colour = \u0026quot;black\u0026quot;) + geom_polygon(data = bathy_mask, fill = \u0026quot;grey80\u0026quot;, colour = \u0026quot;black\u0026quot;) + geom_point(data = ctd, aes(x = lon, y = depth), colour = 'black', size = 0.2, alpha = 0.4, shape = 8) + scale_fill_gradientn(colours = rev(ODV_colours)) + labs(y = \u0026quot;depth (m)\u0026quot;, x = \u0026quot;longitude (°E)\u0026quot;, fill = \u0026quot;temp. (°C)\u0026quot;) + coord_cartesian(expand = F, ylim = c(-200, 0)) + theme(panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;))  Figure 4: The plotting area cropped to the interpolated data, rather than the bathymetry mask. Also shown with black dots are the original CTD data.\nSummary In this tutorial we have seen how to plot a bathymetry overlay that matches the lon/lat coordinates of the CTD casts. I\u0026rsquo;m sure there is a way to force the interpolation to fill in values at a greater depth to match the bathymetry, but the focus of this blog was on adding the bathymetry mask itself, and I think we have addressed this issue. The workflow outlined above has a couple of bumps in it, but should be adaptable to a range of applications.\n","date":1621555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"b08a0a1fc800548380241459cb5520ec","permalink":"https://theoceancode.netlify.app/post/odv_bathy/","publishdate":"2021-05-21T00:00:00Z","relpermalink":"/post/odv_bathy/","section":"post","summary":"Objective Nearly four years after writing a blog post about recreating R figures in ODV I had someone reach out to me expressing interest in adding a bathymetry layer over the interpolated data. It\u0026rsquo;s always nice to know that these blog posts are being found useful for other researchers. And I have to admit I\u0026rsquo;m a bit surprised that the code still runs 4 years later. Especially considering that it uses the tidyverse which is notorious for breaking backwards compatibility.","tags":["ODV","visuals","interpolation"],"title":"ODV figures in R with bathymetry","type":"post"},{"authors":["RW Schlegel","ECJ Oliver","K Chen"],"categories":null,"content":"","date":1615244400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615244400,"objectID":"38349ba573c615bf8cbefa0617e11099","permalink":"https://theoceancode.netlify.app/publication/primary_drivers/","publishdate":"2021-03-09T00:00:00+01:00","relpermalink":"/publication/primary_drivers/","section":"publication","summary":"Marine heatwaves (MHWs) are increasing in duration and intensity at a global scale and are projected to continue to increase due to the anthropogenic warming of the climate. Because MHWs may have drastic impacts on fisheries and other marine goods and services, there is a growing interest in understanding the predictability and developing practical predictions of these events. A necessary step toward prediction is to develop a better understanding of the drivers and processes responsible for the development of MHWs. Prior research has shown that air–sea heat flux and ocean advection across sharp thermal gradients are common physical processes governing these anomalous events. In this study we apply various statistical analyses and employ the self-organizing map (SOM) technique to determine specifically which of the many candidate physical processes, informed by a theoretical mixed-layer heat budget, have the most pronounced effect on the onset and/or decline of MHWs on the Northwest Atlantic continental shelf. It was found that latent heat flux is the most common driver of the onset of MHWs. Mixed layer depth (MLD) also strongly modulates the onset of MHWs. During the decay of MHWs, atmospheric forcing does not explain the evolution of the MHWs well, suggesting that oceanic processes are important in the decay of MHWs. The SOM analysis revealed three primary synoptic scale patterns during MHWs: low-pressure cyclonic Autumn-Winter systems, high-pressure anti-cyclonic Spring-Summer blocking, and mild but long-lasting Summer blocking. Our results show that nearly half of past MHWs on the Northwest Atlantic shelf are initiated by positive heat flux anomaly into the ocean, but less than one fifth of MHWs decay due to this process, suggesting that oceanic processes, e.g., advection and mixing are the primary driver for the decay of most MHWs.","tags":["R","marine heatwaves"],"title":"Drivers of Marine Heatwaves in the Northwest Atlantic: The Role of Air–Sea Interaction During Onset and Decline","type":"publication"},{"authors":["A Abrahams","RW Schlegel","AJ Smit"],"categories":null,"content":"","date":1612306800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612306800,"objectID":"b34643b0d52c8d7539e708b72d18d23a","permalink":"https://theoceancode.netlify.app/publication/upwelling_detection/","publishdate":"2021-02-03T00:00:00+01:00","relpermalink":"/publication/upwelling_detection/","section":"publication","summary":"Global increases in temperature are altering land-sea temperature gradients. Bakun (1990) hypothesized that changes within these gradients will directly affect atmospheric pressure cells associated with the development of winds and will consequently impact upwelling patterns within ecologically important Eastern Boundary Upwelling Systems (EBUS). In this study we used daily time series of NOAA Optimally Interpolated sea surface temperature (SST) and ERA 5 reanalysis wind products to calculate a series novel of metrics related to upwelling dynamics. We then use these to objectively describe upwelling signals in terms of their frequency, intensity and duration throughout the four EBUS during summer months over the last 37 years (1982–2019). We found that a decrease (increase) in SST is associated with an increase (decrease) in the number of upwelling “events,” a decrease (increase) in the intensity of upwelling, and an increase (decrease) in the cumulative intensity of upwelling, with differences between EBUS and regions within EBUS. The Humboldt Current is the only EBUS that shows a consistent response from north to south with a general intensification of upwelling. However, we could not provide clear evidence for associated changes in the wind dynamics hypothesized to drive the upwelling dynamics.","tags":["R","coastal","atmosphere","ocean"],"title":"Variation and Change of Upwelling Dynamics Detected in the World’s Eastern Boundary Upwelling Systems","type":"publication"},{"authors":["ECJ Oliver","JA Benthuysen","S Darmaraki","MG Donat","AJ Hobday","NJ Holbrook","RW Schlegel","A Sen Gupta"],"categories":null,"content":"","date":1609628400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609628400,"objectID":"bc1220b745bae1e5659bd1bed744d09d","permalink":"https://theoceancode.netlify.app/publication/mhw/","publishdate":"2021-01-03T00:00:00+01:00","relpermalink":"/publication/mhw/","section":"publication","summary":"Marine heatwaves (MHWs) are increasing in duration and intensity at a global scale and are projected to continue to increase due to the anthropogenic warming of the climate. Because MHWs may have drastic impacts on fisheries and other marine goods and services, there is a growing interest in understanding the predictability and developing practical predictions of these events. A necessary step toward prediction is to develop a better understanding of the drivers and processes responsible for the development of MHWs. Prior research has shown that air–sea heat flux and ocean advection across sharp thermal gradients are common physical processes governing these anomalous events. In this study we apply various statistical analyses and employ the self-organizing map (SOM) technique to determine specifically which of the many candidate physical processes, informed by a theoretical mixed-layer heat budget, have the most pronounced effect on the onset and/or decline of MHWs on the Northwest Atlantic continental shelf. It was found that latent heat flux is the most common driver of the onset of MHWs. Mixed layer depth (MLD) also strongly modulates the onset of MHWs. During the decay of MHWs, atmospheric forcing does not explain the evolution of the MHWs well, suggesting that oceanic processes are important in the decay of MHWs. The SOM analysis revealed three primary synoptic scale patterns during MHWs: low-pressure cyclonic Autumn-Winter systems, high-pressure anti-cyclonic Spring-Summer blocking, and mild but long-lasting Summer blocking. Our results show that nearly half of past MHWs on the Northwest Atlantic shelf are initiated by positive heat flux anomaly into the ocean, but less than one fifth of MHWs decay due to this process, suggesting that oceanic processes, e.g., advection and mixing are the primary driver for the decay of most MHWs.","tags":["R","marine heatwaves"],"title":"Marine Heatwaves","type":"publication"},{"authors":["RW Schlegel","S Darmaraki","JA Benthuysen","K Filbee-Dexter","ECJ Oliver"],"categories":null,"content":"","date":1609628400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609628400,"objectID":"f23837efef53ad13a42925dc379aa67b","permalink":"https://theoceancode.netlify.app/publication/mcs/","publishdate":"2021-01-03T00:00:00+01:00","relpermalink":"/publication/mcs/","section":"publication","summary":"Characterising ocean temperature variability and extremes is fundamental for understanding the thermal bounds in which marine ecosystems have adapted. While there is growing evidence of how marine heatwaves threaten marine ecosystems, prolonged periods of extremely cold ocean temperatures, marine cold-spells, have received less global attention. We synthesize the literature on cold ocean temperature extremes and their ecological impacts and physical mechanisms. Ecological impacts of these events were observed across a range of species and biophysical processes, including mass mortalities, range shifts, marine habitat loss, and altered phenology. The development of marine cold-spells is often due to wind-induced ocean processes, but a range of physical mechanisms are documented in the literature. Given the need for consistent comparison of marine cold-spells, we develop a definition for detecting these events from temperature time series and for classifying them into four categories. This definition is used to consistently detect marine cold-spells globally over the satellite record and to compare the characteristics of notable cold events. Globally, marine cold-spells’ occurrence, duration, and intensity are decreasing, with some areas, such as the Southern Ocean, showing signs of increase over the past 15 years. All marine cold-spell categories are affected by these decreases, with the exception of “IV Extreme” events, which were so rare that there has been little decrease. While decreasing occurrences of marine cold-spells could be viewed as providing a beneficial reduction in cold stress for marine ecosystems, fewer cold spells will alter the temperature regime that marine ecosystems experience and could have important consequences on ecological structure and function.","tags":["R","marine cold-spells"],"title":"Marine cold-spells","type":"publication"},{"authors":null,"categories":["R"],"content":" Objective While running some brief quality control tests on Bio-Oracle layers before using them for a recent project it was detected that some of the layers in the current version of the Bio-Oracle product appear to have very large errors. Specifically the error is that there are layers where the minimum values are greater than the maximum values. It is unclear how this could be possible, so in the following text and code we will look into how we go about investigating these data layers and we will discuss which layers are fine, and which are not. This error was first detected in the current velocity layers but a brief search turned up errors in other layers, too. So in this post we will be going through each individual layer to test for this max less than min error. We will look at all of the different depths as well as the future projections.\n# Load required libraries library(tidyverse) library(sdmpredictors) # The possible layers for download from Bio-Oracle BO_layers \u0026lt;- list_layers(datasets = \u0026quot;Bio-ORACLE\u0026quot;) # The future layers BO_layers_future \u0026lt;- list_layers_future(datasets = \u0026quot;Bio-ORACLE\u0026quot;)  Testing pipeline The following code chunk contains a function that will run the testing pipeline that highlights any errors in the data. To use it we choose a variable from the list of Bio-Oracle variables shown above that have a max and min version of the layer. One must replace the \u0026lsquo;max\u0026rsquo; or \u0026lsquo;min\u0026rsquo; with \u0026lsquo;X\u0026rsquo; and give that to the function, it will do the rest. Note that if one is running this script the figures this function will save to disk take about 1 minute to render due to their high resolution. Also please note that this function assumes there is a \u0026ldquo;figures\u0026rdquo; folder in the root directory on the computer on which this code is being run. If not, one must be created or the function must be changed to point to the desired folder.\nBO_test \u0026lt;- function(var_name, scenario = \u0026quot;present\u0026quot;, year = NA){ # Establish min/max layer names min_layer \u0026lt;- gsub(\u0026quot;X\u0026quot;, \u0026quot;min\u0026quot;, var_name) max_layer \u0026lt;- gsub(\u0026quot;X\u0026quot;, \u0026quot;max\u0026quot;, var_name) # Download data if(scenario == \u0026quot;present\u0026quot;){ BO_layers_dl \u0026lt;- load_layers(c(min_layer, max_layer)) var_title \u0026lt;- var_name } else { BO_layer_names \u0026lt;- get_future_layers(c(min_layer, max_layer), scenario = scenario, year = year) BO_layers_dl \u0026lt;- load_layers(BO_layer_names$layer_code) var_title \u0026lt;- gsub(\u0026quot;max_\u0026quot;, \u0026quot;X_\u0026quot;, BO_layer_names$layer_code[1]) } # Prepare data for plotting BO_layers_test \u0026lt;- as.data.frame(BO_layers_dl, xy = T) %\u0026gt;% dplyr::rename(lon = x, lat = y) %\u0026gt;% mutate(lon = round(lon, 4), lat = round(lat, 4)) %\u0026gt;% na.omit() %\u0026gt;% `colnames\u0026lt;-`(c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;, \u0026quot;min_val\u0026quot;, \u0026quot;max_val\u0026quot;)) %\u0026gt;% mutate(max_min = ifelse(max_val \u0026gt;= min_val, TRUE, FALSE)) # Visualise pixels where min values are greater than the max test_plot \u0026lt;- ggplot(data = BO_layers_test, aes(x = lon, y = lat)) + geom_raster(aes(fill = max_min)) + coord_quickmap(expand = F) + labs(fill = \u0026quot;Max greater than min\u0026quot;, x = NULL, y = NULL, title = var_title) + theme(legend.position = \u0026quot;bottom\u0026quot;) # Save figure to disk ggsave(paste0(\u0026quot;~/figures/\u0026quot;,var_title,\u0026quot;.png\u0026quot;), test_plot, height = 5, width = 8) }  Look at layers In this section we will go through all of the BO layers that have a min/max option and we will compare them to ascertain whether the maximum values are always greater than the minimums, which they should be, but we have found that sometimes this is not the case. When possible we will also look at future projections of the layers with RCP8.5 at 2050 and 2100. In the first code chunk in this section we will look at the older Bio-Oracle layers.\n# Up first we start with the older Bio-Oracle layers # Bathymetry BO_test(\u0026quot;BO_bathyX\u0026quot;) # No issues # Chlorophyll BO_test(\u0026quot;BO_chloX\u0026quot;) # No issues # Cloud fraction BO_test(\u0026quot;BO_cloudX\u0026quot;) # No issues # Diffuse attenuation BO_test(\u0026quot;BO_daX\u0026quot;) # No issues # SST BO_test(\u0026quot;BO_sstX\u0026quot;) # No issues  It is reassuring to see that all of the older BO layers have no issues in them. From my initial testing it looked like the layers with errors may have been from data assimilation from the GLORYS product for the most recent Bio-Oracle layers. That the older layers have no issues appears to support the hypothesis that the bug in the Bio-Oracle pipeline was introduced in the BO2 version of the product.\nThe next code chunk goes through all of the newer layers and where possible the future projections, too. Note that many layers have four different depth options. The surface (ss), the min (bdmin), the mean (bdmean), and the max (bdmax) depths present at each pixel. We are testing all of these as I have hypothesised that the inclusion of these three different depths may be responsible for some of the errors observed. Another distinction to make for the following tests is that there are min/max values for each layer, which take the absolute min/max recorded at a pixel. And then there are the long-term min/max values, which are the average annual min/max recorded over the length of the available data. The long-term min/max values are more representative of the climatological means within an area, and the absolute min/max are representative of the most extreme events that may occur in an area. Generally one is going to be more interested in the long-term values for normal species distribution modelling (SDM) applications. Because these min/max values are calculated differently it is necessary to test both of them to see if the errors in the data differ in any discernible way.\n# Carbon phytoplankton biomass absolute BO_test(\u0026quot;BO2_carbonphytoX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoX_ss\u0026quot;) # No issues # Carbon phytoplankton biomass long-term BO_test(\u0026quot;BO2_carbonphytoltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_carbonphytoltX_ss\u0026quot;) # No issues # Chlorophyll absolute BO_test(\u0026quot;BO2_chloX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Most pixels fail BO_test(\u0026quot;BO2_chloX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Slightly better than the 2050 data # Chlorophyll long-term BO_test(\u0026quot;BO2_chloltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloltX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_chloltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Same issues as absolute layer BO_test(\u0026quot;BO2_chloltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Slightly better than the 2050 data # Current velocities absolute BO_test(\u0026quot;BO2_curvelX_bdmax\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmean\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmin\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelX_ss\u0026quot;) # Mostly fails around the equator BO_test(\u0026quot;BO2_curvelX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Mirror image errors of present day BO_test(\u0026quot;BO2_curvelX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Mirror image errors of present day # Current velocities long-term # The first issue noted in the BO2 layers were these BO_test(\u0026quot;BO2_curvelltX_bdmax\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmean\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmin\u0026quot;) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_curvelltX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_curvelltX_ss\u0026quot;) # Mostly fails around the equator BO_test(\u0026quot;BO2_curvelltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Mirror image errors of present day BO_test(\u0026quot;BO2_curvelltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Mirror image errors of present day # Dissolved oxygen BO_test(\u0026quot;BO2_dissoxX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxX_ss\u0026quot;) # No issues # Dissolved oxygen long-term BO_test(\u0026quot;BO2_dissoxltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_dissoxltX_ss\u0026quot;) # No issues # Ice cover BO_test(\u0026quot;BO2_icecoverX_ss\u0026quot;) # No issues # Ice cover long-term BO_test(\u0026quot;BO2_icecoverltX_ss\u0026quot;) # No issues # Ice thickness BO_test(\u0026quot;BO2_icethickX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_icethickX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # All ice layers areas appear to be wrong BO_test(\u0026quot;BO2_icethickX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # All ice layers areas appear to be wrong # Ice thickness long-term BO_test(\u0026quot;BO2_icethickltX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_icethickltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # All ice layers areas appear to be wrong BO_test(\u0026quot;BO2_icethickltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # All ice layers areas appear to be wrong # Iron BO_test(\u0026quot;BO2_ironX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironX_ss\u0026quot;) # No issues # Iron long-term BO_test(\u0026quot;BO2_ironltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_ironltX_ss\u0026quot;) # No issues # Light at bottom BO_test(\u0026quot;BO2_lightbotX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_lightbotX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_lightbotX_bdmin\u0026quot;) # No issues # Light at bottom long-term BO_test(\u0026quot;BO2_lightbotltX_bdmax\u0026quot;) # Global errors different from current velocity errors BO_test(\u0026quot;BO2_lightbotltX_bdmean\u0026quot;) # Global errors different from current velocity errors BO_test(\u0026quot;BO2_lightbotltX_bdmin\u0026quot;) # Global errors different from current velocity errors # Nitrate BO_test(\u0026quot;BO2_nitratemax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitratemax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitratemax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitratemax_ss\u0026quot;) # No issues # Nitrate long-term BO_test(\u0026quot;BO2_nitrateltmax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitrateltmax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitrateltmax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_nitrateltmax_ss\u0026quot;) # No issues # Phosphate absolute BO_test(\u0026quot;BO2_phosphateX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateX_ss\u0026quot;) # No issues # Phosphate long-term BO_test(\u0026quot;BO2_phosphateltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_phosphateltX_ss\u0026quot;) # No issues # Primary production absolute BO_test(\u0026quot;BO2_ppmax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppmax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppmax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppmax_ss\u0026quot;) # No issues # Primary production long-term BO_test(\u0026quot;BO2_ppltmax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppltmax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppltmax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_ppltmax_ss\u0026quot;) # No issues # Salinity absolute BO_test(\u0026quot;BO2_salinityX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Most pixels fail BO_test(\u0026quot;BO2_salinityX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_salinityX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail # Salinity long-term BO_test(\u0026quot;BO2_salinityltX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityltX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_salinityltX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_salinityltX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityltX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_salinityltX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_salinityltX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityltX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Global issues BO_test(\u0026quot;BO2_salinityltX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Global issues BO_test(\u0026quot;BO2_salinityltX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_salinityltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_salinityltX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail # Silicate absolute BO_test(\u0026quot;BO2_silicatemax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicatemax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicatemax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicatemax_ss\u0026quot;) # No issues # Silicate long-term BO_test(\u0026quot;BO2_silicateltmax_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicateltmax_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicateltmax_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_silicateltmax_ss\u0026quot;) # No issues # Temperature absolute BO_test(\u0026quot;BO2_tempX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_tempX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_tempX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_tempX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_tempX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_tempX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # All pixels fail BO_test(\u0026quot;BO2_tempX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # All pixels fail # Temperature long-term BO_test(\u0026quot;BO2_templtX_bdmax\u0026quot;) # No issues BO_test(\u0026quot;BO2_templtX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_bdmax\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_bdmean\u0026quot;) # No issues BO_test(\u0026quot;BO2_templtX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_bdmean\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_bdmin\u0026quot;) # No issues BO_test(\u0026quot;BO2_templtX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_bdmin\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # Almost all pixels fail BO_test(\u0026quot;BO2_templtX_ss\u0026quot;) # No issues BO_test(\u0026quot;BO2_templtX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2050) # All but a few pixels fail BO_test(\u0026quot;BO2_templtX_ss\u0026quot;, scenario = \u0026quot;RCP85\u0026quot;, year = 2100) # All but a few different pixels fail  Discussion This is not meant to be exhaustive, but does represent the majority of the data layers offered by Bio-Oracle. I had hypothesised that the three different depth options per pixel would have been related to the issues in the data assimilation but it does not appear to be the case. I\u0026rsquo;ve concluded this because whenever a depth layer has issues, those errors appear to be very similar across the three different depth layers. One consistent pattern we do see is if there are any errors, then every depth layer for that variable will also have errors. This also applies to absolute vs. long-term max/min layers. Errors in one means errors in all. Another consistent pattern in the error was that all of the future projections at all depths for absolute and long-term min/max values had errors in them. This begs the question of how it is that the future projection can have no errors, while the present day layers do not. How are these future projection layers calculated differently from the present day? Are they not based on the same data?\nMost of the layers that have issues are physical layers. It is my understanding that these layers would have been adapted from the GLORYS reanalysis product. Therefore the next logical step in understanding this issue would be to investigate the GLORYS data. But even if there were issues in the GLORYS product, which I doubt, it would not explain how the data layers here could have minimum values being reported as greater than the maximum values in the distribution. The only thing that makes any sense is that the data layers are created independently of each other. But why?\nWithout being able to see the pipeline code myself all I can do is ponder, which isn\u0026rsquo;t terribly useful. So to wrap things up I\u0026rsquo;ll provide two tables; the layers with no issues, and those with issues. I would strongly recommend against using any data layers that did not pass the tests in this analysis until the curators of the Bio-Oracle data address these issues in a future release/version.\nLayers with no issues (fine for use): - All of the older BO layers appear fine - Carbon phytoplankton biomass absolute and long-term at all depths - Chlorophyll absolute and long-term at all depths for present day projections only - Dissolved oxygen absolute and long-term at all depths - Ice cover absolute and long-term - Ice thickness absolute and long-term for present day projections only - Iron absolute and long-term for all depths - Nitrate absolute and long-term for all depths - Phosphate absolute and long-term for all depths - Primary productivity absolute and long-term for all depths - Salinity absolute and long-term at all depths for present day projections only - Temperature absolute and long-term at all depths for present day projections only\nProblem layers (do not use): - Chlorophyll future projections for absolutes and long-terms at all depths - Current velocity absolutes and long-terms for all depths and all present and future projections - Ice thickness absolute and long-term for future projections - Light at bottom absolute and long-term for all depths - Salinity absolute and long-term at all depths for future projections - Temperature absolute and long-term at all depths for future projections\n","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"48593584279ea66b7bfa4846cc3596fb","permalink":"https://theoceancode.netlify.app/post/bo_analysis/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/post/bo_analysis/","section":"post","summary":"Objective While running some brief quality control tests on Bio-Oracle layers before using them for a recent project it was detected that some of the layers in the current version of the Bio-Oracle product appear to have very large errors. Specifically the error is that there are layers where the minimum values are greater than the maximum values. It is unclear how this could be possible, so in the following text and code we will look into how we go about investigating these data layers and we will discuss which layers are fine, and which are not.","tags":["data","SDM"],"title":"Analysis of Bio-Oracle data","type":"post"},{"authors":["J Goldsmit","CW McKindsey","RW Schlegel","DB Stewart","P Archambault","KL Howland"],"categories":null,"content":"","date":1589407200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589407200,"objectID":"547c73bd9edcf84cd9dbc6eb27e4dbdf","permalink":"https://theoceancode.netlify.app/publication/what_where/","publishdate":"2020-05-14T00:00:00+02:00","relpermalink":"/publication/what_where/","section":"publication","summary":"The risk of aquatic invasions in the Arctic is expected to increase with climate warming, greater shipping activity and resource exploitation in the region. Planktonic and benthic marine aquatic invasive species (AIS) with the greatest potential for invasion and impact in the Canadian Arctic were identified and the 23 riskiest species were modelled to predict their potential spatial distributions at pan-Arctic and global scales. Modelling was conducted under present environmental conditions and two intermediate future (2050 and 2100) global warming scenarios. Invasion hotspots—regions of the Arctic where habitat is predicted to be suitable for a high number of potential AIS—were located in Hudson Bay, Northern Grand Banks/Labrador, Chukchi/Eastern Bering seas and Barents/White seas, suggesting that these regions could be more vulnerable to invasions. Globally, both benthic and planktonic organisms showed a future poleward shift in suitable habitat. At a pan-Arctic scale, all organisms showed suitable habitat gains under future conditions. However, at the global scale, habitat loss was predicted in more tropical regions for some taxa, particularly most planktonic species. Results from the present study can help prioritize management efforts in the face of climate change in the Arctic marine ecosystem. Moreover, this particular approach provides information to identify present and future high-risk areas for AIS in response to global warming.","tags":["R","SDM"],"title":"What and where? Predicting invasion hotspots in the Arctic marine realm","type":"publication"},{"authors":null,"categories":["R"],"content":" Objective Having been working in environmental science for several years now, entirely using R, I\u0026rsquo;ve come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn\u0026rsquo;t be. Most data hosting organisations do want scientists to use their data and do make it freely available. But sometimes it feels like the path to access was designed by crab people, rather than normal topside humans. I recently needed to gather several new data products and in classic \u0026lsquo;cut your nose off to spite your face\u0026rsquo; fashion I insisted on doing all of it directly through an R script that could be run in RStudio. Besides being stubborn, one of the main reasons I felt this was necessary is that I wanted these download scripts to be able to be run operationally via a cron job. I think I came out pretty successful in the end so wanted to share the code with the rest of the internet. Enjoy.\n# Packages not available via CRAN remotes::install_github(\u0026quot;skgrange/threadr\u0026quot;) remotes::install_github(\u0026quot;markpayneatwork/RCMEMS\u0026quot;) # The packages we will use library(tidyverse) # A staple for most modern data management in R library(RCurl) # For helping R to make sense of URLs for web hosted data library(XML) # For reading the HTML tables created by RCurl library(tidync) # For easily dealing with NetCDF data library(doParallel) # For parallel processing library(threadr) # For downloading from FTP sites that require user credentials library(RCMEMS) # For subsetting CMEMS data before download  Downloading NOAA OISST I\u0026rsquo;ve already written a post about how to download NOAA OISST data using the rerddap package which may be found here. That post talks about how to get subsets of NOAA data, which is useful for projects with a refined scope, but it is laboriously slow if one simply wants the full global product. It must also be noted that as of this writing (June 3rd, 2020) the new OISST v2.1 data were not yet available on the ERDDAP server even though the old v2 data have now been rendered unavailable. For the time being it is necessary to download the full global data and then subset down to one\u0026rsquo;s desired study area. The following section of this blog post will outline how to do that.\nI need to stress that this is a very direct and unlimited method for accessing these data and I urge responsibility in only downloading as much data as are necessary. Please do not download the entire dataset just because you can.\n# First we tell R where the data are on the interwebs OISST_url_month \u0026lt;- \u0026quot;https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/\u0026quot; # Then we pull that into a happy format # There is a lot here so it takes ~1 minute OISST_url_month_get \u0026lt;- getURL(OISST_url_month) # Before we continue let's set a limit on the data we are going to download # NB: One should not simply download the entire dataset just because it is possible. # There should be a compelling reason for doing so. start_date \u0026lt;- as.Date(\u0026quot;2019-01-01\u0026quot;) # Now we strip away all of the unneeded stuff to get just the months of data that are available OISST_months \u0026lt;- data.frame(months = readHTMLTable(OISST_url_month_get, skip.rows = 1:2)[[1]]$Name) %\u0026gt;% mutate(months = lubridate::as_date(str_replace(as.character(months), \u0026quot;/\u0026quot;, \u0026quot;01\u0026quot;))) %\u0026gt;% filter(months \u0026gt;= max(lubridate::floor_date(start_date, unit = \u0026quot;month\u0026quot;))) %\u0026gt;% # Filtering out months before Jan 2019 mutate(months = gsub(\u0026quot;-\u0026quot;, \u0026quot;\u0026quot;, substr(months, 1, 7))) %\u0026gt;% na.omit() # Up next we need to now find the URLs for each individual day of data # To do this we will wrap the following chunk of code into a function so we can loop it more easily OISST_url_daily \u0026lt;- function(target_month){ OISST_url \u0026lt;- paste0(OISST_url_month, target_month,\u0026quot;/\u0026quot;) OISST_url_get \u0026lt;- getURL(OISST_url) OISST_table \u0026lt;- data.frame(files = readHTMLTable(OISST_url_get, skip.rows = 1:2)[[1]]$Name) %\u0026gt;% mutate(files = as.character(files)) %\u0026gt;% filter(grepl(\u0026quot;avhrr\u0026quot;, files)) %\u0026gt;% mutate(t = lubridate::as_date(sapply(strsplit(files, \u0026quot;[.]\u0026quot;), \u0026quot;[[\u0026quot;, 2)), full_name = paste0(OISST_url, files)) return(OISST_table) } # Here we collect the URLs for every day of data available from 2019 onwards OISST_filenames \u0026lt;- plyr::ldply(OISST_months$months, .fun = OISST_url_daily) # Just to keep things tidy in this vignette I am now going to limit this data collection even further OISST_filenames \u0026lt;- OISST_filenames %\u0026gt;% filter(t \u0026lt;= \u0026quot;2019-01-31\u0026quot;) # This function will go about downloading each day of data as a NetCDF file # We will run this via plyr to expedite the process # Note that this will download files into a 'data/OISST' folder in the root directory # If this folder does not exist it will create it # This function will also check if the file has been previously downloaded OISST_url_daily_dl \u0026lt;- function(target_URL){ dir.create(\u0026quot;~/data/OISST\u0026quot;, showWarnings = F) file_name \u0026lt;- paste0(\u0026quot;~/data/OISST/\u0026quot;,sapply(strsplit(target_URL, split = \u0026quot;/\u0026quot;), \u0026quot;[[\u0026quot;, 10)) if(!file.exists(file_name)) download.file(url = target_URL, method = \u0026quot;libcurl\u0026quot;, destfile = file_name) } # The way this code has been written it may be run on multiple cores # Most modern laptops have at least 4 cores, so we will utilise 3 of them here # One should always leave at least 1 core free doParallel::registerDoParallel(cores = 3) # And with that we are clear for take off system.time(plyr::ldply(OISST_filenames$full_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds # In roughly 15 seconds a user may have a full month of global data downloaded # This scales well into years and decades, too  Because it is not currently possible to download subsetted OISST data from a GRIDDAP server I find that it is useful to include here the code one would use to load and subset downloaded OISST data. Please note that the OISST data have longitude values from 0 to 360, not -180 to 180.\n# This function will load and subset daily data into one data.frame # Note that the subsetting of lon/lat is done before the data are loaded # This means it will use much less RAM and is viable for use on most laptops # Assuming one's study area is not too large OISST_load \u0026lt;- function(file_name, lon1, lon2, lat1, lat2){ OISST_dat \u0026lt;- tidync(file_name) %\u0026gt;% hyper_filter(lon = between(lon, lon1, lon2), lat = between(lat, lat1, lat2)) %\u0026gt;% hyper_tibble() %\u0026gt;% select(lon, lat, time, sst) %\u0026gt;% dplyr::rename(t = time, temp = sst) %\u0026gt;% mutate(t = as.Date(t, origin = \u0026quot;1978-01-01\u0026quot;)) return(OISST_dat) } # Locate the files that will be loaded OISST_files \u0026lt;- dir(\u0026quot;~/data/OISST\u0026quot;, full.names = T) # Load the data in parallel OISST_dat \u0026lt;- plyr::ldply(.data = OISST_files, .fun = OISST_load, .parallel = T, lon1 = 260, lon2 = 280, lat1 = 30, lat2 = 50) # This should only take a few seconds to run at most  Downloading CCI An up-and-coming star in the world of remotely sensed data products, the Climate Change Initiative (CCI) has recently been putting out some industry leading products. These are all freely available for access and use for scientific research purposes. These have quickly become regarded as the most accurate products available and their use is now encouraged over other products. Unfortunately they are not available in near-real-time and so can currently only be used for historic analyses. A recent update of these data for 2017 and 2018 was made available and one assumes that 2019 will follow suit some time by the end of 2020.\n# The URLs where the data are housed for direct download # NB: Note that the versions are different; v2.1 vs. v2.0 # NB: It looks like going straight through the thredds server is a more stable option CCI_URL_old \u0026lt;- \u0026quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/esacci/sst/data/CDR_v2/Analysis/L4/v2.1\u0026quot; CCI_URL_new \u0026lt;- \u0026quot;http://dap.ceda.ac.uk/thredds/fileServer/neodc/c3s_sst/data/ICDR_v2/Analysis/L4/v2.0\u0026quot; # The date ranges that are housed therein # NB: These are historic repos and therefore the dates are static # I assume that the 'new' data will be updated through 2019 by the end of 2020 date_range_old \u0026lt;- seq(as.Date(\u0026quot;1981-09-01\u0026quot;), as.Date(\u0026quot;2016-12-31\u0026quot;), by = \u0026quot;day\u0026quot;) date_range_new \u0026lt;- seq(as.Date(\u0026quot;2017-01-01\u0026quot;), as.Date(\u0026quot;2018-12-31\u0026quot;), by = \u0026quot;day\u0026quot;) # The function we will use to download the data download_CCI \u0026lt;- function(date_choice, CCI_URL){ # Prep the necessary URL pieces date_slash \u0026lt;- str_replace_all(date_choice, \u0026quot;-\u0026quot;, \u0026quot;/\u0026quot;) date_nogap \u0026lt;- str_replace_all(date_choice, \u0026quot;-\u0026quot;, \u0026quot;\u0026quot;) if(str_detect(CCI_URL, \u0026quot;esacci\u0026quot;)){ tail_chunk \u0026lt;- \u0026quot;120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.nc\u0026quot; } else if(str_detect(CCI_URL, \u0026quot;c3s_sst\u0026quot;)){ tail_chunk \u0026lt;- \u0026quot;120000-C3S-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR2.0-v02.0-fv01.0.nc\u0026quot; } else{ stop(\u0026quot;The URL structure has changed.\u0026quot;) } complete_URL \u0026lt;- paste0(CCI_URL,\u0026quot;/\u0026quot;,date_slash,\u0026quot;/\u0026quot;,date_nogap,tail_chunk) # Note that this will download the files to data/CCI in the root directory file_name \u0026lt;- paste0(\u0026quot;~/data/CCI/\u0026quot;,date_nogap,tail_chunk) # Download and save the file if needed if(file.exists(file_name)){ return() } else{ download.file(url = complete_URL, method = \u0026quot;libcurl\u0026quot;, destfile = file_name) } Sys.sleep(2) # Give the server a quick breather } # Run in parallel # Most laptops have 4 cores, so 3 is a good choice doParallel::registerDoParallel(cores = 3) # Download all old data: 1981-09-01 to 2016-12-31 # NB: Note the '[1:3]' below. This limits the downloads to only the first three files # Delete that to download everything. # But please do not download ALL of the files unless there is a need to do so. plyr::l_ply(date_range_old[1:3], .fun = download_CCI, CCI_URL = CCI_URL_old, .parallel = T) # Download all new data: 2016-01-01 to 2018-12-31 plyr::l_ply(date_range_new[1:3], .fun = download_CCI, CCI_URL = CCI_URL_new, .parallel = T)  Downloading OSTIA As noted above, CCI data products are quickly becoming the preferred standard. Unfortunately they are not available in near-real-time. This is where OSTIA data come in to fill the gap. Though not exactly the same assimilation process as CCI, these products come from the same suite of data sources. I do not yet know if these data for 2019 onwards can be used in combination with a climatology created from the CCI data, but it is on my to do list to find out. In order to download these data one will need to have a CMEMS account. This is free for researchers and very fast to sign up for. Once one has received a user name and password it is possible to use the code below to download the data via their FTP server. No Python required!\n# The URL where the data are housed for FTP OSTIA_URL \u0026lt;- \u0026quot;ftp://nrt.cmems-du.eu/Core/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2\u0026quot; # The date ranges that are housed therein # NB: These are historic repos and therefore the dates are static # I assume that the 'new' data will be updated through 2019 by the end of 2020 date_range \u0026lt;- seq(as.Date(\u0026quot;2019-01-01\u0026quot;), as.Date(\u0026quot;2019-01-03\u0026quot;), by = \u0026quot;day\u0026quot;) # Enter ones credentials here # Note that the ':' between 'username' and 'password' is required user_credentials \u0026lt;- \u0026quot;username:password\u0026quot; # Download function download_OSTIA \u0026lt;- function(date_choice, user_credentials){ # Prep the necessary URL pieces date_slash \u0026lt;- strtrim(str_replace_all(date_choice, \u0026quot;-\u0026quot;, \u0026quot;/\u0026quot;), width = 7) date_nogap_day \u0026lt;- str_replace_all(date_choice, \u0026quot;-\u0026quot;, \u0026quot;\u0026quot;) tail_chunk \u0026lt;- \u0026quot;120000-UKMO-L4_GHRSST-SSTfnd-OSTIA-GLOB-v02.0-fv02.0.nc\u0026quot; complete_URL \u0026lt;- paste0(OSTIA_URL,\u0026quot;/\u0026quot;,date_slash,\u0026quot;/\u0026quot;,date_nogap_day,tail_chunk) file_name \u0026lt;- paste0(\u0026quot;~/data/OSTIA/\u0026quot;,date_nogap_day,tail_chunk) # Download and save the file if needed if(file.exists(file_name)){ return() } else{ download_ftp_file(complete_URL, file_name, verbose = TRUE, credentials = user_credentials) } Sys.sleep(2) # Give the server a quick breather } # Run in parallel doParallel::registerDoParallel(cores = 3) # Download data from 2019-01-01 to present day # NB: Some files won't download when run in parallel # I think the FTP server may be more aware of multiple requests than an HTTPS server # It may also have been due to high server use at the time, too plyr::l_ply(date_range, .fun = download_OSTIA, .parallel = T, user_credentials = user_credentials)  Downloading GLORYS At this point one may be thinking \u0026ldquo;wait a second, these have all been SST only products, this isn\u0026rsquo;t really a post about environmental data!\u0026rdquo;. But that\u0026rsquo;s where GLORYS comes in. This product has a range of variables one may be interested in. Not just SST. But yes, they are all physical variables. No bio-geochemistry in sight. Bamboozled! But if you\u0026rsquo;ve read this far, why stop now?! These data require a CMEMS account, same as the OSTIA data. They can also be downloaded via direct FTP access, but these files are enormous so in almost every use case this is not what one is intending to do. Rather these data are almost always subsetted in some way first. Luckily CMEMS has made this available to their user base with the introduction of the MOTU client. Unluckily they have only made this available for use in Python. I have asked the people behind this process in person if there are plans for an officially supported R version and the short and long answers were no. That\u0026rsquo;s where Mark Payne and his RCMEMS package enter the picture. He has wrapped the MOTU client for Python up in a handy R package that allows us to access, subset, and download CMEMS data all through the comfort of the RStudio interface! There is a tutorial on the GitHub repo that walks through the process that I show below if one would like an additional look at this process.\n# Non-R software # Unfortunately the subsetting of CMEMS data will require that one has Python installed # https://www.python.org/downloads/ # Then one must download # https://github.com/clstoulouse/motu-client-python/releases # For instructions on how and why to properly install please see: # https://github.com/markpayneatwork/RCMEMS # Here is a cunning method of generating a brick of year-month values date_range \u0026lt;- base::expand.grid(1993:2018, 1:12) %\u0026gt;% dplyr::rename(year = Var1, month = Var2) %\u0026gt;% arrange(year, month) %\u0026gt;% mutate(year_mon = paste0(year,\u0026quot;-\u0026quot;,month)) %\u0026gt;% dplyr::select(year_mon) # Download function # NB: This function is currently designed to subset data to a specific domain # Please change your lon/lat accordingly # NB: This function will save files to data/GLORYS in the root directory # To change this change the --out-dir argument near the end of the chunk of text # NB: This big text chunk needs to be left as one long line # NB: The --user and --pwd arguments need to be given the users real username and passwords # from their CMEMS account download_GLORYS \u0026lt;- function(date_choice){ # The GLORYS script # This is a dummy script first generated by using the UI on the CMEMS website # No need to change anything here except for the --user and --pwd at the end # Please place your CMEMS username and password in those fields GLORYS_script \u0026lt;- 'python ~/motuclient-python/motuclient.py --motu http://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_030-TDS --product-id global-reanalysis-phy-001-030-daily --longitude-min -180 --longitude-max 179.9166717529297 --latitude-min -80 --latitude-max 90 --date-min \u0026quot;2018-12-25 12:00:00\u0026quot; --date-max \u0026quot;2018-12-25 12:00:00\u0026quot; --depth-min 0.493 --depth-max 0.4942 --variable thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi --out-dir data --out-name test.nc --user username --pwd password' # Prep the necessary URL pieces date_start \u0026lt;- parse_date(date_choice, format = \u0026quot;%Y-%m\u0026quot;) # A clever way of finding the end date of any month! # I found this on stackoverflow somewhere... date_end \u0026lt;- date_start %m+% months(1) - 1 # Cannot get data past 2018-12-25 if(date_end \u0026gt; as.Date(\u0026quot;2018-12-25\u0026quot;)) date_end \u0026lt;- as.Date(\u0026quot;2018-12-25\u0026quot;) # Set the file name file_name \u0026lt;- paste0(\u0026quot;GLORYS_\u0026quot;,date_choice,\u0026quot;.nc\u0026quot;) # Take the chunk of code above and turn it into something useful cfg \u0026lt;- parse.CMEMS.script(GLORYS_script, parse.user = T) # This is where one should make any required changes to the subsetting of the data # This is now the magic of the RCMEMS package, which allows us to interface with the Python code as though it were R cfg_update \u0026lt;- RCMEMS::update(cfg, variable = \u0026quot;thetao --variable bottomT --variable so --variable zos --variable uo --variable vo --variable mlotst --variable siconc --variable sithick --variable usi --variable vsi\u0026quot;, longitude.min = \u0026quot;-80.5\u0026quot;, longitude.max = \u0026quot;-40.5\u0026quot;, latitude.min = \u0026quot;31.5\u0026quot;, latitude.max = \u0026quot;63.5\u0026quot;, date.min = as.character(date_start), date.max = as.character(date_end), out.name = file_name) # Download and save the file if needed if(file.exists(paste0(\u0026quot;~/data/GLORYS/\u0026quot;,file_name))){ return() } else{ CMEMS.download(cfg_update) } Sys.sleep(2) # Give the server a quick breather } # I've limited the download to only 1 file # Delete '[1]' to download everything #NB: The CMEMS server is a little wonky, rather not try to multicore this plyr::l_ply(date_range$year_mon[1], .fun = download_GLORYS, .parallel = F)  Conclusion I hope this has been a useful whack of data for anyone looking to download any of these products for their science. The techniques laid out in the code here should apply to most other data products as well as there aren\u0026rsquo;t that many different methods of hosting data. If I\u0026rsquo;ve missed anything that people feel is an important data source that can\u0026rsquo;t be adapted from the code here let me know and I\u0026rsquo;m happy to see what I can do.\n","date":1581638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581638400,"objectID":"2f524cd1f0a10410e5085dbdb3b0870a","permalink":"https://theoceancode.netlify.app/post/dl_env_data_r/","publishdate":"2020-02-14T00:00:00Z","relpermalink":"/post/dl_env_data_r/","section":"post","summary":"Objective Having been working in environmental science for several years now, entirely using R, I\u0026rsquo;ve come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn\u0026rsquo;t be. Most data hosting organisations do want scientists to use their data and do make it freely available.","tags":["environmental","data","download"],"title":"Downloading environmental data in R","type":"post"},{"authors":["RW Schlegel","ECJ Oliver","AJ Hobday","AJ Smit"],"categories":null,"content":"","date":1576364400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576364400,"objectID":"8f8477b5796870138c2b80d6d7e60083","permalink":"https://theoceancode.netlify.app/publication/detection/","publishdate":"2019-12-15T00:00:00+01:00","relpermalink":"/publication/detection/","section":"publication","summary":"Marine heatwaves (MHWs), or prolonged periods of anomalously warm sea water temperature, have been increasing in duration and intensity globally for decades. However, there are many coastal, oceanic, polar, and sub-surface regions where our ability to detect MHWs is uncertain due to limited high quality data. Here, we investigate the effect that short time series length, missing data, or linear long-term temperature trends may have on the detection of MHWs. We show that MHWs detected in time series as short as 10 years did not have durations or intensities appreciably different from events detected in a standard 30 year long time series. We also show that the output of our MHW algorithm for time series missing less than 25% data did not differ appreciably from a complete time series, and that the level of allowable missing data could cautiously be increased to 50% when gaps were filled by linear interpolation. Finally, linear long-term trends of 0.10°C/decade or greater added to a time series caused larger changes (increases) to the count and duration of detected MHWs than shortening a time series to 10 years or missing more than 25% of the data. The long-term trend in a time series has the largest effect on the detection of MHWs and has the largest range in added uncertainty in the results. Time series length has less of an effect on MHW detection than missing data, but adds a larger range of uncertainty to the results. We provide suggestions for best practices to improve the accuracy of MHW detection with sub-optimal time series and show how the accuracy of these corrections may change regionally.","tags":["R","marine heatwaves"],"title":"Detecting Marine Heatwaves With Sub-Optimal Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1576364400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576364400,"objectID":"80683243cdd84ab14ace23e27d1ef78c","permalink":"https://theoceancode.netlify.app/package/heatwaver/","publishdate":"2019-12-15T00:00:00+01:00","relpermalink":"/package/heatwaver/","section":"package","summary":"An R package for the detection of heatwaves and cold-spells.","tags":["marine heatwaves","R"],"title":"heatwaveR","type":"package"},{"authors":null,"categories":null,"content":"","date":1576278000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576278000,"objectID":"c878226291a427aa835ed7f884c45ae7","permalink":"https://theoceancode.netlify.app/package/coastr/","publishdate":"2019-12-14T00:00:00+01:00","relpermalink":"/package/coastr/","section":"package","summary":"An R package with useful functions for coastal oceanography.","tags":["coastal","R"],"title":"coastR","type":"package"},{"authors":null,"categories":null,"content":"","date":1557093600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557093600,"objectID":"ab06caae2f4aaeec7310038b72ff7534","permalink":"https://theoceancode.netlify.app/poster/mhw_detection_poster/","publishdate":"2019-05-06T00:00:00+02:00","relpermalink":"/poster/mhw_detection_poster/","section":"poster","summary":"Research conducted on how sub-optimal a time series may be and still produce comparable marine heatwaves to an optimal time series.","tags":["marine heatwaves","R"],"title":"Detecting marine heatwaves with sub-optimal data","type":"poster"},{"authors":null,"categories":null,"content":"","date":1553468400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553468400,"objectID":"3931e1ef172d9d676500b31413bf66b5","permalink":"https://theoceancode.netlify.app/project/demomhw/","publishdate":"2019-03-25T00:00:00+01:00","relpermalink":"/project/demomhw/","section":"project","summary":"An interactive application that allows the user to explore the components of the MHW definition.","tags":["marine heatwaves"],"title":"Demo for the MHW definition","type":"project"},{"authors":null,"categories":null,"content":"","date":1553468400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553468400,"objectID":"60f603ab4bf81263197ca2919c94d98e","permalink":"https://theoceancode.netlify.app/project/mhwtracker/","publishdate":"2019-03-25T00:00:00+01:00","relpermalink":"/project/mhwtracker/","section":"project","summary":"A daily updating web application for tracking the occurrence of marine heatwaves around the globe.","tags":["marine heatwaves"],"title":"Marine heatwave tracker","type":"project"},{"authors":null,"categories":null,"content":"","date":1552950000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552950000,"objectID":"723a6df0678636279dea984872dd1ec8","permalink":"https://theoceancode.netlify.app/talk/mhw_2019/","publishdate":"2019-03-19T00:00:00+01:00","relpermalink":"/talk/mhw_2019/","section":"talk","summary":"","tags":[],"title":"Marine heatwaves: the new normal","type":"talk"},{"authors":["RW Schlegel"],"categories":null,"content":"","date":1543618800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543618800,"objectID":"3960b1f84910b04e1ef375a27f6cd585","permalink":"https://theoceancode.netlify.app/publication/tracker/","publishdate":"2018-12-01T00:00:00+01:00","relpermalink":"/publication/tracker/","section":"publication","summary":"The Marine Heatwave Tracker software provides all of the tools necessary to download, process, and visualise marine heatwaves (MHWs) globally. This web application may be set up to show the occurrence of MHWs around the world in near-real-time (roughly a one-two day delay) with the use of a cron job that runs a bash script calling 'MHW_daily.R'. The Tracker also show the historic records for the entire planet going back to January 1st, 1982. There are several other data layers offered in the Tracker. Please see the 'How do I use this?' section on the app for further details (http://www.marineheatwaves.org/tracker.html).","tags":["R","marine heatwaves"],"title":"Marine Heatwave Tracker: The app to see when and where marine heatwaves are happening around the world","type":"publication"},{"authors":["RW Schlegel","AJ Smit"],"categories":null,"content":"","date":1535666400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535666400,"objectID":"65e60d72de4212cc6e6789900b2a3e87","permalink":"https://theoceancode.netlify.app/publication/heatwaver/","publishdate":"2018-08-31T00:00:00+02:00","relpermalink":"/publication/heatwaver/","section":"publication","summary":"","tags":["R","marine heatwaves"],"title":"heatwaveR: A central algorithm for the detection of heatwaves and cold-spells","type":"publication"},{"authors":null,"categories":["R"],"content":" Objective In South Africa there are a range of idioms for different time frames in which someone may (or may not) do something. The most common of these are: \u0026lsquo;now\u0026rsquo;, \u0026lsquo;just now\u0026rsquo;, and \u0026lsquo;now now\u0026rsquo;. If one were to Google these sayings one would find that there is general agreements on how long these time frames are, but that agreement is not absolute.\nThis got me to wondering just how much disagreement there may be around the country. And more specifically I wanted to know how these times changed between specific locations. If one is interested in contributing to the survey, it may be taken here. To avoid too much confusion with the answers that could be given, specific times were provided to the participants in a multiple choice format. These times (minutes) were: 5, 15, 30, 60, 120, 300.\nData prep The survey data are downloaded from Google Forms rather easily as a .csv file, so that\u0026rsquo;s nice. Unfortunately, the way in which one sets up the survey for humans is difficult for R to understand so we need quite a bit of processing after we load the data.\nlibrary(tidyverse) library(ggpubr) library(broom) survey \u0026lt;- read_csv(\u0026quot;../../static/data/SA Time Survey.csv\u0026quot;, skip = 1, col_types = \u0026quot;cccccc\u0026quot;, col_names = c(\u0026quot;time\u0026quot;, \u0026quot;just now\u0026quot;, \u0026quot;now now\u0026quot;, \u0026quot;now\u0026quot;, \u0026quot;province\u0026quot;, \u0026quot;city\u0026quot;)) %\u0026gt;% select(-time) %\u0026gt;% mutate(`just now` = as.numeric(sapply(strsplit(`just now`, \u0026quot; \u0026quot;), \u0026quot;[[\u0026quot;, 1)), `now now` = as.numeric(sapply(strsplit(`now now`, \u0026quot; \u0026quot;), \u0026quot;[[\u0026quot;, 1)), `now` = as.numeric(sapply(strsplit(`now`, \u0026quot; \u0026quot;), \u0026quot;[[\u0026quot;, 1)), province = gsub(\u0026quot;Kzn\u0026quot;, \u0026quot;KZN\u0026quot;, province), province = gsub(\u0026quot;GP\u0026quot;, \u0026quot;Gauteng\u0026quot;, province), province = as.factor(province), city = as.factor(city)) # Check that city and province names are all lekker # levels(survey$province) # levels(survey$city) # Create a long version for easier stats survey_long \u0026lt;- survey %\u0026gt;% gather(key = \u0026quot;saying\u0026quot;, value = \u0026quot;minutes\u0026quot;, -province, -city) %\u0026gt;% na.omit()  Participant locations With our data prepared, I first wanted to see from where the surveys were taken. I\u0026rsquo;ve done this by splitting them up into province or city. This is also one of the few situations in which a bar plot is an appropriate visualisation. The columns below that show \u0026lsquo;NA\u0026rsquo; are for participants that declined to share their location.\nprovince_plot \u0026lt;- ggplot(data = survey, aes(x = province)) + geom_bar(aes(fill = province), show.legend = F) + ggtitle(\u0026quot;Provinces of participants\u0026quot;) + labs(x = \u0026quot;\u0026quot;) # province_plot city_plot \u0026lt;- ggplot(data = survey, aes(x = city)) + geom_bar(aes(fill = city), show.legend = F) + ggtitle(\u0026quot;Cities of participants\u0026quot;) + labs(x = \u0026quot;\u0026quot;) + theme(axis.text.x = element_text(angle = 15)) # city_plot ggarrange(province_plot, city_plot, ncol = 1, nrow = 2)  Figure 1: Bar plots showing the total pariticpants by province or city.\n Participant time frames With the locations of of our participants visualised we now want to see what sort of time frames people around the country attribute to the three most common idioms.\nggplot(data = survey_long, aes(x = saying, y = minutes, fill = saying)) + geom_boxplot(show.legend = FALSE, outlier.colour = NA) + geom_jitter(shape = 21, alpha = 0.6, width = 0.3, height = 0.0, show.legend = FALSE) + scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) + scale_fill_brewer(palette = \u0026quot;Accent\u0026quot;) + labs(x = \u0026quot;\u0026quot;)  Figure 2: Boxplots showing the distribution of times (minutes) survey participants gave for the three most common time frame idioms in South Africa.\n As we may see in Figure 2, \u0026lsquo;just now\u0026rsquo; and \u0026lsquo;now now\u0026rsquo; appear to have similar distributions, whereas \u0026lsquo;now\u0026rsquo; is markedly different. Participants answered the maximum score of 300 minutes for all idioms, but for \u0026lsquo;just now\u0026rsquo; and \u0026lsquo;now now\u0026rsquo; this was infrequent enough that the large scores are considered to be outliers. For \u0026lsquo;now\u0026rsquo;, enough participants answered with longer times that the distribution appears much larger than for the other two idioms. Even though these distributions appear different, let\u0026rsquo;s run the stats on them to make sure. Because we want to compare distributions of scores for three different categories we will be using an ANOVA if the data meet a few basic assumptions.\nJust as a quick recap, these assumptions are: homoscedasticity (homogeneity of variance), normality of distribution, random \u0026amp; independently sampled. It is also a good idea to have at least a sample size of ten for each category. Seeing as how this was an Internet survey, we were not able to ensure that the participants are a random representation of the South African populace, nor can we be certain that they submitted their answers independent of one another. I am however just going to go ahead and assume that they did. As for the assumptions of homoscedasticity and the normality of the distributions, we can directly test these. In R, the normality of a distribution is tested with shapiro.test(). Any non-significant result (p \u0026gt; 0.05) means that the data are normally distributed. To test for homoscedasticity we will use bartlett.test(). A non-significant result (p \u0026gt; 0.05) from this test indicates that the variances between the categories are equivalent.\n# test for normality survey_long %\u0026gt;% group_by(saying) %\u0026gt;% summarise(noramlity = as.numeric(shapiro.test(minutes)[1]))  ## # A tibble: 3 × 2 ## saying noramlity ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 just now 0.565 ## 2 now 0.713 ## 3 now now 0.671  # Test for homoscedasticity survey_long %\u0026gt;% bartlett.test(minutes ~ saying, data = .)  ## ## Bartlett test of homogeneity of variances ## ## data: minutes by saying ## Bartlett's K-squared = 5.227, df = 2, p-value = 0.07328  Surprisingly these data meet all of our assumptions so we may perform a simple one-way ANOVA on them to detect any significant differences.\nglance(aov(minutes ~ saying, data = survey_long))  ## # A tibble: 1 × 6 ## logLik AIC BIC deviance nobs r.squared ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -670. 1349. 1360. 1143843. 111 0.0615  Less surprisingly, there is a significant difference in the times given for these three idioms. But let\u0026rsquo;s dive just a bit deeper with a post-hoc Tukey (not Turkey) test to see which categories specifically are different from which.\nTukeyHSD(aov(minutes ~ saying, data = survey_long))  ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = minutes ~ saying, data = survey_long) ## ## $saying ## diff lwr upr p adj ## now-just now 60.81081 3.949653 117.67197 0.0330869 ## now now-just now 14.18919 -42.671969 71.05035 0.8241544 ## now now-now -46.62162 -103.482779 10.23954 0.1302148  Looking at the names given in the \u0026lsquo;saying\u0026rsquo; column, and the values given in the \u0026lsquo;p adj\u0026rsquo; column we may see which individual idioms are different from which. Unsurprisingly, judging from Figure 2, \u0026lsquo;now now\u0026rsquo; and \u0026lsquo;just now\u0026rsquo; are not different. Interesting though is that \u0026lsquo;now\u0026rsquo; is significantly different from \u0026lsquo;just now\u0026rsquo;, but not \u0026lsquo;now now\u0026rsquo;. The initial results made it look as though \u0026lsquo;now\u0026rsquo; would have been significantly different from both.\nProvince time frames Preferably, many more people would have taken the survey so that we could draw more conclusive results. I\u0026rsquo;m rather certain that should more people take the survey the distributions of the scores for the three idioms would even out more until there were no significant differences between any of them. Enough participants have taken the test however that we may compare the results for a few different provinces.\n# Remove provinces with fewer than nine entries # Ten would be preferable, but at nine we allow the inclusion of KZN survey_province \u0026lt;- survey_long %\u0026gt;% group_by(province) %\u0026gt;% filter(n() \u0026gt;= 9) %\u0026gt;% ungroup()  And now with the provinces that have only a few answers filtered out, let\u0026rsquo;s see what the data look like as boxplots.\nggplot(data = survey_province, aes(x = saying, y = minutes, fill = province)) + geom_boxplot(outlier.colour = NA) + geom_point(shape = 21, position = position_jitterdodge()) + scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) + scale_fill_brewer(palette = \u0026quot;Set2\u0026quot;) + labs(x = \u0026quot;\u0026quot;)  Figure 3: Boxplots showing the distribution of times by province.\n Right away for me it appears that the time frames for the Eastern Cape are shorter than for the other three provinces. KZN appears to be the longest, with Gauteng and the Western Cape seeming similar. Because we have multiple independent variables (saying and province) we want a two-way ANOVA. But before we do that, let\u0026rsquo;s again check for normality and homoscedasticity.\n# test for normality survey_province %\u0026gt;% group_by(saying, province) %\u0026gt;% summarise(normality = as.numeric(shapiro.test(minutes)[1]))  ## # A tibble: 12 × 3 ## # Groups: saying [3] ## saying province normality ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 just now Eastern Cape 0.640 ## 2 just now Gauteng 0.867 ## 3 just now KZN 0.865 ## 4 just now Western Cape 0.537 ## 5 now Eastern Cape 0.766 ## 6 now Gauteng 0.721 ## 7 now KZN 0.75 ## 8 now Western Cape 0.729 ## 9 now now Eastern Cape 0.684 ## 10 now now Gauteng 0.689 ## 11 now now KZN 0.813 ## 12 now now Western Cape 0.718  # Test for homoscedasticity survey_province %\u0026gt;% bartlett.test(minutes ~ interaction(saying, province), data = .)  ## ## Bartlett test of homogeneity of variances ## ## data: minutes by interaction(saying, province) ## Bartlett's K-squared = 69.523, df = 11, p-value = 1.505e-10  The different groups of time are all normally distributed, but the variances differ. Regardless, we are going to stick to a normal two-way ANOVA as there are no good alternatives to this for non-parametric data and transforming these data is a bother.\nglance(aov(minutes ~ saying + province, data = survey_province))  ## # A tibble: 1 × 6 ## logLik AIC BIC deviance nobs r.squared ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -623. 1261. 1279. 881685. 105 0.204  Significant differences are to be had here. So let\u0026rsquo;s break it down and see which groups specifically differ.\nTukeyHSD(aov(minutes ~ saying + province, data = survey_province))  ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = minutes ~ saying + province, data = survey_province) ## ## $saying ## diff lwr upr p adj ## now-just now 70.14286 16.46427 123.821448 0.0068600 ## now now-just now 15.42857 -38.25002 69.107162 0.7734013 ## now now-now -54.71429 -108.39288 -1.035695 0.0447008 ## ## $province ## diff lwr upr p adj ## Gauteng-Eastern Cape 63.452381 -15.761193 142.66595 0.1624466 ## KZN-Eastern Cape 139.722222 39.043527 240.40092 0.0025348 ## Western Cape-Eastern Cape 73.289474 6.613379 139.96557 0.0252743 ## KZN-Gauteng 76.269841 -21.982505 174.52219 0.1845631 ## Western Cape-Gauteng 9.837093 -53.115472 72.78966 0.9768852 ## Western Cape-KZN -66.432749 -154.888584 22.02309 0.2092492  We may see that by removing some of the answers from the less represented provinces there is now a significant difference between the scores for \u0026lsquo;now\u0026rsquo; and \u0026lsquo;now now\u0026rsquo; as well as \u0026lsquo;just now\u0026rsquo;. Additionally we may see that the scores for Eastern Cape differ significantly from KZN and the Western Cape. One could also look at the interactions between the two independent variables but I\u0026rsquo;m not quite interested in that level of depth here.\nCity time frames With the breakdown for the province time frames out of the way, we are going to wrap up this analysis by looking at the difference between cities.\n# Remove provinces with fewer than five entries # Ten is preferable, but at five we may include Mthatha survey_city \u0026lt;- survey_long %\u0026gt;% group_by(saying, city) %\u0026gt;% filter(n() \u0026gt;= 5) %\u0026gt;% ungroup()  ggplot(data = survey_city, aes(x = saying, y = minutes, fill = city)) + geom_boxplot(outlier.colour = NA) + geom_point(shape = 21, position = position_jitterdodge()) + scale_y_continuous(breaks = c(5, 15, 30, 60, 120, 300)) + scale_fill_brewer(palette = \u0026quot;Set3\u0026quot;) + labs(x = \u0026quot;\u0026quot;)  Figure 4: Boxplots showing the distribution of times by city.\n Again with the assumptions\u0026hellip;\n# test for normality survey_city %\u0026gt;% group_by(saying, city) %\u0026gt;% summarise(normality = as.numeric(shapiro.test(minutes)[1]))  ## # A tibble: 9 × 3 ## # Groups: saying [3] ## saying city normality ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 just now Cape Town 0.577 ## 2 just now Johannesburg 0.912 ## 3 just now Mthatha 0.552 ## 4 now Cape Town 0.739 ## 5 now Johannesburg 0.757 ## 6 now Mthatha 0.754 ## 7 now now Cape Town 0.739 ## 8 now now Johannesburg 0.741 ## 9 now now Mthatha 0.552  # Test for homoscedasticity survey_city %\u0026gt;% bartlett.test(minutes ~ interaction(saying, city), data = .)  ## ## Bartlett test of homogeneity of variances ## ## data: minutes by interaction(saying, city) ## Bartlett's K-squared = 55.177, df = 8, p-value = 4.078e-09  \u0026hellip; and again we see that while normally distributed, the variance of our sample sets differ significantly from one another. Regardless, we\u0026rsquo;ll stick to the two-way ANOVA.\nglance(aov(minutes ~ saying + city, data = survey_city))  ## # A tibble: 1 × 6 ## logLik AIC BIC deviance nobs r.squared ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -481. 973. 987. 674933. 81 0.205  As one likely would have deduced from Figure 4, the scores are significantly different from one another.\nTukeyHSD(aov(minutes ~ saying + city, data = survey_city))  ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = minutes ~ saying + city, data = survey_city) ## ## $saying ## diff lwr upr p adj ## now-just now 77.96296 16.65151 139.2744119 0.0090081 ## now now-just now 16.85185 -44.45960 78.1633008 0.7889537 ## now now-now -61.11111 -122.42256 0.2003379 0.0509401 ## ## $city ## diff lwr upr p adj ## Johannesburg-Cape Town -10.72917 -72.99124 51.532904 0.9108240 ## Mthatha-Cape Town -84.89583 -151.53238 -18.259285 0.0088590 ## Mthatha-Johannesburg -74.16667 -152.92265 4.589316 0.0691761  From the post-hoc test we may see that \u0026lsquo;now\u0026rsquo; differs significantly from \u0026lsquo;just now in these three cities, and if we\u0026rsquo;re feeling generous we may say that \u0026lsquo;now\u0026rsquo; also differs significantly from \u0026lsquo;now now\u0026rsquo;. But like with the rest of the country, \u0026lsquo;now now\u0026rsquo; and \u0026lsquo;just now\u0026rsquo; are not significantly different time frames. Looking at the city breakdown we see that the only significant difference is between Mthatha and Cape Town. Johannesburg and Cape Town do not differ.\nConclusion The general conclusion I\u0026rsquo;ve drawn from the analysis of the South Africa time survey results is that the understanding people in the Eastern Cape have of the time frames for these idioms is significantly faster than in the other major provinces in South Africa. The numbers don\u0026rsquo;t lie!\nSpeaking to people about this survey (before the results were out), the general consensus was that Johannesburg people would give significantly faster scores than people in other parts of the country, particularly Cape Town. That does not however appear to be the case. Also surprising from these results was that \u0026lsquo;now\u0026rsquo; tended to be considered to be a significantly longer time frame than \u0026lsquo;just now\u0026rsquo; and \u0026lsquo;now now\u0026rsquo;. Most people I\u0026rsquo;ve talked to about these idioms agree that \u0026lsquo;now\u0026rsquo; is meant to be the fastest\u0026hellip; so I\u0026rsquo;m not sure how that worked out. Unsurprising to me, but perhaps to some South Africans, was that there is no difference between \u0026lsquo;just now\u0026rsquo; and \u0026lsquo;now now\u0026rsquo;. I was not surprised by this because talking with people around the country I very infrequently heard people, even while in the same room, agree on the time frames for these idioms.\nThere are of course a host of issues with this study. One thing I\u0026rsquo;m wondering about the Eastern Cape scores being significantly faster than the other provinces is if perhaps people in the Eastern Cape have a wider range of idioms for giving someone a time frame? Meaning, perhaps \u0026lsquo;now\u0026rsquo;, \u0026lsquo;just now\u0026rsquo;, and \u0026lsquo;now now\u0026rsquo; are all much faster than other provinces because there is some other popular idiom people use there that denotes a longer time frame. It\u0026rsquo;s also possible that people in other provinces misunderstood the survey. Though I did make it very clear that the answers were in minutes and not seconds, specifically to attempt to prevent people from making that mistake. Lastly, this analysis suffers from a regrettably small sample size. I would have preferred at least 100 responses, rather than 40. It is still wonderful to be able to get some numbers in the game and get a glimpse of the fact that there is little agreement about these time frames.\nHopefully posting these results will snare a few more people into taking the survey and in another couple of months I can make a follow up post with the additional feedback.\n","date":1531785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531785600,"objectID":"ac631174e7e52529d4812120ed1aeb65","permalink":"https://theoceancode.netlify.app/post/sa_time_survey/","publishdate":"2018-07-17T00:00:00Z","relpermalink":"/post/sa_time_survey/","section":"post","summary":"Objective In South Africa there are a range of idioms for different time frames in which someone may (or may not) do something. The most common of these are: \u0026lsquo;now\u0026rsquo;, \u0026lsquo;just now\u0026rsquo;, and \u0026lsquo;now now\u0026rsquo;. If one were to Google these sayings one would find that there is general agreements on how long these time frames are, but that agreement is not absolute.\nThis got me to wondering just how much disagreement there may be around the country.","tags":["survey","statistics"],"title":"South Africa time survey","type":"post"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530136800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://theoceancode.netlify.app/privacy/","publishdate":"2018-06-28T00:00:00+02:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":["R"],"content":" Preface This week I have expanded the coastR package with the inclusion of a function that calculates the angle of the heading for alongshore or shore-normal transects. The rest of this blog post is the vignette that I\u0026rsquo;ve written detailing the set of this function. Next week I\u0026rsquo;ll likely be taking a break from coastR development to finally create a package for the SACTN dataset. That is a project that has been in the works for a loooong time and it will be good to finally see a development release available to the public.\nOverview There are a number of reasons why one would want to calculate transects along or away from a coastline. Examples include: finding the fetch across an embayment, finding the coordinates of a point 200 km from the coast, finding the appropriate series of SST pixels along/away from the coast, (or if one is feeling particular feisty) the creation of shape files for a given area away from the coast. The function that we will be introducing here does none of these things. What the transects() function does do is calculate the angle of the heading along or away from the coast against true North, which is then the basis for all of the other fancy things one may want to do. Baby steps people. Baby steps.\n# devtools::install_github(\u0026quot;robwschlegel/coastR\u0026quot;) # Install coastR library(coastR) library(dplyr) library(ggplot2) library(gridExtra) library(geosphere)  Sample locations For this vignette we will re-use the same coastlines as those created for the sequential sites vignette. The ordering of the sites remains jumbled up to demonstrate that transects() does not require orderly data. Should one want to order ones site list before calculating transect headings it is possible to do so with seq_sites(). This is of course a recommended step in any workflow.\n# Cape Point, South Africa cape_point \u0026lt;- SACTN_site_list %\u0026gt;% slice(c(31, 22, 26, 17, 19, 21, 30)) %\u0026gt;% mutate(order = 1:n()) # South Africa south_africa \u0026lt;- SACTN_site_list %\u0026gt;% slice(c(1,34, 10, 20, 50, 130, 90)) %\u0026gt;% mutate(order = 1:n()) # Baja Peninsula, Mexico baja_pen \u0026lt;- data.frame( order = 1:7, lon = c(-116.4435, -114.6800, -109.6574, -111.9503, -112.2537, -113.7918, -114.1881), lat = c(30.9639, 30.7431, 22.9685, 26.9003, 25.0391, 29.4619, 28.0929) ) # Bohai Sea, China bohai_sea \u0026lt;- data.frame( order = 1:7, lon = c(122.0963, 121.2723, 121.0687, 121.8742, 120.2962, 117.6650, 122.6380), lat = c(39.0807, 39.0086, 37.7842, 40.7793, 40.0691, 38.4572, 37.4494) )  Transects With our site lists created we now want to see what the correct headings for alongshore and shore-normal transects are for our sites. We will also demonstrate what happens when we increase the spread used in the calculation and also how the inclusion of island masks affects the angle of the headings.\n# Cape Point, South Africa cape_point_along \u0026lt;- transects(cape_point, alongshore = T) cape_point_away \u0026lt;- transects(cape_point) # South Africa south_africa_along \u0026lt;- transects(south_africa, alongshore = T) south_africa_away \u0026lt;- transects(south_africa) # NB: Note here the use of the `spread` argument south_africa_along_wide \u0026lt;- transects(south_africa, alongshore = T, spread = 30) south_africa_away_wide \u0026lt;- transects(south_africa, spread = 30) # Baja Peninsula, Mexico baja_pen_along \u0026lt;- transects(baja_pen, alongshore = T) baja_pen_away \u0026lt;- transects(baja_pen) # NB: Note here the use of the `coast` argument baja_pen_island \u0026lt;- transects(baja_pen, coast = FALSE) # Bohai sea, China bohai_sea_along \u0026lt;- transects(bohai_sea, alongshore = T) bohai_sea_away \u0026lt;- transects(bohai_sea)  Visualise Now that the correct headings have been calculated for our alongshore and shore-normal transects let\u0026rsquo;s visualise them with ggplot. First we will create a function that does this in order to keep the length of this vignette down.\n# Create base map world_map \u0026lt;- ggplot() + borders(fill = \u0026quot;grey40\u0026quot;, colour = \u0026quot;black\u0026quot;) # Create titles titles \u0026lt;- c(\u0026quot;Alongshore\u0026quot;, \u0026quot;Shore-normal\u0026quot;, \u0026quot;Islands\u0026quot;) # Plotting function plot_sites \u0026lt;- function(site_list, buffer, title_choice, dist){ # Find the point 200 km from the site manually to pass to ggplot heading2 \u0026lt;- data.frame(geosphere::destPoint(p = select(site_list, lon, lat), b = site_list$heading, d = dist)) # Add the new coordinates tot he site list site_list \u0026lt;- site_list %\u0026gt;% mutate(lon_dest = heading2$lon, lat_dest = heading2$lat) # Visualise world_map + geom_segment(data = site_list, colour = \u0026quot;red4\u0026quot;, aes(x = lon, y = lat, xend = lon_dest, yend = lat_dest)) + geom_point(data = site_list, size = 3, colour = \u0026quot;black\u0026quot;, aes(x = lon, y = lat)) + geom_point(data = site_list, size = 3, colour = \u0026quot;red\u0026quot;, aes(x = lon_dest, y = lat_dest)) + coord_cartesian(xlim = c(min(site_list$lon - buffer), max(site_list$lon + buffer)), ylim = c(min(site_list$lat - buffer), max(site_list$lat + buffer))) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, colour = \u0026quot;Site\\norder\u0026quot;) + ggtitle(titles[title_choice]) }  Cape Point, South Africa The transect() function is designed to work well at small scales by default. We may see this here with the effortlessness of plotting transects around a peninsula and then across an embayment in one go.\ncape_point_along_map \u0026lt;- plot_sites(cape_point_along, 0.5, 1, 10000) cape_point_away_map \u0026lt;- plot_sites(cape_point_away, 0.5, 2, 10000) grid.arrange(cape_point_along_map, cape_point_away_map, nrow = 1)  (\\#fig:cape_point_trans)Alongshore and shore-normal transects around Cape Point and False Bay, South Africa.\n South Africa The intentions one may have for calculating shore-normal transects will differ depending on ones research question. If one is interested in visualising the convolutions of a coastline at a sub-meso-scale then the default spread of the transect() function is probably the way to go, as shown above. If however one is interested in seeing the shore-normal transects broadly for the coastline of an entire country it is likely that one will want to greatly expand the spread of coastline used to calculate said transects. In the figure below we may see how changing the spread of the coastline considered for the transects changes the results. The top row shows the transects resulting from the narrow default spread, while the bottom row shows the results of using a much wider spread for the calculation. Note particularly how the transect changes at St. Helena Bay and Gansbaai (second and fourth sites from the top left), as well as a general smoothing of all of the other transects. This is due to the sensitivity of the function. The St. Helena Bay and Gansbaai sites lay within embayments; therefore, the shore-normal transects that would come out directly from these sites will not follow the general contour of the coastline of South Africa. Should we be interested in the \u0026ldquo;bigger picture\u0026rdquo; we must increase the spread argument in transects(). This may require some trial and error for particularly difficult coastlines before a satisfactory result is produced, but it is certainly still faster than running the calculations by hand. Should small scale accuracy along part of the coast, and broader accuracy elsewhere be required, one must simply divide the site list into the different sections and run transects() on each subset with the desired spread.\nsouth_africa_along_map \u0026lt;- plot_sites(south_africa_along, 1, 1, 100000) south_africa_away_map \u0026lt;- plot_sites(south_africa_away, 1, 2, 100000) south_africa_along_wide_map \u0026lt;- plot_sites(south_africa_along_wide, 1, 1, 100000) south_africa_away_wide_map \u0026lt;- plot_sites(south_africa_away_wide, 1, 2, 100000) grid.arrange(south_africa_along_map, south_africa_away_map, south_africa_along_wide_map, south_africa_away_wide_map, nrow = 2)  (\\#fig:south_africa_trans)Alongshore and shore-normal transects around all of South Africa.\n Baja Peninsula, Mexico In the following figure we see how the inclusion of islands affects the results of our transects. The first site up from the tip of the peninsula on the left-hand side is on an island. Note the minor adjustment to the transect when the island mask is used for the calculation. In this case it\u0026rsquo;s not large, but in other instances it may be massive. By default island masks are removed and it is our advice that they not be used unless extreme caution is observed.\nbaja_pen_along_map \u0026lt;- plot_sites(baja_pen_along, 1, 1, 100000) baja_pen_away_map \u0026lt;- plot_sites(baja_pen_away, 1, 2, 100000) baja_pen_island_map \u0026lt;- plot_sites(baja_pen_island, 1, 3, 100000) grid.arrange(baja_pen_along_map, baja_pen_away_map, baja_pen_island_map, nrow = 1)  (\\#fig:baja_pen_trans)Alongshore and shore-normal transects around the Baja Peninsula.\n Bohai Sea, China This figure serves as a good visualisation for just how localised the coastline is that is used to calculate the shore-normal transects. Note how the alongshore transects look a little dodgy, but when shown as shore-normal transects everything works out. This is something to consider if one is interested in calculating alongshore transects rather than shore-normal transects. For alongshore transects that show more fidelity for coastal direction it is advisable to increase the spread argument.\nbohai_sea_along_map \u0026lt;- plot_sites(bohai_sea_along, 1, 1, 70000) bohai_sea_away_map \u0026lt;- plot_sites(bohai_sea_away, 1, 2, 70000) grid.arrange(bohai_sea_along_map, bohai_sea_away_map, nrow = 1)  (\\#fig:bohai_sea_trans)Alongshore and shore-normal transects within the Bohai Sea.\n Conclusion As we may see in the previous example figures, the transect() function tends to work better by default at smaller scales. This was an intentional decision as it is much more accurate when scaling the function up for larger coastal features than when scaling it down for smaller ones.\nThe calculation of the heading for alongshore and shore-normal transects is rarely the end goal itself. One then generally wants to find specific points from the coastline along the transects that have been determined. This is done in the code above within the plot_sites() function created within this vignette, but the process is not detailed specifically. How to do more elaborate things with transects will be explained with the following functions to be added to coastR. This will include how to draw coastal polygons based on distance and bathymetry.\n","date":1512691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512691200,"objectID":"d919acb2f5fa49d3a53d868999c31c0c","permalink":"https://theoceancode.netlify.app/post/transects/","publishdate":"2017-12-08T00:00:00Z","relpermalink":"/post/transects/","section":"post","summary":"Preface This week I have expanded the coastR package with the inclusion of a function that calculates the angle of the heading for alongshore or shore-normal transects. The rest of this blog post is the vignette that I\u0026rsquo;ve written detailing the set of this function. Next week I\u0026rsquo;ll likely be taking a break from coastR development to finally create a package for the SACTN dataset. That is a project that has been in the works for a loooong time and it will be good to finally see a development release available to the public.","tags":["coastal"],"title":"Transects","type":"post"},{"authors":["RW Schlegel","ECJ Oliver","S Perkins-Kirkpatrick","A Kruger","AJ Smit"],"categories":null,"content":"","date":1508191200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508191200,"objectID":"0b102118b91144a4196a666ef4f2aefc","permalink":"https://theoceancode.netlify.app/publication/predominant/","publishdate":"2017-10-17T00:00:00+02:00","relpermalink":"/publication/predominant/","section":"publication","summary":"As the mean temperatures of the worlds oceans increase, it is predicted that marine heatwaves (MHWs) will occur more frequently and with increased severity. However, it has been shown that variables other than increases in sea water temperature have been responsible for MHWs. To better understand these mechanisms driving MHWs we have utilized atmospheric (ERA-Interim) and oceanic (OISST, AVISO) data to examine the patterns around southern Africa during coastal (","tags":["R","coastal","atmosphere","ocean"],"title":"Predominant Atmospheric and Oceanic Patterns during Coastal Marine Heatwaves","type":"publication"},{"authors":null,"categories":["R"],"content":" Objective Whilst cruising about on Imgur I found a post about science stuff. Not uncommon, which is nice. These sorts of grab-bag posts about nothing in particular often include some mention of climate science, almost exclusively some sort of clever visualisation of a warming planet. That seems to be what people are most interested in. I\u0026rsquo;m not complaining though, it keeps me employed. The aforementioned post caught my attention more than usual because it included a GIF, and not just a static picture of some sort of blue thing that is becoming alarmingly red (that was not meant to be a political metaphor). I\u0026rsquo;m referring to the now famous GIF by climate scientist Ed Hawkins (@ed_hawkins) whose blog may be found here, and the specific post in question here. A quick bit of research on this animation revealed that it has likely been viewed by millions of people, was featured in the opening ceremony of the Rio Olympics, and was created in MATLAB. Those three key points made me decide to do a post on how to re-create this exact figure in R via a bit of reverse engineering. The original GIF in question is below.\nFigure 1: The ever broadening spiral of global temperatures created by Ed Hawkins.\nData Figure 1 above uses the global mean temperature anomalies taken from HadCRUT4. These data have an impressive range of collection, going back to 1850. Very few datasets match this length of collection, and I\u0026rsquo;m not going to attempt to do so here. What I am going to do is use the data that I work with on a daily basis. These are the SACTN data that may also be downloaded here via a GUI. As a coastal oceanographer I am mostly interested in changing climates in the near shore. While not publish explicitly, a paper about the appropriate methodology one should use does exist, and this methodology has been applied to all of the time series in the SACTN dataset accordingly. It is therefore known what the rates of decadal change along the coast of South Africa are, and we may rely on this in order to cherry pick the more dramatic time series in order to make prettier visuals.\nCode With our end goal established (Figure 1), and our dataset chosen (SACTN), we may now get busy with the actual code necessary. As one may have inferred from the title of this post, Figure 1 is what we call a \u0026ldquo;polar plot\u0026rdquo;. This may appear complex to some, but is actually a very simple visualisation, as we shall see below. But first we need to prep our data. For consistency in the creation of the anomaly values below I will use 1981 \u0026ndash; 2010 for the climatology of each time series.\n## Libraries library(tidyverse) library(viridis) library(lubridate) library(zoo) library(gridExtra) library(animation) ## Data # SACTN load(\u0026quot;../../static/data/SACTN_monthly_v4.2.RData\u0026quot;) ## Subset # Subseting function ts.sub \u0026lt;- function(site){ ts \u0026lt;- SACTN_monthly_v4.2 %\u0026gt;% filter(index == site) %\u0026gt;% mutate(year = year(as.yearmon(date)), month = month(as.yearmon(date), label = T), clim = mean(temp[year %in% seq(1981,2010)], na.rm = T), anom = temp-clim, index = as.character(index)) %\u0026gt;% rename(site = index) %\u0026gt;% select(site, year, month, anom) return(ts) } # Warming site PN \u0026lt;- ts.sub(\u0026quot;Port Nolloth/SAWS\u0026quot;) # Cooling site SP \u0026lt;- ts.sub(\u0026quot;Sea Point/SAWS\u0026quot;) # Neutral site KB \u0026lt;- ts.sub(\u0026quot;Kent Bay/KZNSB\u0026quot;)  With our data prepared we may now create the series of functions that will make a spiralling polar plot of temperatures for any time series we feed into it. I prefer to use the animation package to create animations in R. This requires that one also installs image magick beforehand. This is a free software that is available for all major operating systems. There are a few ways to create animations in R, but I won\u0026rsquo;t go into that now. The method I employ to create the animations below may seem odd at first, but as far as I have seen it is the most efficient way to do so. The philosophy employed here is that we want to have one final function that simply counts forward one step at a time, creating each frame of the GIF. This function calls on other functions that are calculating the necessary stats and creating the visuals from them in the background. By creating animations in this way, our up front prep and calculation time is almost non-existent. It does mean that the animations take longer to compile, but they are also much more dynamic and we may feed any number of different dataframes into them to get different outputs. I have found over the years that the more automated ones code can be the better.\n## Function that creates a polar plot polar.plot \u0026lt;- function(df, i){ # Add bridges for polar coordinates years \u0026lt;- unique(df$year)[1:i] df2 \u0026lt;- filter(df, year %in% years) bridges \u0026lt;- df2[df2$month == 'Jan',] bridges$year \u0026lt;- bridges$year - 1 if(nrow(bridges) == 0){ bridges \u0026lt;- data.frame(site = df2$site[1], year = min(df2$year), month = NA, anom = NA) } else { bridges$month \u0026lt;- NA } blanks \u0026lt;- data.frame(site = df2$site[1], expand.grid(year = min(df2$year)-1, month = month.abb), anom = NA) # Polar plot pp \u0026lt;- ggplot(data = rbind(blanks, df2, bridges), aes(x = month, y = anom, group = year)) + # Circular black background geom_rect(colour = \u0026quot;black\u0026quot;, fill = \u0026quot;black\u0026quot;, aes(xmin = \u0026quot;Jan\u0026quot;, xmax = NA, ymin = min(df$anom, na.rm = T), ymax = max(df$anom, na.rm = T))) + # ymin = min(df$anom, na.rm = T), ymax = 3)) + # Anomaly threshold labels geom_hline(aes(yintercept = 1.0), colour = \u0026quot;red\u0026quot;) + geom_label(aes(x = \u0026quot;Jan\u0026quot;, y = 1.0, label = \u0026quot;1.0°C\u0026quot;), colour = \u0026quot;red\u0026quot;, fill = \u0026quot;black\u0026quot;, size = 3) + geom_hline(aes(yintercept = 2.0), colour = \u0026quot;red\u0026quot;) + geom_label(aes(x = \u0026quot;Jan\u0026quot;, y = 2.0, label = \u0026quot;2.0°C\u0026quot;), colour = \u0026quot;red\u0026quot;, fill = \u0026quot;black\u0026quot;, size = 3) + geom_hline(aes(yintercept = 3.0), colour = \u0026quot;red\u0026quot;) + geom_label(aes(x = \u0026quot;Jan\u0026quot;, y = 3.0, label = \u0026quot;3.0°C\u0026quot;), colour = \u0026quot;red\u0026quot;, fill = \u0026quot;black\u0026quot;, size = 3) + geom_hline(aes(yintercept = 4.0), colour = \u0026quot;red\u0026quot;) + geom_label(aes(x = \u0026quot;Jan\u0026quot;, y = 4.0, label = \u0026quot;4.0°C\u0026quot;), colour = \u0026quot;red\u0026quot;, fill = \u0026quot;black\u0026quot;, size = 3) + # Temperature spiral geom_path(aes(colour = anom), show.legend = F) + # Scale corrections scale_colour_viridis(limits = c(min(df$anom, na.rm = T), max(df$anom, na.rm = T))) + scale_x_discrete(expand = c(0,0), breaks = month.abb) + scale_y_continuous(expand = c(0,0), limits = c(min(df$anom, na.rm = T), max(df$anom, na.rm = T))) + # Year label geom_text(aes(x = \u0026quot;Jan\u0026quot;, y = min(df$anom, na.rm = T), label = max(df2$year, na.rm = T)), colour = \u0026quot;ivory\u0026quot;, size = 8) + # Additional tweaks ggtitle(paste0(df$site[1],\u0026quot; temperature change (\u0026quot;,min(df$year),\u0026quot;-\u0026quot;,max(df$year),\u0026quot;)\u0026quot;)) + coord_polar() + theme(panel.background = element_rect(fill = \u0026quot;grey20\u0026quot;), plot.background = element_rect(fill = \u0026quot;grey20\u0026quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), # axis.text.x = element_text(colour = \u0026quot;ivory\u0026quot;), axis.text.x = element_text(colour = \u0026quot;ivory\u0026quot;, angle = (360/(2*pi)*rev(seq(pi/12, 2*pi-pi/12, len = 12)))+15, size = 12), axis.text.y = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), axis.ticks.length = unit(0, \u0026quot;cm\u0026quot;), plot.title = element_text(hjust = 0.5, colour = \u0026quot;ivory\u0026quot;, size = 15)) print(pp) } ## Create animation of polar plots animate.polar.plot \u0026lt;- function(df) { lapply(seq(1,length(unique(df$year))), function(i) { polar.plot(df = df, i = i) }) }  With the above two functions created, we may now call them nested within one another via the saveGIF function below.\n# By default 'saveGIF()' outputs to the same folder # the script where the code is being run from is located. # For that reason one may want to manually change the # working directory beforehand. # setwd(\u0026quot;somewhere else\u0026quot;) system.time(saveGIF(animate.polar.plot(df = PN), interval = 0.4, ani.width = 457, movie.name = \u0026quot;polar_plot_PN.gif\u0026quot;)) ## 262 seconds system.time(saveGIF(animate.polar.plot(df = SP), interval = 0.4, ani.width = 457, movie.name = \u0026quot;polar_plot_SP.gif\u0026quot;)) ## 221 seconds system.time(saveGIF(animate.polar.plot(df = KB), interval = 0.4, ani.width = 457, movie.name = \u0026quot;polar_plot_KB.gif\u0026quot;)) ## 183 seconds # setwd(\u0026quot;back to where you were\u0026quot;)  Summary As one may see in the following GIFs, local extremes often outpace global averages. This should not be terribly surprising. In order to better illustrate this I have expanded the anomaly labels along the y-axes more so than seen in Figure 1. The increasing patterns are not as clear in these following GIFs as in the original that they are based on. This is because the original is based on a global average, which provides for a much smoother trend. I hope people enjoy these and feel free to plop your own temperature time series into the code to create your own polar plot figures!\nFigure 2: The polar plot for Port Nolloth, where temperatures have been increasing.\nFigure 3: The polar plot for Sea Point, where temperatures have been decreasing.\nFigure 4: The polar plot for Kent Bay, where temperatures have been holding level.\n","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"5eec39593b9acac61c0e3d4e1c2f2b7b","permalink":"https://theoceancode.netlify.app/post/polar_plot_clims/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/post/polar_plot_clims/","section":"post","summary":"Objective Whilst cruising about on Imgur I found a post about science stuff. Not uncommon, which is nice. These sorts of grab-bag posts about nothing in particular often include some mention of climate science, almost exclusively some sort of clever visualisation of a warming planet. That seems to be what people are most interested in. I\u0026rsquo;m not complaining though, it keeps me employed. The aforementioned post caught my attention more than usual because it included a GIF, and not just a static picture of some sort of blue thing that is becoming alarmingly red (that was not meant to be a political metaphor).","tags":["visuals","climatology"],"title":"Polar plot climatologies","type":"post"},{"authors":null,"categories":["R"],"content":" Preface The rest of the blog post after this preface section is a copy of the vignette I\u0026rsquo;ve written for the first function in the new package I am developing: coastR. This package aims to provide functions that are useful for coastal oceanography but that do not yet exist in the R language. It is not my intention to provide algorithms for physical oceanography as these may already be found elsewhere. This post covers how one may determine the correct sequence of sites along a convoluted coastline.\nNow that I\u0026rsquo;ve handed in my PhD I am a little less pressed as far as deadlines go and I would like to return to my habit of creating a new blog post every Friday. I\u0026rsquo;ve written quite a bit of code over the last three years and much of it needs to find it\u0026rsquo;s way into the coastR. Next week I am planning on uploading a function for calculating shore normal transects. Until then, please enjoy the spectacle of sequential ordering. Woo.\nOverview The human mind prefers to see patterns in whatever it encounters. To this end we try to provide ourselves with data that are stored in a way that will appeal to that disposition. For a time series this usually means that the data are saved sequentially through time. For spatial data this means that the data are saved in some sort of sequential state, too. But what might that be? For 2D, 3D, or 4D data this can get tricky rather quickly and one tends to default to netcdf files. But with 1D data we are able to decide how we want the data to be structured as they will fit within a simple dataframe. But how can spatial data be 1D? Nothing in nature truly is, but I use 1D here as an expedient way of describing data that are constrained to some physical (usually continuous) barrier. Specifically for use with seq_sites() we will be looking at sites along a coastline.\nIf one has meta-data for a number of sampling sites they should be saved in the order they may be found along the coastline. Some would perhaps prefer to order sites alphabetically, I am not one of them for a number of reasons. Not least of which being that this is too simple a way of organising. One could also choose to organise ones coastal sites in numerical order of either longitude or latitude. This quickly becomes problematic for most stretches of coastline as natural formations such as peninsulas and embayments will prevent the correct ordering of sites based on only latitude or longitude. It is therefore necessary to query the longitude and latitude of each site in a list against a land mask in order to determine the correct order along the coastline. This is what will be demonstrated below.\n# devtools::install_github(\u0026quot;robwschlegel/coastR\u0026quot;) # Install coastR library(coastR) library(tidyverse) library(gridExtra)  Sample locations For the purpose of this vignette we will manually create a few dataframes for different coastlines around the world of varying degrees of complexity and length. The first two dataframes are taken from the SACTN site list included in the coastR package. The rest have their lon/lat values grabbed from Google maps. Note that the order of the sites is intentionally entered incorrectly so as to be able to demonstrate the efficacy of seq_sites().\n# Cape Point, South Africa cape_point \u0026lt;- SACTN_site_list %\u0026gt;% slice(c(31, 22, 26, 17, 19, 21, 30)) %\u0026gt;% mutate(order = 1:n()) # South Africa south_africa \u0026lt;- SACTN_site_list %\u0026gt;% slice(c(1,34, 10, 20, 50, 130, 90)) %\u0026gt;% mutate(order = 1:n()) # Baja Peninsula, Mexico baja_pen \u0026lt;- data.frame( order = 1:7, lon = c(-116.4435, -114.6800, -109.6574, -111.9503, -112.2537, -113.7918, -114.1881), lat = c(30.9639, 30.7431, 22.9685, 26.9003, 25.0391, 29.4619, 28.0929) ) # Bohai Sea, China bohai_sea \u0026lt;- data.frame( order = 1:7, lon = c(122.0963, 121.2723, 121.0687, 121.8742, 120.2962, 117.6650, 122.6380), lat = c(39.0807, 39.0086, 37.7842, 40.7793, 40.0691, 38.4572, 37.4494) )  Sequential sites Now that we have our sample sites it is time to order them correctly along the coast sequentially. Should one prefer the opposite order to what seq_sites() produces, this may be changed by using the reverse argument found within the function. Additionally, if one has sites located on islands just off the coast, one may choose to allow the algorithm to take these islands into account. Note that this then will force the algorithm to calculate the sequential order of these sites as though they were part of a different sequence because they will no longer be on the same 1D plain. Generally this would not be desirable and one would rather order sites on coastal islands in line with the rest of the coast. This is the default setting, but we may see how this changes with the Baja Peninsula site list.\n# NB: This code will produce warnings # This is fine as it is stating that the # 'order' column has been re-written, # which is the intended result of this function. # Cape Point, South Africa cape_point_seq \u0026lt;- seq_sites(cape_point) # South Africa south_africa_seq \u0026lt;- seq_sites(south_africa) # Baja Peninsula, Mexico baja_pen_seq \u0026lt;- seq_sites(baja_pen) baja_pen_island_seq \u0026lt;- seq_sites(baja_pen, coast = FALSE) # Bohai sea, China bohai_sea_seq \u0026lt;- seq_sites(bohai_sea)  Comparison With the sites correctly ordered sequentially along the coast we may now compare the before and after products. To do so in a tidy way we will first create a function that plots our sites for us on a global map.\n# Create base map world_map \u0026lt;- ggplot() + borders(fill = \u0026quot;grey40\u0026quot;, colour = \u0026quot;black\u0026quot;) # Create titles titles \u0026lt;- c(\u0026quot;Deurmekaar\u0026quot;, \u0026quot;Sequential\u0026quot;, \u0026quot;Islands\u0026quot;) # Plotting function plot_sites \u0026lt;- function(site_list, buffer, title_choice){ world_map + geom_point(data = site_list, size = 6, aes(x = lon, y = lat, colour = as.factor(order))) + coord_cartesian(xlim = c(min(site_list$lon - buffer), max(site_list$lon + buffer)), ylim = c(min(site_list$lat - buffer), max(site_list$lat + buffer))) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, colour = \u0026quot;Site\\norder\u0026quot;) + ggtitle(titles[title_choice]) }  Cape Point, South Africa cape_point_map \u0026lt;- plot_sites(cape_point, 0.5, 1) cape_point_seq_map \u0026lt;- plot_sites(cape_point_seq, 0.5, 2) grid.arrange(cape_point_map, cape_point_seq_map, nrow = 1)  (\\#fig:cape_point_comp)Comparison of site ordering around Cape Point, South Africa.\n South Africa south_africa_map \u0026lt;- plot_sites(south_africa, 1, 1) south_africa_seq_map \u0026lt;- plot_sites(south_africa_seq, 1, 2) grid.arrange(south_africa_map, south_africa_seq_map, nrow = 1)  (\\#fig:south_africa_comp)Comparison of site ordering around South Africa.\n Baja Peninsula, Mexico Note in the image below that site seven in the \u0026lsquo;Islands\u0026rsquo; panel appears to be ordered incorrectly. This is because we have asked the function to first look for sites along the coast, and then order sites around nearby islands by setting the argument coast to TRUE. This is because the algorithm only works on one continuous line. When islands are introduced this then represents a second set of 1D coordinates and so the algorithm plans accordingly. This feature has been added so that if one chooses to have islands be apart from the initial ordering of the coastal sites it may be done. The default however is to remove islands from the coastal land mask so that they are ordered according to their nearest location to the coast.\nbaja_pen_map \u0026lt;- plot_sites(baja_pen, 1, 1) baja_pen_seq_map \u0026lt;- plot_sites(baja_pen_seq, 1, 2) baja_pen_island_seq_map \u0026lt;- plot_sites(baja_pen_island_seq, 1, 3) grid.arrange(baja_pen_map, baja_pen_seq_map, baja_pen_island_seq_map, nrow = 1)  (\\#fig:baja_pen_comp)Comparison of site ordering around the Baja Peninsula, Mexico.\n Bohai Sea, China Below in the \u0026lsquo;Sequential\u0026rsquo; panel we see the result of having set the reverse argument to TRUE. Hardly noticeable, but potentially useful.\nbohai_sea_map \u0026lt;- plot_sites(bohai_sea, 1, 1) bohai_sea_seq_map \u0026lt;- plot_sites(bohai_sea_seq, 1, 2) grid.arrange(bohai_sea_map, bohai_sea_seq_map, nrow = 1)  (\\#fig:bohai_sea_comp)Comparison of site ordering around the Bohai Sea, China.\n Conclusion The usefulness of the seq_sites() function is demonstrated above on a number of different scales and coastal features. This is in no way an exhaustive test of this function and I welcome any input from anyone that uses it for their own work. The premise on which this function operates is very basic and so theoretically it should be very adaptive. The only thing to look out for is if one has a very convoluted coastline with a long stretch without any sites as the algorithm may think this is two separate coastlines.\n","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"d56597bc3a19b65a2764e3a455117836","permalink":"https://theoceancode.netlify.app/post/seq_sites/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/post/seq_sites/","section":"post","summary":"Preface The rest of the blog post after this preface section is a copy of the vignette I\u0026rsquo;ve written for the first function in the new package I am developing: coastR. This package aims to provide functions that are useful for coastal oceanography but that do not yet exist in the R language. It is not my intention to provide algorithms for physical oceanography as these may already be found elsewhere.","tags":["coastal"],"title":"Sequential sites","type":"post"},{"authors":null,"categories":["R"],"content":" Objective There are many different things that require scientists to use programming languages (like R). Far too many to count here. There is however one common use amongst almost all environmental scientists: mapping. Almost every report, research project or paper will have need to refer to a study area. This is almost always \u0026ldquo;Figure 1\u0026rdquo;. To this end, whenever I teach R, or run workshops on it, one of the questions I am always prepared for is how to create a map of a particular area. Being a happy convert to the tidyverse I only teach the graphics of ggplot2. I have found that people often prefer to use the ggmap extension to create ggplot quality figures with Google map backgrounds, but I personally think that a more traditional monotone background for maps looks more professional. What I\u0026rsquo;ve decided to showcase this week is the data and code required to create a publication quality map. Indeed, the following code will create the aforementioned obligatory \u0026ldquo;Figure 1\u0026rdquo; in a paper I am currently preparing for submission.\nData There are heaps of packages etc. that one may use to create maps. And there is a never ending source of blogs, books and tutorials that illustrate many of the different ways to visualise spatial data. For my international and geographic borders I prefer to use data I\u0026rsquo;ve downloaded from GSHHSG and then converted to dataframes using functions found in the PBSmapping package. I then save these converted dataframes as .Rdata objects on my computer for ease of use with all of my projects. For the domestic borders of a country, which I won\u0026rsquo;t use in this post, one may go here. Note however that for some strange reason this website still has the pre-1994 borders for South Africa. For the correct SA borders one must go here. The current SA borders may actually be download in the .Rdata format, which is neat.\nOnce one has the borders to be used in the map, the next step is to think about what one actually wants to show. The main purpose of this map is to show where several in situ coastal seawater temperature time series were collected. This could be done quite simply but a plain black and white map is offensively boring so we want to make sure there is a good amount of (but not too much!) colour in order to entice the reader. I personally find pictures of meso-scale oceanic phenomena particularly beautiful so try to include them whenever I can. Luckily that is also what I study so it is not strange that I include such things in my work. Now if only I studied panda\u0026rsquo;s, too\u0026hellip;\nPanda\u0026rsquo;s aside, the current work I am engaged in also requires that the atmospheric processes around southern Africa be considered in addition to the oceanography. To visualise both air and sea concurrently would be a mess so we will want to create separate panels for each. Because I have been working with reanalysis data lately, and not satellite data, I am also able to include the wind/ current vectors in order to really help the temperature patterns pop. The oceanic data are from the BRAN2016 product and the atmospheric data are from ERA-Interim. Both of which are available for download for free for scientific pursuits. I\u0026rsquo;ve chosen here to use the mean values for January 1st as the summer months provide the most clear example of the thermal differences between the Agulhas and Benguela currents. The code used to create the scale bar in the maps may be found here. It\u0026rsquo;s not a proper ggplot geom function but works well enough. I\u0026rsquo;ve also decided to add the 200 m isobath to the sea panel. These data come from NOAA.\n## Libraries library(tidyverse) library(viridis) library(gridExtra) ## Data # South Africa map data load(\u0026quot;../../static/data/southern_africa_coast.Rdata\u0026quot;) # Lowres names(southern_africa_coast)[1] \u0026lt;- \u0026quot;lon\u0026quot; load(\u0026quot;../../static/data/sa_shore.Rdata\u0026quot;) # Hires names(sa_shore)[4:5] \u0026lt;- c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;) # International borders load(\u0026quot;../../static/data/africa_borders.Rdata\u0026quot;) # Reanalysis data load(\u0026quot;../../static/data/all_jan1_0.5.Rdata\u0026quot;) names(all_jan1_0.5)[1:2] \u0026lt;- c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;) # In situ time series locations site_list \u0026lt;- read_csv(\u0026quot;../../static/data/mg_site_list.csv\u0026quot;) site_list$order \u0026lt;- 1:nrow(site_list) # Bathymetry data load(\u0026quot;../../static/data/sa_bathy.Rdata\u0026quot;) ## Scale bar function source(\u0026quot;../../static/func/scale.bar.func.R\u0026quot;)  Mapping I find that it is easier to keep track of the different aspects of a map when they are stored as different dataframes. One should however avoid having too many loose dataframes running about in the global environment. It is a balancing act and requires one to find a happy middle ground. Here I am going to cut the all_jan1_0.5 dataframe into 4. One each for air and sea temperatures and vectors. I am also going to reduce the resolution of the wind so that the vectors will plot more nicely.\n# Devide the reanalysis data sea_temp \u0026lt;- filter(all_jan1_0.5, variable == \u0026quot;BRAN/temp\u0026quot;) air_temp \u0026lt;- filter(all_jan1_0.5, variable == \u0026quot;ERA/temp\u0026quot;) currents \u0026lt;- filter(all_jan1_0.5, variable == \u0026quot;BRAN/u\u0026quot; | variable == \u0026quot;BRAN/v\u0026quot;) %\u0026gt;% select(-date, -index) %\u0026gt;% spread(key = variable, value = value) %\u0026gt;% rename(u = \u0026quot;BRAN/u\u0026quot;, v = \u0026quot;BRAN/v\u0026quot;) winds \u0026lt;- filter(all_jan1_0.5, variable == \u0026quot;ERA/u\u0026quot; | variable == \u0026quot;ERA/v\u0026quot;) %\u0026gt;% select(-date, -index) %\u0026gt;% spread(key = variable, value = value) %\u0026gt;% rename(u = \u0026quot;ERA/u\u0026quot;, v = \u0026quot;ERA/v\u0026quot;) # Reduce wind/ current vectors lon_sub \u0026lt;- seq(10, 40, by = 1) lat_sub \u0026lt;- seq(-40, -15, by = 1) # currents \u0026lt;- currents[(currents$lon %in% lon_sub \u0026amp; currents$lat %in% lat_sub),] winds \u0026lt;- winds[(winds$lon %in% lon_sub \u0026amp; winds$lat %in% lat_sub),]  With just a few alterations to our nicely divided up dataframes we are ready to create a map. We will look at the code required to create each map and then put it all together in the end.\nFirst up is the most busy. The following code chunk will create the top panel of our map, the sea state. It is necessary to label all of the locations mentioned in the text and so they are thrown on here. In order to make the site label easier to read I\u0026rsquo;ve made them red. This is particularly jarring but I think I like it.\n# Establish the vector scalar for the currents current_uv_scalar \u0026lt;- 2 # The top figure (sea) mg_top \u0026lt;- ggplot(data = southern_africa_coast, aes(x = lon, y = lat)) + # The ocean temperature geom_raster(data = sea_temp, aes(fill = value)) + # The bathymetry stat_contour(data = sa_bathy[sa_bathy$depth \u0026lt; -200 \u0026amp; sa_bathy$depth \u0026gt; -2000,], aes(x = lon, y = lat, z = depth, alpha = ..level..), colour = \u0026quot;ivory\u0026quot;, size = 0.5, binwidth = 1000, na.rm = TRUE, show.legend = FALSE) + # The current vectors geom_segment(data = currents, aes(xend = lon + u * current_uv_scalar, yend = lat + v * current_uv_scalar), arrow = arrow(angle = 15, length = unit(0.02, \u0026quot;inches\u0026quot;), type = \u0026quot;closed\u0026quot;), alpha = 0.4) + # The land mass geom_polygon(aes(group = group), fill = \u0026quot;grey70\u0026quot;, colour = \u0026quot;black\u0026quot;, size = 0.5, show.legend = FALSE) + geom_path(data = africa_borders, aes(group = group)) + # The legend for the vector length geom_label(aes(x = 36, y = -37, label = \u0026quot;1.0 m/s\\n\u0026quot;), size = 3, label.padding = unit(0.5, \u0026quot;lines\u0026quot;)) + geom_segment(aes(x = 35, y = -37.5, xend = 37, yend = -37.5)) + # The in situ sites geom_point(data = site_list, shape = 1, size = 2.8, colour = \u0026quot;ivory\u0026quot;) + geom_text(data = site_list, aes(label = order), size = 1.9, colour = \u0026quot;red\u0026quot;) + # Oceans annotate(\u0026quot;text\u0026quot;, label = \u0026quot;INDIAN\\nOCEAN\u0026quot;, x = 37.00, y = -34.0, size = 4.0, angle = 0, colour = \u0026quot;ivory\u0026quot;) + annotate(\u0026quot;text\u0026quot;, label = \u0026quot;ATLANTIC\\nOCEAN\u0026quot;, x = 13.10, y = -34.0, size = 4.0, angle = 0, colour = \u0026quot;ivory\u0026quot;) + # Benguela geom_segment(aes(x = 17.2, y = -32.6, xend = 15.2, yend = -29.5), arrow = arrow(length = unit(0.3, \u0026quot;cm\u0026quot;)), size = 0.5, colour = \u0026quot;ivory\u0026quot;) + annotate(\u0026quot;text\u0026quot;, label = \u0026quot;Benguela\u0026quot;, x = 16.0, y = -31.8, size = 3.5, angle = 298, colour = \u0026quot;ivory\u0026quot;) + # Agulhas geom_segment(aes(x = 33, y = -29.5, xend = 29.8, yend = -33.0), arrow = arrow(length = unit(0.3, \u0026quot;cm\u0026quot;)), size = 0.5, colour = \u0026quot;ivory\u0026quot;) + annotate(\u0026quot;text\u0026quot;, label = \u0026quot;Agulhas\u0026quot;, x = 31.7, y = -31.7, size = 3.5, angle = 53, colour = \u0026quot;ivory\u0026quot;) + # Agulhas Bank annotate(\u0026quot;text\u0026quot;, label = \u0026quot;Agulhas\\nBank\u0026quot;, x = 22.5, y = -35.5, size = 3.0, angle = 0, colour = \u0026quot;ivory\u0026quot;) + # Cape Peninsula annotate(\u0026quot;text\u0026quot;, label = \u0026quot;Cape\\nPeninsula\u0026quot;, x = 17.2, y = -35, size = 3.0, angle = 0, colour = \u0026quot;ivory\u0026quot;) + # Improve on the x and y axis labels scale_x_continuous(breaks = seq(15, 35, 5), labels = scales::unit_format(prefix = \u0026quot;°E\u0026quot;, sep = \u0026quot;\u0026quot;), position = \u0026quot;top\u0026quot;) + scale_y_continuous(breaks = seq(-35, -30, 5), labels = c(\u0026quot;35°S\u0026quot;, \u0026quot;30°S\u0026quot;)) + labs(x = NULL, y = NULL) + # Slightly shrink the plotting area coord_cartesian(xlim = c(10.5, 39.5), ylim = c(-39.5, -25.5), expand = F) + # Use viridis colour scheme scale_fill_viridis(name = \u0026quot;Temp.\\n(°C)\u0026quot;, option = \u0026quot;D\u0026quot;) + # Adjust the theme theme_bw() + theme(panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = 1), axis.text = element_text(colour = \u0026quot;black\u0026quot;), axis.ticks = element_line(colour = \u0026quot;black\u0026quot;))  Many of the sites that need to be plotted are laying on top of each other. This is never good, but is made worse when the sites in question are refereed to frequently in the text. For this reason we need to create a little panel inside of the larger figure that shows a zoomed in picture of False Bay. Complete with text labels.\n# False Bay inset fb \u0026lt;- ggplot(data = sa_shore, aes(x = lon, y = lat)) + # The land mass geom_polygon(aes(group = PID), fill = \u0026quot;grey70\u0026quot;, colour = NA, size = 0.5, show.legend = FALSE) + # The in situ sites geom_point(data = site_list, shape = 1, size = 3, colour = \u0026quot;black\u0026quot;) + geom_text(data = site_list, aes(label = order), size = 2.3, colour = \u0026quot;red\u0026quot;) + # Text label geom_text(aes(x = 18.65, y = -34.25, label = \u0026quot;False\\nBay\u0026quot;), size = 2.7) + # Control the x and y axes coord_cartesian(xlim = c(18.2, 19), ylim = c(-34.5, -33.8), expand = F) + scale_x_continuous(breaks = c(18.5), label = \u0026quot;18.5°E\u0026quot;) + scale_y_continuous(breaks = c(-34.1), label = \u0026quot;34.1°S\u0026quot;) + labs(x = NULL, y = NULL) + # Change the theme for cleaner over-plotting theme_bw() + theme(plot.background = element_blank(), axis.text = element_text(colour = \u0026quot;ivory\u0026quot;), axis.text.y = element_text(angle = 90, hjust = 0.5), axis.ticks = element_line(colour = \u0026quot;ivory\u0026quot;), panel.border = element_rect(colour = \u0026quot;ivory\u0026quot;), panel.grid = element_blank())  We could possibly create another inset panel for the clomp of sites around Hamburg but this figure is already getting too busy. So we\u0026rsquo;ll leave it for now. One inset panel will serve to illustrate the code necessary to create a faceted map so for the purposes of this post it will also suffice. That leaves us with only the bottom panel to create. The air state. I\u0026rsquo;ve decided to put the scale bar/ North arrow on this panel in an attempt to balance the amount of information in each panel.\n# Establish the vector scalar for the wind wind_uv_scalar \u0026lt;- 0.5 # The bottom figure (air) mg_bottom \u0026lt;- ggplot(data = southern_africa_coast, aes(x = lon, y = lat)) + # The ocean temperature geom_raster(data = air_temp, aes(fill = value)) + # The land mass geom_polygon(aes(group = group), fill = NA, colour = \u0026quot;black\u0026quot;, size = 0.5, show.legend = FALSE) + geom_path(data = africa_borders, aes(group = group)) + # The current vectors geom_segment(data = winds, aes(xend = lon + u * wind_uv_scalar, yend = lat + v * wind_uv_scalar), arrow = arrow(angle = 15, length = unit(0.02, \u0026quot;inches\u0026quot;), type = \u0026quot;closed\u0026quot;), alpha = 0.4) + # The legend for the vector length geom_label(aes(x = 36, y = -37, label = \u0026quot;4.0 m/s\\n\u0026quot;), size = 3, label.padding = unit(0.5, \u0026quot;lines\u0026quot;)) + geom_segment(aes(x = 35, y = -37.5, xend = 37, yend = -37.5)) + # Improve on the x and y axis labels scale_x_continuous(breaks = seq(15, 35, 5), labels = scales::unit_format(prefix = \u0026quot;°E\u0026quot;, sep = \u0026quot;\u0026quot;)) + scale_y_continuous(breaks = seq(-35, -30, 5), labels = c(\u0026quot;35°S\u0026quot;, \u0026quot;30°S\u0026quot;)) + labs(x = NULL, y = NULL) + # Scale bar scaleBar(lon = 13, lat = -38.0, distanceLon = 200, distanceLat = 50, distanceLegend = 90, dist.unit = \u0026quot;km\u0026quot;, arrow.length = 200, arrow.distance = 130, arrow.North.size = 4) + # Slightly shrink the plotting area coord_cartesian(xlim = c(10.5, 39.5), ylim = c(-39.5, -25.5), expand = F) + # Use viridis colour scheme scale_fill_viridis(name = \u0026quot;Temp.\\n(°C)\u0026quot;, option = \u0026quot;A\u0026quot;) + # Adjust the theme theme_bw() + theme(panel.border = element_rect(fill = NA, colour = \u0026quot;black\u0026quot;, size = 1), axis.text = element_text(colour = \u0026quot;black\u0026quot;), axis.ticks = element_line(colour = \u0026quot;black\u0026quot;))  With our three pieces of the map complete, it is time to stick them together. There are many ways to do this but I have recently found that using annotation_custom allows one to stick any sort of ggplot like object onto any other sort of ggplot object. This is an exciting development and opens up a lot of doors for some pretty creative stuff. Here I will just use it to demonstrate simple faceting, but combined with panel gridding. Really though the sky is the limit.\n# Convert the figures to grobs mg_top_grob \u0026lt;- ggplotGrob(mg_top) fb_grob \u0026lt;- ggplotGrob(fb) mg_bottom_grob \u0026lt;- ggplotGrob(mg_bottom) # Stick them together gg \u0026lt;- ggplot() + # First set the x and y axis values so we know what the ranges are # in order to make it easier to place our facets coord_equal(xlim = c(1, 10), ylim = c(1, 10), expand = F) + # Then we place our facetsover one another using the coordinates we created annotation_custom(mg_top_grob, xmin = 1, xmax = 10, ymin = 5.5, ymax = 10) + annotation_custom(fb_grob, xmin = 3.5, xmax = 5.5, ymin = 7.2, ymax = 8.8) + annotation_custom(mg_bottom_grob, xmin = 1, xmax = 10, ymin = 1, ymax = 5.5)  Summary The developments in the gridding system have brought the potential for using ggplot for these more complex maps forward quite a bit. As long as one does not use a constrained mapping coordinate system (i.e. coord_fixed) the grob-ification of the ggplot objects seems to allow the placing of the pieces into a common area to be performed smoothly. Displaying many different bits of information cleanly is always a challenge. This figure is particularly busy, out of necessity. I think it turned out very nicely though.\nFigure 1: Map showing the southern tip of the African continent. The top panel shows the typical sea surface temperature and surface currents on January 1st. The bottom panel likewise shows the typical surface air temperatures and winds on any given January 1st.\n","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"58d3999195805433f05fb210b287e6d8","permalink":"https://theoceancode.netlify.app/post/mapping_with_ggplot2/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/post/mapping_with_ggplot2/","section":"post","summary":"Objective There are many different things that require scientists to use programming languages (like R). Far too many to count here. There is however one common use amongst almost all environmental scientists: mapping. Almost every report, research project or paper will have need to refer to a study area. This is almost always \u0026ldquo;Figure 1\u0026rdquo;. To this end, whenever I teach R, or run workshops on it, one of the questions I am always prepared for is how to create a map of a particular area.","tags":["visuals","mapping","wind","ocean","atmosphere"],"title":"Mapping with ggplot2","type":"post"},{"authors":null,"categories":["R"],"content":" Objective A few weeks ago for a post about the relationship between gender equality and GDP/ capita I found a nifty website that has a massive amount of census information for most countries on our planet. Much of this information could be used to answer some very interesting and/ or important questions. But some of the data can be used to answer seemingly pointless questions. And that\u0026rsquo;s what I intend to do this week. Specifically, which countries in the world have the highest rates of goats/ capita?\nData The goats per capita data were downloaded from the clia-infra website. These data are already in the format we need so there is little to be done before jumping straight into the analysis. We will however remove any records from before 1900 as these are almost entirely estimates, and not real records.\n# Load libraries library(tidyverse) library(gridExtra) # Load data goats \u0026lt;- read_csv(\u0026quot;../../static/data/GoatsperCapita_Compact.csv\u0026quot;) %\u0026gt;% filter(year \u0026gt;= 1900)  Analysis First of all, I would like to know what the global trend in goats/ capita has been since 1900. To do so we need to create annual averages and apply a simple linear model to them. We will also plot boxplots to give us an idea of the spread of goats/ capita over the world.\ngoats %\u0026gt;% group_by(year) %\u0026gt;% select(-ccode, -country.name) %\u0026gt;% summarise(value = mean(value)) %\u0026gt;% ggplot(aes(x = year, y = value)) + geom_boxplot(data = goats, aes(group = year)) + geom_smooth(method = \u0026quot;lm\u0026quot;)  Figure 1: Boxplots with a fitted linear model showing the global trend in goats/ capita over the last century.\nAs we may see in Figure 1, the overall trend in goats/ capita in the world has been decreasing very slightly over the last century. The striking result from Figure 1 however is the massive range of values as seen by the outliers from the boxplots. So which countries are these that have so many more goats/ capita than the rest of the world?\nWe want to see which countries have the most goats/ capita but there are 172 unique countries in this dataset so it would look much too busy to plot them all. To that end we want only the top and bottom 10 countries from the most recent year of reporting (2010).\n# Top 10 goat having countries goats_top \u0026lt;- goats %\u0026gt;% arrange(desc(value)) %\u0026gt;% filter(ccode %in% head(unique(ccode), 10)) # Bottom 10 goats_bottom \u0026lt;- goats %\u0026gt;% arrange(value) %\u0026gt;% filter(ccode %in% head(unique(ccode), 10)) # Line graphs gt \u0026lt;- ggplot(data = goats_top, aes(x = year, y = value)) + geom_line(aes(colour = country.name)) + labs(y = \u0026quot;Goats/ Capita\u0026quot;, x = NULL) + scale_x_continuous(expand = c(0,0)) + scale_color_brewer(name = NULL, palette = \u0026quot;Set3\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) gb \u0026lt;- ggplot(data = goats_bottom, aes(x = year, y = value)) + geom_line(aes(colour = country.name)) + labs(y = \u0026quot;Goats/ Capita\u0026quot;, x = NULL) + scale_x_continuous(expand = c(0,0)) + theme(legend.position = \u0026quot;bottom\u0026quot;, legend.title = element_blank()) # Combine grid.arrange(gt, gb)  Figure 2: Line graphs showing the rate of goats/ capita for the top and bottom 10 goat having countries in the world over the last century.\nSummary I was a bit surprised to find that Mongolia is far and away the country with the most goats/ capita at 5.140 in 2010. Less surprising is that the other top 9 goat having countries in the world in 2010 were all in Africa and their rate of goats/ capita was between 1.634 (Mauritania) to 0.124 (South Africa). This makes for a massive spread in what is already an outlying set of countries. How is it that Mongolia has so many more goats/ capita? This is a very odd result but the data were reported annually from 2000 to 2010 and they consistently show similarly high rates for Mongolia.\nThe bottom 10 goat having countries in the world are a mix of European, Asian, North American and Pacific Islands. This mix is not surprising as we may see in Figure 1 that there are no outliers in the bottom of the distribution. The highest value for the bottom 10 countries in 2010 was Tonga at 0.121. This is very close to the lowest value from the top 10 countries, and shows us that most of the 172 countries in this dataset have ~0.12 goats per person. With this average in mind, we see that the other bottom nine countries in Figure 2 really are much lower than the global average with rates approaching 0 goats/ capita. It is worth mentioning that the lowest overall rate of goats/ capita in 2010 was Japan at 0.0001. Meaning that there is only one goat in Japan for every 10,000 people. As opposed to Mongolia that has more than five goats for every one person. Therefore there were 50,000 times more goats/ capita in Mongolia than Japan in 2010\u0026hellip;\nI supposes the take away message from this analysis is that if one ever wants to get away from it all and just go spend time with a lot of goats, Mongolia is the place for you!\n(and definitely avoid Japan)\n","date":1499644800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499644800,"objectID":"5ce639dfbcfa42c6eddc58157c34d131","permalink":"https://theoceancode.netlify.app/post/goats_per_capita/","publishdate":"2017-07-10T00:00:00Z","relpermalink":"/post/goats_per_capita/","section":"post","summary":"Objective A few weeks ago for a post about the relationship between gender equality and GDP/ capita I found a nifty website that has a massive amount of census information for most countries on our planet. Much of this information could be used to answer some very interesting and/ or important questions. But some of the data can be used to answer seemingly pointless questions. And that\u0026rsquo;s what I intend to do this week.","tags":["goats"],"title":"Goats per capita","type":"post"},{"authors":null,"categories":["R"],"content":" Objective As an immigrant myself, all of the talk of immigration to be found in main stream media outlets today makes me a bit nervous. Whereas most people that speak of the pro\u0026rsquo;s and con\u0026rsquo;s of immigration do so from the point of view of how it may affect the country of their birth, I view this issue as something that affects my ability to live outside the country of my birth. I immigrated into the Republic of South Africa in 2013 and have been living here since. I would do a piece on South African immigration but the numbers are difficult to get a hold of and honestly most people are less interest in South Africa than the USA.\nImmigration is not a new talking point. It\u0026rsquo;s something that comes up in political and a-political circles all of the time. The current debate on the Muslim Ban in the USA may have reached a new level for this sort of rhetoric in the West, but targeted crackdowns of this sort are not new in the world. I won\u0026rsquo;t bother with citations here, but if one is interested a quick google of \u0026ldquo;xenophobia\u0026rdquo; + \u0026ldquo;border control\u0026rdquo; should yield some convincing results. As this current row of immigration debates in the USA has become so partisan, I decided that an interesting question to ask would be \u0026ldquo;Under which of the two parties have more people immigrated into the USA?\u0026rdquo; and \u0026ldquo;Under which of the two parties have more people been removed from the USA?\u0026rdquo;\nData The historical data on immigration into the USA are located at the Department of Homeland Securities website. In 2013 the DHS started keeping very detailed reports of all immigration by age, country, marital status, etc. These highly detailed data are very interesting but will not help us to ask our central questions. We want long time series of data so that we may compare many different administrations from each party. For ease of analysis I have chosen to classify the party in power at any point in time based on the party of the President. I understand that the Senate or Congress would perhaps be better, if not more egalitarian choices, but the current focus of this issue has the US President at it\u0026rsquo;s core, so I decided to keep that theme constant in this analysis. I\u0026rsquo;ll only start from Eisenhower and go up until Obama as the publicly available DHS data end in 2015. They begin as far back as 1892, but ggplot2 has built into it a US president dataframe and I am going to just use that because I\u0026rsquo;m lazy.\n# Libraries library(tidyverse) library(lubridate) library(broom) library(gridExtra) # President data data(presidential) presidential$start \u0026lt;- year(presidential$start) presidential$end \u0026lt;- year(presidential$end)-1 # Create index of party years # I couldn't think of a clever automatic way of doing this... party_year \u0026lt;- data.frame(Year = seq(1953, 2016), Party = c(rep(\u0026quot;Republican\u0026quot;,8), rep(\u0026quot;Democrat\u0026quot;,8), rep(\u0026quot;Republican\u0026quot;,8), rep(\u0026quot;Democrat\u0026quot;,4), rep(\u0026quot;Republican\u0026quot;,12), rep(\u0026quot;Democrat\u0026quot;,8), rep(\u0026quot;Republican\u0026quot;,8), rep(\u0026quot;Democrat\u0026quot;,8))) # Immigration data green_card \u0026lt;- read_csv(\u0026quot;../../static/data/Persons Obtaining Lawful Permanent Resident Status-Fiscal Years 1820 to 2015.csv\u0026quot;) green_card \u0026lt;- merge(green_card, party_year, by = \u0026quot;Year\u0026quot;) removals \u0026lt;- read_csv(\u0026quot;../../static/data/Aliens Removed Or Returned-Fiscal Years 1892 To 2015.csv\u0026quot;) removals \u0026lt;- merge(removals, party_year, by = \u0026quot;Year\u0026quot;)  Immigration In this first figure we are defining immigration as the number of people actually receiving a green card in any given year. This is the most strict definition of \u0026ldquo;immigration\u0026rdquo; and I think may be best used to show whom the USA was choosing to let in.\nggplot(data = green_card, aes(x = Year, y = Number)) + geom_col(aes(fill = Party), colour = \u0026quot;black\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, colour = \u0026quot;black\u0026quot;) + labs(x = NULL, y = \u0026quot;Green Cards Granted\u0026quot;) + scale_fill_manual(values = c(\u0026quot;slateblue1\u0026quot;, \u0026quot;firebrick1\u0026quot;))  Figure 1: Bar charts showing the number of green cards granted each year in the USA. The colour of the bars show the ruling party at the time of issuance. A linear model is imposed in black.\nWe may see in Figure 1 that the all time high for the granting of green cards was during the four year administration of George Bush Senior. These values are so much higher than the other administrations that it leverages the linear model drawn on these data up past where it should normally be to show the more normal trend exhibited by all of the other administrations since Eisenhower. That being said, we actually see a bit of a turn down during the Obama administration, with the largest year of green card issuance during the George Bush Junior administration larger than any year under Obama. I find that surprising. Figure 1 also shows us that we can\u0026rsquo;t really directly compare the different administrations because as populations increase, so too will the number of people that want to immigrate. In a quick pinch however we may use the residuals from the linear model to give us a slightly better visualisation of how the parties stack up against one another.\n# Calculate residuals green_card_resids \u0026lt;- augment(lm(Number~Year, data = green_card)) green_card_resids \u0026lt;- merge(green_card_resids, party_year, by = \u0026quot;Year\u0026quot;) # Plot them ggplot(data = green_card_resids, aes(x = Year, y = .resid)) + geom_col(aes(fill = Party), colour = \u0026quot;black\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, colour = \u0026quot;black\u0026quot;) + labs(x = NULL, y = \u0026quot;Green Cards Granted\u0026quot;) + scale_fill_manual(values = c(\u0026quot;slateblue1\u0026quot;, \u0026quot;firebrick1\u0026quot;))  Figure 2: The residuals from a linear model fitted to the data shown in Figure 1.\nIt appears as though whenever there was a Bush in office it was much easier to get a green card. And that Democrats generally made it more difficult to do so.\nExpulsion Now that we have seen that it is easier to enter the USA during a Republican presidency, let\u0026rsquo;s see under which party an illegal immigrant is most likely to be expelled. There are two different classes of expulsion: \u0026lsquo;Removal\u0026rsquo; and \u0026lsquo;Return\u0026rsquo;. Removal means that a legal order was issued to remove the individual. Return means that the individual was likewise not legally in the states, but left of their own volition.\n# Returns return_bar \u0026lt;- ggplot(data = removals, aes(x = Year, y = Returns)) + geom_col(aes(fill = Party), colour = \u0026quot;black\u0026quot;) + # geom_smooth(method = \u0026quot;lm\u0026quot;, colour = \u0026quot;black\u0026quot;) + labs(x = NULL, y = \u0026quot;Immigrants Returned\u0026quot;) + scale_fill_manual(values = c(\u0026quot;slateblue1\u0026quot;, \u0026quot;firebrick1\u0026quot;)) + ggtitle(\u0026quot;Returns\u0026quot;) # Removals removal_bar \u0026lt;- ggplot(data = removals, aes(x = Year, y = Removals)) + geom_col(aes(fill = Party), colour = \u0026quot;black\u0026quot;) + # geom_smooth(method = \u0026quot;lm\u0026quot;, colour = \u0026quot;black\u0026quot;) + labs(x = NULL, y = \u0026quot;Immigrants Removed\u0026quot;) + scale_fill_manual(values = c(\u0026quot;slateblue1\u0026quot;, \u0026quot;firebrick1\u0026quot;)) + ggtitle(\u0026quot;Removals\u0026quot;) # Stick'em grid.arrange(return_bar, removal_bar)  Figure 3: Two bar charts showing the rates of returns and removals for immigrants from the USA.\nFigure 3 tells a very interesting story. In the top panel (Returns), we see that from 1953-55 (shortly after WWII) there were massive numbers of immigrants that returned to their home countries voluntarily. Then there is a period of increase leading up to the 80\u0026rsquo;s. This then follows a somewhat normal distribution, peaking in the late 90\u0026rsquo;s near the end of the Clinton administration before the peaceful return of immigrants drops steadily through the 8 years of Bush then Obama. The bottom panel (Removals) shows the reason for this apparent relaxation on immigrants. It isn\u0026rsquo;t that immigrants were being sent away less, but rather they began to be removed more forcefully than appears to have been the policy until something changed during the Clinton Administration. The rate of immigrants being removed became greater than those being returned in 2011 under Obama. Again we see a heavy hand on immigration during years with a Democratic president in office. It is hard to compare the parties on this issue as the policy of forcefully removing immigrants in favour of having them leave peacefully has only been in practice over three administrations (Clinton, Bush Jr. and Obama). That being said, it is worth noting that the y axes on these two figures are not the same. The increase in removals does not outweigh the decrease in returns. Overall the rate of expulsion of immigrants from the states declined during the Obama administration. And perhaps also Bush Jr.\nIt is fair criticism to point out that a green card may take several years to acquire. This means that when one begins the process of applying for a green card it may take so long that a different party will be in power by the time it is granted\u0026hellip; or not. I would argue however that most of these parties (with the exception of Carter) are in office for eight year stretches. This is not meant to be a definitive analysis, but I think it has proven to be a rather interesting first step. I didn\u0026rsquo;t expect the data to look like this. George Bush Senior, saviour to immigrants, who would have thought.\n","date":1499040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499040000,"objectID":"d4858617b345881a7994e9e86622b06b","permalink":"https://theoceancode.netlify.app/post/party_immigration/","publishdate":"2017-07-03T00:00:00Z","relpermalink":"/post/party_immigration/","section":"post","summary":"Objective As an immigrant myself, all of the talk of immigration to be found in main stream media outlets today makes me a bit nervous. Whereas most people that speak of the pro\u0026rsquo;s and con\u0026rsquo;s of immigration do so from the point of view of how it may affect the country of their birth, I view this issue as something that affects my ability to live outside the country of my birth.","tags":["politics"],"title":"Party immigration","type":"post"},{"authors":null,"categories":["R"],"content":" Objective With more and more scientists moving to open source software (i.e. R or Python) to perform their numerical analyses the opportunities for collaboration increase and we may all benefit from this enhanced productivity. At the risk of sounding sycophantic, the future of scientific research truly is in multi-disciplinary work. What then could be inhibiting this slow march towards progress? We tend to like to stick to what is comfortable. Oceanographers in South Africa have been using MATLAB and ODV (Ocean Data View) since about the time that Jesus was lacing up his sandals for his first trip to Palestine. There has been much debate on the future of MATLAB in science, so I won\u0026rsquo;t get into that here, but I will say that the package oce contains much of the code that one would need for oceanographic work in R, and the package angstroms helps one to work with ROMS (Regional Ocean Modeling System) output. The software that has however largely gone under the radar in these software debates has been ODV. Probably because it is free (after registration) it\u0026rsquo;s fate has not been sealed by university departments looking to cut costs. The issue with ODV however is the same with all Microsoft products; the sin of having a \u0026ldquo;pointy clicky\u0026rdquo; user interface. One cannot perform truly reproducible research with a menu driven user interface. The steps must be written out in code. And so here I will lay out those necessary steps to create an interpolated CTD time series of temperature values that looks as close to the default output of ODV as possible.\nColour palette Perhaps the most striking thing about the figures that ODV creates is it\u0026rsquo;s colour palette. A large criticism of this colour palette is that the range of colours used are not equally weighted visually, with the bright reds drawing ones eye more than the muted blues. This issue can be made up for using the viridis package, but for now we will stick to a ODV-like colour palette as that is part of our current objective. To create a colour palette that appears close to the ODV standard I used GIMP to extract the hexadecimal colour values from the colour bar in Figure 1.\n# Load libraries library(tidyverse) library(lubridate) library(reshape2) library(MBA) library(mgcv) # Load and screen data # For ease I am only using monthly means # and depth values rounded to 0.1 metres ctd \u0026lt;- read_csv(\u0026quot;../../static/data/ctd.csv\u0026quot;) %\u0026gt;% mutate(depth = -depth) %\u0026gt;% # Correct for plotting filter(site == 1) %\u0026gt;% select(date, depth, temperature) %\u0026gt;% rename(temp = temperature) #%\u0026gt;% ### Uncomment out the following lines to reduce the data resolution # mutate(date = round_date(date, unit = \u0026quot;month\u0026quot;)) %\u0026gt;% # mutate(depth = round(depth, 1)) %\u0026gt;% # group_by(date, depth) %\u0026gt;% # summarise(temp = round(mean(temp, na.rm = TRUE),1)) ### # Manually extracted hexidecimal ODV colour palette ODV_colours \u0026lt;- c(\u0026quot;#feb483\u0026quot;, \u0026quot;#d31f2a\u0026quot;, \u0026quot;#ffc000\u0026quot;, \u0026quot;#27ab19\u0026quot;, \u0026quot;#0db5e6\u0026quot;, \u0026quot;#7139fe\u0026quot;, \u0026quot;#d16cfa\u0026quot;) # Create quick scatterplot ggplot(data = ctd, aes(x = date, y = depth)) + geom_point(aes(colour = temp)) + scale_colour_gradientn(colours = rev(ODV_colours)) + labs(y = \u0026quot;depth (m)\u0026quot;, x = NULL, colour = \u0026quot;temp. (°C)\u0026quot;)  Figure 2: A non-interpolated scatterplot of our temperature (°C) data shown as a function of depth (m) over time.\nInterpolating Figure 2 is a far cry from the final product we want, but it is a step in the right direction. One of the things that sets ODV apart from other visualisation software is that it very nicely interpolates the data you give it. While this looks nice, there is some criticism that may be leveled against doing so. That being said, what we want is a pretty visualisation of our data. We are not going to be using these data for any numerical analyses so the main priority is that the output allows us to better visually interpret the data. The package MBA already has the necessary functionality to do this, and it works with ggplot2, so we will be using this to get our figure. The interpolation method used by mba.surf() is multilevel B-splines.\nIn order to do so we will need to dcast() our data into a wide format so that it simulates a surface layer. spread() from the tidyr package doesn\u0026rsquo;t quite do what we need as we want a proper surface map, which is outside of the current ideas on the structuring of tidy data. Therefore, after casting our data wide we will use melt(), rather than gather(), to get the data back into long format so that it works with ggplot2.\nIt is important to note with the use of mba.surf() that it transposes the values while it is creating the calculations and so creating an uneven grid does not work well. Using the code written below one will always need to give the same specifications for pixel count on the x and y axes.\n# The date column must then be converted to numeric values ctd$date \u0026lt;- decimal_date(ctd$date) # Now we may interpolate the data ctd_mba \u0026lt;- mba.surf(ctd, no.X = 300, no.Y = 300, extend = T) dimnames(ctd_mba$xyz.est$z) \u0026lt;- list(ctd_mba$xyz.est$x, ctd_mba$xyz.est$y) ctd_mba \u0026lt;- melt(ctd_mba$xyz.est$z, varnames = c('date', 'depth'), value.name = 'temp') %\u0026gt;% filter(depth \u0026lt; 0) %\u0026gt;% mutate(temp = round(temp, 1)) # Finally we create our gridded result ggplot(data = ctd_mba, aes(x = date, y = depth)) + geom_raster(aes(fill = temp)) + scale_fill_gradientn(colours = rev(ODV_colours)) + geom_contour(aes(z = temp), binwidth = 2, colour = \u0026quot;black\u0026quot;, alpha = 0.2) + geom_contour(aes(z = temp), breaks = 20, colour = \u0026quot;black\u0026quot;) + ### Activate to see which pixels are real and not interpolated # geom_point(data = ctd, aes(x = date, y = depth), # colour = 'black', size = 0.2, alpha = 0.4, shape = 8) + ### labs(y = \u0026quot;depth (m)\u0026quot;, x = NULL, fill = \u0026quot;temp. (°C)\u0026quot;) + coord_cartesian(expand = 0)  Figure 3: The same temperature (°C) profiles seen in Figure 2 with the missing values filled in with multilevel B-splines. Note the artefact created in the bottom right corner. The 20°C contour line is highlighted in black.\nAt first glance this now appears to be a very good approximation of the output from ODV. An astute eye will have noticed that the temperatures along the bottom right corner of this figure are not interpolating in a way that appears possible. It is very unlikely that there would be a deep mixed layer underneath the thermoclines detected during 2015. The reason the splines create this artefact is that they are based on a convex hull around the real data points and so the interpolating algorithm wants to perform a regression towards a mean value away from the central point of where the spline is being calculated from. Because the thermoclines detected are interspersed between times where the entire water column consists of a mixed layer mba.surf() is filling in the areas without data as though they are a fully mixed surface layer.\nBounding boxes There are many ways to deal with this problem with four possible fixes coming to mind quickly. The first is to set extend = F within mba.surf(). This tells the algorithm not to fill up every part of the plotting area and will alleviate some of the inaccurate interpolation that occurs but will not eliminate it. The second fix, which would prevent all inaccurate interpolation would be to limit the depth of all of the temperature profiles to be the same as the shallowest sampled profile. This is not an ideal fix because we would then lose quite a bit of information from the deeper sampling that occurred from 2013 to 2014. The third fix is to create a bounding box and screen out all of the interpolated data outside of it. A fourth option is to use soap-film smoothing over some other interpolation method, such as a normal GAM, concurrently with a bounding box. This is normally a good choice but does not work well with these data so I have gone with option three.\n# Create a bounding box # We want to slightly extend the edges so as to use all of our data left \u0026lt;- ctd[ctd$date == min(ctd$date),] %\u0026gt;% select(-temp) %\u0026gt;% ungroup() %\u0026gt;% mutate(date = date-0.01) bottom \u0026lt;- ctd %\u0026gt;% group_by(date) %\u0026gt;% summarise(depth = min(depth)) %\u0026gt;% mutate(depth = depth-0.01) right \u0026lt;- ctd[ctd$date == max(ctd$date),] %\u0026gt;% select(-temp) %\u0026gt;% ungroup() %\u0026gt;% mutate(date = date+0.01) top \u0026lt;- ctd %\u0026gt;% group_by(date) %\u0026gt;% summarise(depth = max(depth)) %\u0026gt;% mutate(depth = depth+0.01) bounding_box \u0026lt;- rbind(data.frame(left[order(nrow(left):1),]), data.frame(bottom), data.frame(right), data.frame(top[order(nrow(top):1),])) # Now that we have a bounding box we need to # screen out the pixels created outside of it bounding_box_list \u0026lt;- list(bounding_box) names(bounding_box_list[[1]]) \u0026lt;- c(\u0026quot;v\u0026quot;,\u0026quot;w\u0026quot;) v \u0026lt;- ctd_mba$date w \u0026lt;- ctd_mba$depth ctd_mba_bound \u0026lt;- ctd_mba[inSide(bounding_box_list, v, w),] # Correct date values back to date format # Not used as it introduces blank space into the figure # ctd_mba_bound$date \u0026lt;- as.Date(format(date_decimal(ctd_mba_bound$date), \u0026quot;%Y-%m-%d\u0026quot;)) # The screened data ggplot(data = ctd_mba_bound, aes(x = date, y = depth)) + geom_raster(aes(fill = temp)) + scale_fill_gradientn(colours = rev(ODV_colours)) + geom_contour(aes(z = temp), binwidth = 2, colour = \u0026quot;black\u0026quot;, alpha = 0.2) + geom_contour(aes(z = temp), breaks = 20, colour = \u0026quot;black\u0026quot;) + labs(y = \u0026quot;depth (m)\u0026quot;, x = NULL, fill = \u0026quot;temp. (°C)\u0026quot;) + coord_cartesian(expand = 0)  Figure 4: The same temperature (°C) profiles seen in Figure 3 with a bounding box used to screen out data interpolated outside of the range of the recordings.\nSummary In this short tutorial we have seen how to create an interpolated temperature depth profile over time after a fashion very similar to the default output from ODV. We have also seen how to either fill the entire plotting area with interpolated data (Figure 3), or quickly generate a bounding box in order to remove any potential interpolation artefacts (Figure 4). I think this is a relatively straight forward work flow and would work with any tidy dataset. It is worth noting that this is not constrained to depth profiles, but would work just as well with map data. One would only need to change the date and depth variables to lon and lat.\nR is an amazingly flexible language with an incredible amount of support and I\u0026rsquo;ve yet to see it not be able to emulate, or improve upon, an analysis or graphical output from any other software.\n","date":1498435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498435200,"objectID":"ec985d2b4d04f20d8b62b4d246152695","permalink":"https://theoceancode.netlify.app/post/odv_figures/","publishdate":"2017-06-26T00:00:00Z","relpermalink":"/post/odv_figures/","section":"post","summary":"Objective With more and more scientists moving to open source software (i.e. R or Python) to perform their numerical analyses the opportunities for collaboration increase and we may all benefit from this enhanced productivity. At the risk of sounding sycophantic, the future of scientific research truly is in multi-disciplinary work. What then could be inhibiting this slow march towards progress? We tend to like to stick to what is comfortable.","tags":["ODV","visuals","interpolation"],"title":"ODV figures in R","type":"post"},{"authors":null,"categories":["R"],"content":" Objective Most people living in the Western World are very quick to extol the virtues of gender equality. There are however many places where this is not so. This then inevitably leads to conflict as cultures and nations are drawn closer together on our ever shrinking earth. Perhaps not the sort of conflict that leads to sabre rattling, but certainly ideological disagreements that affect policy and have real impacts on large swathes of humanity. So what is to be done? How can say how anyone else should be. There are of course all sorts of moral back and forth\u0026rsquo;s that could be pursued cyclically ad nauseum, but what if we could show quantitatively what the benefit of gender equality was to a nation and the lives of it\u0026rsquo;s people? That is the exact sort of question I like to answer here at this blog and it is a question that came up in my daily life a couple of weeks ago. Because most metrics for most countries are recorded, this is the sort of thing that can be answered. Indeed, it has been before, so here I add another take on an argument that really shouldn\u0026rsquo;t still be happening\u0026hellip;\nData In order to quantify the benefit of gender equality we are going to need global census data, as this is a conversation that needs to be had at a global level. Happily for us these data may be found at Clio Infra. This is an amazing web hosted database that contains an unexpected range of information as it aggregates data from many other sources. I\u0026rsquo;ll likely be going back to this data repository on multiple occasions in the future. But for now we\u0026rsquo;ll stick to gender equality. For this post we will be looking at a number of variables as this allows us to develop a more nuanced view of the relationships that exist between the wealth of nations and the egalitarianism their cultures engender. It is unfortunately not possible to show causality in the relationships we will model below but I think that educated readers will likely arrive at the same inferences as myself. Of course, if anyone has any objections, faults or errors to point out in the analysis that follows I would be very happy to hear them.\nThe data used in this blog were downloaded in \u0026ldquo;Compact\u0026rdquo; form, which comes in .xlsx format. These files were then opened and only the tab containing the data in long format were saved as a .csv file. As an aside, these data are visualised on the Clio Infra website as boxplots using what is clearly ggplot2 code. Also note that unless specified all values are taken as either women/ men or women-men. Meaning that proportions below 1, or negative values signify greater abundance for men than women in those populations.\nI\u0026rsquo;ve chosen o use the metrics seen below as I think they represent inequality between genders as well as throughout populations as a whole.\n## Load libraries library(tidyverse) library(broom) # library(RColorBrewer) ## Load data # Educational Inequality Gini Coefficient (after age 15) education_inequality \u0026lt;- read_csv(\u0026quot;../../static/data/EducationalInequalityGiniCoefficient.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;education_inequality\u0026quot;) # GDP per Capita gdp_per_capita \u0026lt;- read_csv(\u0026quot;../../static/data/GDPperCapita.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;gdp_per_capita\u0026quot;) # Gender Equality Years of Education (after age 15) gender_equality_education \u0026lt;- read_csv(\u0026quot;../../static/data/GenderEqualityYearsofEducation.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;gender_equality_education\u0026quot;) # Historical Gender Equality Index gender_equality_index \u0026lt;- read_csv(\u0026quot;../../static/data/HistoricalGenderEqualityIndex_Compact.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;gender_equality_index\u0026quot;) # Gender Equality of Numeracy gender_equality_numeracy \u0026lt;- read_csv(\u0026quot;../../static/data/GenderEqualityofNumeracy_Compact.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;gender_equality_numeracy\u0026quot;) # Income Inequality income_inequality \u0026lt;- read_csv(\u0026quot;../../static/data/IncomeInequality.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;income_inequality\u0026quot;) # Sex Ratio (ages 0 - 5) sex_ratio \u0026lt;- read_csv(\u0026quot;../../static/data/SexRatio_Compact.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;sex_ratio\u0026quot;) # Total Population total_population \u0026lt;- read_csv(\u0026quot;../../static/data/TotalPopulation_Compact.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;total_population\u0026quot;, value = scale(value)) # Scale the large range # Average Years of Education (after age 15) years_of_education \u0026lt;- read_csv(\u0026quot;../../static/data/AverageYearsofEducation.csv\u0026quot;) %\u0026gt;% mutate(variable = \u0026quot;education_years\u0026quot;) ## Create different dataframe data_long \u0026lt;- rbind(gdp_per_capita, education_inequality, gender_equality_education, gender_equality_index, gender_equality_numeracy, income_inequality, sex_ratio, total_population, years_of_education)  Analysis I recently learned how to use the dplyr pipe (%\u0026gt;%) to perform nested models and so am excited to put that to use here. Our first step is to run simple linear models for each metrics we have loaded against GDP per capita without controlling for time (year of sampling) or space (country). The GDP per capita will be correctly compared against it\u0026rsquo;s corresponding metric at that year and in that country, but we are then allowing mean values effectively to be made of these results. These models will provide us with a range of R^2 values (coefficients of determination), which is the statistic showing how much of the variance in a dependent variable is explained by the independent variable. This allows us to see which of our metrics have the strongest relationship with GDP per capita. Which we may infer as being related to the prosperity of a nation and it\u0026rsquo;s people.\n# Here we run linear models on all available data against GDP per capita # We are not controlling for country or year lm_data_all \u0026lt;- data_long %\u0026gt;% left_join(., gdp_per_capita[,-5], by = c(\u0026quot;ccode\u0026quot;, \u0026quot;country.name\u0026quot;, \u0026quot;year\u0026quot;)) %\u0026gt;% rename(., value = value.x, gdp = value.y) %\u0026gt;% group_by(variable) %\u0026gt;% mutate(n = sum(!is.na(gdp))) %\u0026gt;% ungroup() %\u0026gt;% nest(-n, -variable) %\u0026gt;% mutate(fit = map(data, ~ lm(value ~ gdp, data = .)), results = map(fit, glance)) %\u0026gt;% unnest(results) %\u0026gt;% select(n, variable, adj.r.squared, p.value) %\u0026gt;% arrange(-adj.r.squared) %\u0026gt;% filter(variable != \u0026quot;gdp_per_capita\u0026quot;) # These data are best represented with a table knitr::kable(lm_data_all, digits = 3, caption = \u0026quot;R^2 and p-values for the relationship between several metrics and GDP per capita. Years and country of sampling were not controlled for.\u0026quot;)  Table: Table 1: R^2 and p-values for the relationship between several metrics and GDP per capita. Years and country of sampling were not controlled for.\n   n variable adj.r.squared p.value     1162 education_years 0.623 0.000   215 gender_equality_index 0.523 0.000   134 gender_equality_education 0.321 0.000   9779 education_inequality 0.318 0.000   656 income_inequality 0.106 0.000   348 gender_equality_numeracy 0.075 0.000   205 sex_ratio 0.053 0.001   1371 total_population -0.001 0.760    It is possible however that these relationships have not been static over time. We will also want to display the changes in the R^2 values as time series.\n# Or we may control for years of sampling # This allows us to see if the relationship changes much over time lm_data_year \u0026lt;- data_long %\u0026gt;% left_join(., gdp_per_capita[,-5], by = c(\u0026quot;ccode\u0026quot;, \u0026quot;country.name\u0026quot;, \u0026quot;year\u0026quot;)) %\u0026gt;% rename(., value = value.x, gdp = value.y) %\u0026gt;% group_by(year, variable) %\u0026gt;% mutate(n = sum(!is.na(gdp))) %\u0026gt;% ungroup() %\u0026gt;% filter(n \u0026gt; 10) %\u0026gt;% # Require at least 10 data points nest(-n, -year, -variable) %\u0026gt;% mutate(fit = map(data, ~ lm(value ~ gdp, data = .)), results = map(fit, glance)) %\u0026gt;% unnest(results) %\u0026gt;% select(n, variable, year, adj.r.squared, p.value) %\u0026gt;% arrange(-adj.r.squared) %\u0026gt;% filter(variable != \u0026quot;gdp_per_capita\u0026quot;) # And now for a time series ggplot(data = lm_data_year, aes(x = year, y = adj.r.squared)) + geom_line(aes(colour = variable)) + labs(y = \u0026quot;R^2\u0026quot;)  # Or we may control for the country of sampling # This allows us to see if these relationships # differ in certain parts of the world lm_data_country \u0026lt;- data_long %\u0026gt;% left_join(., gdp_per_capita[,-5], by = c(\u0026quot;ccode\u0026quot;, \u0026quot;country.name\u0026quot;, \u0026quot;year\u0026quot;)) %\u0026gt;% rename(., value = value.x, gdp = value.y) %\u0026gt;% group_by(country.name, variable) %\u0026gt;% mutate(n = sum(!is.na(gdp))) %\u0026gt;% ungroup() %\u0026gt;% filter(n \u0026gt; 10) %\u0026gt;% # Require at least 10 data points nest(-n, -country.name, -variable) %\u0026gt;% mutate(fit = map(data, ~ lm(value ~ gdp, data = .)), results = map(fit, glance)) %\u0026gt;% unnest(results) %\u0026gt;% select(n, variable, country.name, adj.r.squared, p.value) %\u0026gt;% arrange(-adj.r.squared) %\u0026gt;% filter(variable != \u0026quot;gdp_per_capita\u0026quot;) # Create labels for figure lm_label \u0026lt;- lm_data_country %\u0026gt;% select(n, variable) %\u0026gt;% group_by(variable) %\u0026gt;% summarise(n = sum(n)) # These data allow us to display the range of R^2 values # for each country with boxplots # Just to be cheeky let's use the same themeas Clio Infra ggplot(data = lm_data_country, aes(x = variable, y = adj.r.squared)) + geom_boxplot(outlier.colour = NA) + geom_point(colour = \u0026quot;steelblue\u0026quot;, alpha = 0.6, position = \u0026quot;jitter\u0026quot;) + geom_text(data = lm_label, aes(y = -0.1, label = n), colour = \u0026quot;royalblue3\u0026quot;) + labs(y = \u0026quot;R^2\u0026quot;) + theme(axis.text.x = element_text(angle = 15))  Years of education of the populace is constantly coming out on top, with gender equality in education in third. These two metrics must be related in some way though so let\u0026rsquo;s look specifically at that relationship.\ndata_gender \u0026lt;- years_of_education[,-5] %\u0026gt;% left_join(., gender_equality_education[,-5], by = c(\u0026quot;ccode\u0026quot;, \u0026quot;country.name\u0026quot;, \u0026quot;year\u0026quot;)) %\u0026gt;% rename(., education_years = value.x, gender_equality_education = value.y) data_gender \u0026lt;- data_gender[complete.cases(data_gender$gender_equality_education),] lm_data_gender1 \u0026lt;- glance(lm(gender_equality_education ~ education_years, data = data_gender)) lm_data_gender2 \u0026lt;- glance(lm(gender_equality_education ~ education_years, data = filter(data_gender, education_years \u0026gt;= 5))) # And now for a scatterplot ggplot(data = data_gender, aes(x = education_years, y = gender_equality_education)) + geom_point() + geom_smooth(colour = \u0026quot;blue\u0026quot;, method = \u0026quot;lm\u0026quot;) + geom_smooth(data = filter(data_gender, education_years \u0026gt;= 5), colour = \u0026quot;red\u0026quot;, method = \u0026quot;lm\u0026quot;) + geom_label(aes(x = 8.5, y = 0.5, label = paste0(\u0026quot;blue line\\nR^2 = \u0026quot;, round(lm_data_gender1$adj.r.squared,3), \u0026quot;\\nred line\\nR^2 = \u0026quot;, round(lm_data_gender2$adj.r.squared,3))))  Results From Table 1 we may see that the three metrics in descending order that best explain the variance occurring in GDP per capita over time and space are: education of the population as a whole, equality between the genders and the equality in education between the genders. Equality in education for the populace as a whole came in at a close fourth and is perhaps a better predictor than equality between genders due to the much higher sample size that the result is based on (this is an oddly well reported metric). The metrics of income inequality for the populace as a whole, equality in numeracy between genders, and the ratio of sex near birth showed significant relationships with GDP per capita but explained little of the variance. The total population of a country has absolutely nothing to do with the GDP per capita when time and space are not controlled for.\nWe see in Figure 1 that for our top four metrics (education_years, gender_equality_index, gender_equality_education and education_inequality), only education_years shows any consistency over time. We also see that education_inequality has become a much better test of the GDP per capita of a country since the mid 19th century.\nFigure 2 shows that there is a very large range in the relationship between many of these metrics and GDP per capita when we control for country of measurement. Interestingly, the total_population of the country is relevant to the GDP per capita when this information is analysed independently by each country. More important than total_population we see are the metrics education_years and gender_equality_index.\nWhen we look at the specific relationship between education_years and gender_equality_education it would at first appear that at an R^2 value of 0.518 this was a strong relationship. Looking more closely at Figure 3 one will notice that this relationship is not linear and that the fit of the linear model is being affected by several scores at very low education levels and equality scores. If one were to only account for scores where the education_years metric is at least 5 years the strength of the relationship falls dramatically to R^2 = 0.199.\nDiscussion If one was satisfied to take all of the GDP data and the several metrics of equality used in this analysis and clump them together one would be led to believe that the most important thing for a prosperous society is the overall years of education for that countries citizens. This metric is clearly the most important in Table 1, Figure 1 and Figure 2. Equality in education between the genders is also of great importance, though perhaps half as much so as the overall education of the populace. It leads that a society with more equitable education levels between genders would have a greater proportion of educated people and this is generally so however, Figure 3 shows that if one removes scores from nations with particularly low levels of overall education this relationship falls apart. This is because most countries in the world have relatively even levels of education between genders. There is little pattern here because there is little variance in gender_equality_education once the less educated nations have been removed from the calculation.\nMore important to GDP than equal education for genders appears to be the overall equality between genders. This value accounts for much more than education alone and appears to better capture how an egalitarian society benefits from the equal treatment of it\u0026rsquo;s citizens. Clio Infra states that this metric represents:\n The composite index aims to evaluate countries’ performances regarding the progress they made in closing the gender gap in the fields of health, socio-economic resources, politics and household since 1950\n When I started this research I had assumed that equal education between genders would clearly be a very important metric in regards to a prosperous society. Surprisingly this appears to not be as concrete as expected. It certainly is important, more so than income inequality, for example, but is not as important as the overall equality between the genders. This finding is deceiving though because the equality in education for most countries (not just rich ones) is high. It is the difference in equality between genders generally that shows a broader range, and the difference relates strongly to GDP per capita. So a potential recipe for success seems to be to first ensure that as much of a populace as possible (of all genders) receives as much education as possible. It seems likely that this would then fuel gender equality, but the data don\u0026rsquo;t show this unilaterally. So to really move a nation to the top of the GDP per capita list would also require an active effort in ensuring well rounded rights to all genders.\n","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"26c45d1332ee690bbdd99d0ac767d0d6","permalink":"https://theoceancode.netlify.app/post/gender_gdp/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/gender_gdp/","section":"post","summary":"Objective Most people living in the Western World are very quick to extol the virtues of gender equality. There are however many places where this is not so. This then inevitably leads to conflict as cultures and nations are drawn closer together on our ever shrinking earth. Perhaps not the sort of conflict that leads to sabre rattling, but certainly ideological disagreements that affect policy and have real impacts on large swathes of humanity.","tags":["gender","gdp"],"title":"Gender and GDP","type":"post"},{"authors":null,"categories":["R"],"content":" Objective As more and more physical scientists (e.g. oceanographers) move to R from other object oriented command line programming languages, such as Matlab, there will be more and more demand for the code that is needed to do some basic things that they may already know how to do in their previous languages that they don\u0026rsquo;t yet know how to do in R. Surprisingly, there are many things that should be very easy to find how to do in R that are not. Or are at least not widely publicized. One such example is how to plot wind vectors as a time series. This is a very necessary part of any analysis of the wind or currents in a particular area. Making it useful broadly to most climate scientists. Try as I might, I\u0026rsquo;ve only been able to find one source that gives an example of how to plot wind (or current) vectors as a time series with ggplot2 in R. Having now been asked how to do this by several people I thought it would be useful to write up my workflow and put it on the internet so that there is one more source that people searching for answers may find.\nData The data used in this example come from the ERA-Interim reanalysis product. This is an amazing product with dozens of data layers stretched over an even global grid at a native resolution of 0.75°. Here we use the one pixel from this product closest to Cape Town in order to illustrate how to use these data as a time series for a single point in space at a daily resolution for one month (December, 2016). For the purposes of this example we will only be using the following three layers: \u0026lsquo;2 metre temperature\u0026rsquo;, \u0026lsquo;10 metre U wind component\u0026rsquo; and \u0026lsquo;10 metre V wind component\u0026rsquo;. The data used in this post may be downloaded here.\n# First load libraries library(tidyverse) library(scales) # Then data load('../../static/data/ERA_pixel.Rdata')  Wind Vectors The data as they exist within ERA-Interim are already in the format that we need. U and V vectors. Had they not been then we would have needed to convert them. The other common format for wind data are bearing (degrees) and speed (m/s). With this information it is possible to use trigonometry to create the necessary U and V vectors. One may do so with the following chunk of code:\n## Assuming your data frame is called 'my_df' # First it is necessary to correct any bearings of '0' to '360' my_df$bearing[my_df$bearing == 0] \u0026lt;- 360 my_df$u \u0026lt;- (1 * my_df$speed) * sin((my_df$bearing * pi / 180.0)) my_df$v \u0026lt;- (1 * my_df$speed) * cos((my_df$bearing * pi / 180.0))  Assuming that we now have a u and v column in our wind vector data frame, and that the speed of these vectors are in metres per second, we may now use the functionality within ggplot2 to create our wind vector time series. It\u0026rsquo;s almost anti-climactic how straight forward it all is.\nPlotting As we will be using geom_segment() within ggplot2 to create our wind vectors we will want to maintain some control over the output by introducing a scaling object and a default range for the y axis. For now we will just set the wind scalar to 1. We\u0026rsquo;ll change it later to see what benefit this extra step adds. I have chosen the range of values for the y_axis below based on what I knew would illustrate my point, not through any a priori statistical analysis.\nwind_scale \u0026lt;- 1 y_axis \u0026lt;- seq(-5, 5, 5)  To create a wind vector time series we use the following few lines of code:\nggplot(data = ERA_pixel, aes(x = date, y = y_axis)) + # Here we create the wind vectors as a series of segments with arrow tips geom_segment(aes(x = date, xend = date + u*wind_scale, y = 0, yend = v*wind_scale), arrow = arrow(length = unit(0.15, 'cm')), size = 0.5, alpha = 0.7) + # I think adding points at the base of the vectors makes the figure easier to read geom_point(aes(x = date, y = 0), alpha = 0.5, size = 1) + # Changing the dates to better match the range shown on the x axis scale_x_date(labels = date_format('%Y-%m-%d'), breaks = date_breaks('4 days')) + # Change the y axis labels to make sense scale_y_continuous(breaks = y_axis, labels = as.character(abs(y_axis)/wind_scale)) + # Change the x and y axis labels labs(x = NULL, y = 'Wind Speed (m/s)') + coord_equal(ylim = c(min(y_axis/wind_scale), max(y_axis/wind_scale))) + theme(axis.text.x = element_text(angle = 20, hjust = 0.4, vjust = 0.5, size = 8), axis.text.y = element_text(size = 8))  Visualizing wind data in this way introduces a problem into our figure that we do not normally need to contend with when we create time series figures. That problem is that wind speed/ bearing is a two dimensional value, not one dimensional. Therefore the y axis as well as the x axis must be utilized in order to show the wind speed/ bearing accurately. We find a compromise by using coord_equal() to ensure that every step on the x axis is the same size as the y axis. Thus preventing any distortion of vectors that are showing mostly an East/ West heading.\nFor particularly windy parts of the world (such as Cape Town) it may be better to scale the wind vectors so as to help them to look a bit more tidy. But if we do this, our x and y axis relationship will no longer be 1 for 1, as with the figure above. That is where our decision to set a static wind scalar and y axis come into play.\n# Reset the wind scalar to be half the size wind_scale \u0026lt;- 0.5 # The exact same code as above ggplot(data = ERA_pixel, aes(x = date, y = y_axis)) + geom_segment(aes(x = date, xend = date + u, y = 0, yend = v), arrow = arrow(length = unit(0.15, 'cm')), size = 0.5, alpha = 0.7) + geom_point(aes(x = date, y = 0), alpha = 0.5, size = 1) + scale_x_date(labels = date_format('%Y-%m-%d'), breaks = date_breaks('4 days')) + scale_y_continuous(breaks = y_axis/wind_scale, labels = as.character(abs(y_axis)/wind_scale)) + labs(x = NULL, y = 'Wind Speed (m/s)') + coord_equal(ylim = c(min(y_axis/wind_scale), max(y_axis/wind_scale))) + theme(axis.text.x = element_text(angle = 20, hjust = 0.4, vjust = 0.5, size = 8), axis.text.y = element_text(size = 8))  By assigning our scaling variable to an object and using it to control the y axis of our ggplot code we may be sure that no matter how we choose to scale our wind variables they will be correctly represented. This does however limit how large our y axis may be as it must be kept to the same ratio as the x axis. Any larger than the figure shown above and we would start to have unappealingly square figures.\nMultiple Y Axes While it may be frowned upon in the tidyverse, there are still many people that like to use multiple y axes within one figure. I\u0026rsquo;m not going to get into the pro\u0026rsquo;s and con\u0026rsquo;s here, but I will say that of all the possible reasons for using multiple y axes, comparing wind vectors against some other variable is more reasonable than most. This is because the wind vectors themselves do not really have a y axis per se. As we\u0026rsquo;ve seen above, the actual values on the y axis don\u0026rsquo;t matter as long as they are shown 1 for 1 against whatever is on the x axis. Therefore we may very easily show temperature on the y axis and still overplot wind vectors without needing a proper second y axis. We will however insert a label onto the figure so as to make it clear how the wind vectors are to be estimated.\n# Reset the wind scale to 1 wind_scale \u0026lt;- 1 # Now we use temperature for the y axis, so some things must change ggplot(data = ERA_pixel, aes(x = date, y = temp)) + geom_line(colour = 'salmon', show.legend = F, size = 1.5) + geom_segment(aes(x = date, xend = date + u, y = mean(temp, na.rm = T), yend = mean(temp, na.rm = T) +v), arrow = arrow(length = unit(0.15, 'cm')), size = 0.5, alpha = 0.7) + geom_point(aes(x = date, y = mean(temp, na.rm = T)), alpha = 0.5, size = 1) + # Create the label box for our wind vector length legend geom_label(aes(x = as.Date('2016-12-04'), y = 22, label = ' 4 m/s \\n'), size = 5) + # Create the segment for the wind vector legend geom_segment(aes(x = as.Date('2016-12-02'), xend = as.Date('2016-12-06'), y = 21.4, yend = 21.4), size = 0.5, alpha = 0.7) + scale_x_date(labels = date_format('%Y-%m-%d'), breaks = date_breaks('4 days')) + labs(x = NULL, y = 'Temperature (°C)') + # Here we choose to allow ggplot to determine the best range for y axis values coord_equal() + theme(axis.text.x = element_text(angle = 20, hjust = 0.4, vjust = 0.5, size = 8), axis.text.y = element_text(size = 8))  I\u0026rsquo;ll leave it to the reader to decide if this is a reasonable way to visualise these data. But I do know that there is some use for displaying the information in this way. For example, one can clearly see that on days were the temperature decreases rapidly there is a south westerly wind moving over the city. Of course this could still be elucidated were these two figures plotted next to each other via facets, but it should be noted that it isn\u0026rsquo;t necessary.\n","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"f886c848c6f8edee24a9e28175954a4e","permalink":"https://theoceancode.netlify.app/post/wind_vectors/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/wind_vectors/","section":"post","summary":"Objective As more and more physical scientists (e.g. oceanographers) move to R from other object oriented command line programming languages, such as Matlab, there will be more and more demand for the code that is needed to do some basic things that they may already know how to do in their previous languages that they don\u0026rsquo;t yet know how to do in R. Surprisingly, there are many things that should be very easy to find how to do in R that are not.","tags":["visuals","wind"],"title":"Wind Vector Time Series","type":"post"},{"authors":null,"categories":["R"],"content":"","date":1494806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494806400,"objectID":"4f76419bde128f5b5b02ae0eeedffb46","permalink":"https://theoceancode.netlify.app/post/religious_sentiment/","publishdate":"2017-05-15T00:00:00Z","relpermalink":"/post/religious_sentiment/","section":"post","summary":"","tags":["religion","text mining","sentiment analysis"],"title":"Religious sentiment","type":"post"},{"authors":["RW Schlegel","ECJ Oliver","T Wernberg","AJ Smit"],"categories":null,"content":"","date":1484002800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484002800,"objectID":"23ea356d126b6305a8c493cb8e58c32a","permalink":"https://theoceancode.netlify.app/publication/cooccurrence/","publishdate":"2017-01-10T00:00:00+01:00","relpermalink":"/publication/cooccurrence/","section":"publication","summary":"A changing global climate places shallow water ecosystems at more risk than those in the open ocean as their temperatures may change more rapidly and dramatically. To this end, it is necessary to identify the occurrence of extreme ocean temperature events – marine heatwaves (MHWs) and marine cold-spells (MCSs) – in the nearshore (","tags":["R","coastal","marine heatwaves","marine cold-spells"],"title":"Nearshore and offshore co-occurrence of marine heatwaves and cold-spells","type":"publication"},{"authors":["RW Schlegel","AJ Smit"],"categories":null,"content":"","date":1481756400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481756400,"objectID":"7ada37b9b35f6bf1c621e5e5947cc974","permalink":"https://theoceancode.netlify.app/publication/trend-analysis/","publishdate":"2016-12-15T00:00:00+01:00","relpermalink":"/publication/trend-analysis/","section":"publication","summary":"In South Africa, 129 in situ temperature time series of up to 43 years are used for investigations of the thermal characteristics of coastal seawater. They are collected with handheld thermometers or underwater temperature recorders (UTRs) and are recorded at precisions from 0.5° to 0.001°C. Using the natural range of seasonal signals and variability for 84 of these time series, their length, decadal trend, and data precision were systematically varied before fitting generalized least squares (GLS) models to study the effect these variables have on trend detection. The variables that contributed most to accurate trend detection, in decreasing order, were time series length, decadal trend, variance, percentage of missing data (% NA), and measurement precision. Time series greater than 30 years in length are preferred and although larger decadal trends are modeled more accurately, modeled significance (p value) is largely affected by the variance present. The risk of committing both type-1 and type-2 errors increases when ≥5% NA is present. There is no appreciable effect on model accuracy between measurement precision of 0.1°–0.001°C. Measurement precisions of 0.5°C require longer time series to give equally accurate model results. The implication is that the thermometer time series in this dataset, and others around the world, must be at least two years longer than their UTR counterparts to be useful for decadal-scale climate change studies. Furthermore, adding older lower-precision UTR data to newer higher-precision UTR data within the same time series will increase their usefulness for this purpose.","tags":["time series analysis","R"],"title":"Climate Change in Coastal Waters: Time Series Properties Affecting Trend Estimation","type":"publication"}]