---
title: "Downloading environmental data in R"
author: "Robert W Schlegel"
date: "2020-02-14"
categories: ["R"]
tags: ["environmental", "data", "download"]
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  eval = FALSE
)
```

## Objective
Having been working in environmental science for several years now, entirely using R, I've come to greatly appreciate environmental data sources that are easy to access. If you are reading this text now however, that probably means that you, like me, have found that this often is not the case. The struggle to get data is real. But it shouldn't be. Most data hosting organisations do want scientists to use their data and do make it freely available. But sometimes it feels like the path to access was designed by crab people, rather than normal topside humans. I recently needed to gather several new data products and in classic 'cut your nose off to spite your face' fashion I insisted on doing all of it directly through an R script that could be run in RStudio. Besides being stubborn, one of the main reasons I felt this was necessary is that I wanted these download scripts to be able to be run operationally via a cron job. I think I came out pretty successful in the end so wanted to share the code with the rest of the internet. Enjoy.

```{r}
# The packages we will use
library(tidyverse) # A staple for most modern data management in R
library(RCurl) # For helping R to make sense of URLs for web hosted data
library(XML) # For reading the HTML tables created by RCurl
library(tidync) # For easily dealing with NetCDF data
library(doParallel) # For parallel processing
# library(threadr)
```

## Downloading NOAA OISST
I've already written a post about how to download NOAA OISST data using the __`rerddap`__ package which may be found [here](https://robwschlegel.github.io/heatwaveR/articles/OISST_preparation.html). That post talks about how to get subsets of NOAA data, which is useful for projects with a refined scope, but it is laboriously slow if one simply wants the full global product. It is also important to note that as of December 2019 the NOAA OISST data up on the ERDDAP server stopped working. It may be back as of this writing (Feb 2020), but any data source that is allowed to be down for multiple months is not reliable and should be avoided. On that note, it is also now known that from 2016 onwards massive cold biases have begun to seep into this product. See the poster [here](https://ams.confex.com/ams/2019Annual/webprogram/Manuscript/Paper352621/Liu.bufr.ncei%20AMS%20Poster%2047x33-final2.pdf) that was presented at the 2019 AGU. That being said, this dataset is still incredibly useful and should the fine people at NOAA decide to act on this knowledge the product can be repaired. I think it is still one of the best options for historic SST research and is something I continue to use in my day-to-day. 

I feel like I need to stress here that this is an incredibly fast, perhaps aggressive, method for accessing these data and I urge responsiblity in only downloading as much data as are necessary. Please do not download the entire dataset just because you can.

```{r NOAA-dl}
# Set the number of cores one would like to use for the following tasks
# Most normal laptops have four, so setting 3 cores for calculations is a good idea
registerDoParallel(cores = 3)

# First we tell R where the dat are on the interwebs
OISST_url_month <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/access/avhrr-only/"

# Then we pull that into a happy format
  # There is a lot here so it takes ~1 minute
OISST_url_month_get <- getURL(OISST_url_month)

# Before we continue let's set a limit on the data we are going to download
  # NB: One should not simply download the entire dataset just because it is possible.
  # There should be a compelling reason for doing so.
start_date <- as.Date("2019-01-01")

# Now we strip away all of the unneeded stuff to get just the months of data that are available
OISST_months <- data.frame(months = readHTMLTable(OISST_url_month_get, skip.rows = 1:2)[[1]]$Name) %>% 
  mutate(months = lubridate::as_date(str_replace(as.character(months), "/", "01"))) %>% 
  filter(months >= max(lubridate::floor_date(start_date, unit = "month"))) %>% # Filtering out months before Jan 2019
  mutate(months = gsub("-", "", substr(months, 1, 7))) %>% 
  na.omit()

# Up next we need to now find the URLs for each individual day of data
# To do this we will wrap the following chunk of code into a function so we can loop it more easily
OISST_url_daily <- function(target_month){
  OISST_url <- paste0(OISST_url_month, target_month,"/")
  OISST_url_get <- getURL(OISST_url)
  OISST_table <- data.frame(files = readHTMLTable(OISST_url_get, skip.rows = 1:2)[[1]]$Name) %>% 
    mutate(files = as.character(files)) %>% 
    filter(grepl("avhrr", files)) %>% 
    mutate(t = lubridate::as_date(sapply(strsplit(files, "[.]"), "[[", 2)),
           full_name = paste0(OISST_url, files))
  return(OISST_table)
}

# Here we collect the URLs for every day of data avilable from 2019 onwards
OISST_filenames <- plyr::ldply(OISST_months$months, .fun = OISST_url_daily, .parallel = TRUE)

# Just to keep things tidy in this vignette I am now going to limit this data collection even further
OISST_filenames <- OISST_filenames %>% 
  filter(t <= "2019-01-31")

# This function will go about downloading each day of data as a temporary NetCDF file
# We will run this via plyr to expedite the process
# Note that this will download files into your current working directory
# To change the location of the download change the 'destfile' argument
OISST_url_daily_dl <- function(target_URL){
  download.file(url = target_URL, method = "libcurl", destfile = "temp.nc")
  temp_dat <- tidync("temp.nc") %>% 
    hyper_tibble() %>% 
    select(lon, lat, time, sst) %>% 
    dplyr::rename(t = time, temp = sst) %>% 
    mutate(t = as.Date(t, origin = "1978-01-01"))
  return(temp_dat)
}

# And with that we are clear for take off
system.time(OISST_dat <- plyr::ldply(OISST_filenames$full_name, 
                                     .fun = OISST_url_daily_dl, .parallel = TRUE)) # 42 seconds

# In under 1 minute a user may have a month of global data downloaded, processed, and ready for use in R.
```

## Downloading GLORYS
